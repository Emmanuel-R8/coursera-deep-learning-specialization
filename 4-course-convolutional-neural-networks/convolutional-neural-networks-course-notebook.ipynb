{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Week-1:-Convolutional-Neural-Networks\" data-toc-modified-id=\"Week-1:-Convolutional-Neural-Networks-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Week 1: Convolutional Neural Networks</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Computer-Vision\" data-toc-modified-id=\"Vid:-Computer-Vision-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Vid: Computer Vision</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Edge-Detection-Example\" data-toc-modified-id=\"Vid:-Edge-Detection-Example-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Vid: Edge Detection Example</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-More-Edge-Detection\" data-toc-modified-id=\"Vid:-More-Edge-Detection-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Vid: More Edge Detection</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Padding\" data-toc-modified-id=\"Vid:-Padding-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Vid: Padding</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Strided-Convolutions\" data-toc-modified-id=\"Vid:-Strided-Convolutions-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Vid: Strided Convolutions</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Convolutions-Over-Volume\" data-toc-modified-id=\"Vid:-Convolutions-Over-Volume-16\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Vid: Convolutions Over Volume</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-One-Layer-of-a-Convolutional-Network\" data-toc-modified-id=\"Vid:-One-Layer-of-a-Convolutional-Network-17\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Vid: One Layer of a Convolutional Network</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Simple-Convolutional-Network-Example\" data-toc-modified-id=\"Vid:-Simple-Convolutional-Network-Example-18\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Vid: Simple Convolutional Network Example</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Pooling-Layers\" data-toc-modified-id=\"Vid:-Pooling-Layers-19\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Vid: Pooling Layers</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-CNN-Example\" data-toc-modified-id=\"Vid:-CNN-Example-110\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Vid: CNN Example</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Why-Convolutions\" data-toc-modified-id=\"Vid:-Why-Convolutions-111\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Vid: Why Convolutions</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-1:-Homework\" data-toc-modified-id=\"Week-1:-Homework-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Week 1: Homework</a></div><div class=\"lev2 toc-item\"><a href=\"#Forward-Pass\" data-toc-modified-id=\"Forward-Pass-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Forward Pass</a></div><div class=\"lev2 toc-item\"><a href=\"#Backprop\" data-toc-modified-id=\"Backprop-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Backprop</a></div><div class=\"lev3 toc-item\"><a href=\"#Local-Gradients\" data-toc-modified-id=\"Local-Gradients-221\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Local Gradients</a></div><div class=\"lev3 toc-item\"><a href=\"#Resources\" data-toc-modified-id=\"Resources-222\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Resources</a></div><div class=\"lev2 toc-item\"><a href=\"#Tensorflow-Organization\" data-toc-modified-id=\"Tensorflow-Organization-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Tensorflow Organization</a></div><div class=\"lev3 toc-item\"><a href=\"#Code-Organization\" data-toc-modified-id=\"Code-Organization-231\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Code Organization</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-2:-Case-Studies\" data-toc-modified-id=\"Week-2:-Case-Studies-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Week 2: Case Studies</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Why-Case-Studies\" data-toc-modified-id=\"Vid:-Why-Case-Studies-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Vid: Why Case Studies</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Classic-Networks\" data-toc-modified-id=\"Vid:-Classic-Networks-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Vid: Classic Networks</a></div><div class=\"lev3 toc-item\"><a href=\"#LeNet-5\" data-toc-modified-id=\"LeNet-5-321\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>LeNet-5</a></div><div class=\"lev3 toc-item\"><a href=\"#AlexNet\" data-toc-modified-id=\"AlexNet-322\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>AlexNet</a></div><div class=\"lev3 toc-item\"><a href=\"#VGG-16\" data-toc-modified-id=\"VGG-16-323\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>VGG-16</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-ResNets\" data-toc-modified-id=\"Vid:-ResNets-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Vid: ResNets</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Why-ResNets-Work\" data-toc-modified-id=\"Vid:-Why-ResNets-Work-34\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Vid: Why ResNets Work</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Networks-in-Networks-and-1x1-Convolutions\" data-toc-modified-id=\"Vid:-Networks-in-Networks-and-1x1-Convolutions-35\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Vid: Networks in Networks and 1x1 Convolutions</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Inception-Network-Motivation\" data-toc-modified-id=\"Vid:-Inception-Network-Motivation-36\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Vid: Inception Network Motivation</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Inception-Network\" data-toc-modified-id=\"Vid:-Inception-Network-37\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Vid: Inception Network</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-2:-Practical-Advice-for-Using-CNNs\" data-toc-modified-id=\"Week-2:-Practical-Advice-for-Using-CNNs-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Week 2: Practical Advice for Using CNNs</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Using-Open-Source-Implementation\" data-toc-modified-id=\"Vid:-Using-Open-Source-Implementation-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Vid: Using Open Source Implementation</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Transfer-Learning\" data-toc-modified-id=\"Vid:-Transfer-Learning-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Vid: Transfer Learning</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Data-Augmentation\" data-toc-modified-id=\"Vid:-Data-Augmentation-43\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Vid: Data Augmentation</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-State-of-Computer-Vision\" data-toc-modified-id=\"Vid:-State-of-Computer-Vision-44\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Vid: State of Computer Vision</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-2:-Homework\" data-toc-modified-id=\"Week-2:-Homework-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Week 2: Homework</a></div><div class=\"lev2 toc-item\"><a href=\"#Keras\" data-toc-modified-id=\"Keras-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Keras</a></div><div class=\"lev2 toc-item\"><a href=\"#ResNet-Implementation\" data-toc-modified-id=\"ResNet-Implementation-52\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>ResNet Implementation</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-3:-Object-Detection\" data-toc-modified-id=\"Week-3:-Object-Detection-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Week 3: Object Detection</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Object-Localization\" data-toc-modified-id=\"Vid:-Object-Localization-61\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Vid: Object Localization</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Landmark-Detection\" data-toc-modified-id=\"Vid:-Landmark-Detection-62\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Vid: Landmark Detection</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Object-Detection\" data-toc-modified-id=\"Vid:-Object-Detection-63\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Vid: Object Detection</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Convolutional-Implementation-of-Sliding-Windows\" data-toc-modified-id=\"Vid:-Convolutional-Implementation-of-Sliding-Windows-64\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Vid: Convolutional Implementation of Sliding Windows</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Bounding-Box-Predictions\" data-toc-modified-id=\"Vid:-Bounding-Box-Predictions-65\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Vid: Bounding Box Predictions</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Intersection-Over-Union\" data-toc-modified-id=\"Vid:-Intersection-Over-Union-66\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>Vid: Intersection Over Union</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Non-max-Suppression\" data-toc-modified-id=\"Vid:-Non-max-Suppression-67\"><span class=\"toc-item-num\">6.7&nbsp;&nbsp;</span>Vid: Non-max Suppression</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Anchor-Boxes\" data-toc-modified-id=\"Vid:-Anchor-Boxes-68\"><span class=\"toc-item-num\">6.8&nbsp;&nbsp;</span>Vid: Anchor Boxes</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Complete-YOLO-Algorithm\" data-toc-modified-id=\"Vid:-Complete-YOLO-Algorithm-69\"><span class=\"toc-item-num\">6.9&nbsp;&nbsp;</span>Vid: Complete YOLO Algorithm</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Region-Proposals-(Optional)\" data-toc-modified-id=\"Vid:-Region-Proposals-(Optional)-610\"><span class=\"toc-item-num\">6.10&nbsp;&nbsp;</span>Vid: Region Proposals (Optional)</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-3:-Homework\" data-toc-modified-id=\"Week-3:-Homework-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Week 3: Homework</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-4:-Face-Recognition\" data-toc-modified-id=\"Week-4:-Face-Recognition-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Week 4: Face Recognition</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-What-is-Face-Recognition\" data-toc-modified-id=\"Vid:-What-is-Face-Recognition-81\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Vid: What is Face Recognition</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-One-Shot-Learning\" data-toc-modified-id=\"Vid:-One-Shot-Learning-82\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Vid: One Shot Learning</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Siamese-Network\" data-toc-modified-id=\"Vid:-Siamese-Network-83\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Vid: Siamese Network</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Triplet-Loss\" data-toc-modified-id=\"Vid:-Triplet-Loss-84\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>Vid: Triplet Loss</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Face-Verification-and-Binary-Classification\" data-toc-modified-id=\"Vid:-Face-Verification-and-Binary-Classification-85\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>Vid: Face Verification and Binary Classification</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-4:-Neural-Style-Transfer\" data-toc-modified-id=\"Week-4:-Neural-Style-Transfer-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Week 4: Neural Style Transfer</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-What-is-Neural-Style-Transfer?\" data-toc-modified-id=\"Vid:-What-is-Neural-Style-Transfer?-91\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Vid: What is Neural Style Transfer?</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-What-are-Deep-CNNs-Learning?\" data-toc-modified-id=\"Vid:-What-are-Deep-CNNs-Learning?-92\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Vid: What are Deep CNNs Learning?</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Cost-Function\" data-toc-modified-id=\"Vid:-Cost-Function-93\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Vid: Cost Function</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Content-Cost-Function\" data-toc-modified-id=\"Vid:-Content-Cost-Function-94\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;</span>Vid: Content Cost Function</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Style-Cost-Function\" data-toc-modified-id=\"Vid:-Style-Cost-Function-95\"><span class=\"toc-item-num\">9.5&nbsp;&nbsp;</span>Vid: Style Cost Function</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-1D-and-3D-Generalizations\" data-toc-modified-id=\"Vid:-1D-and-3D-Generalizations-96\"><span class=\"toc-item-num\">9.6&nbsp;&nbsp;</span>Vid: 1D and 3D Generalizations</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-4:-Homework\" data-toc-modified-id=\"Week-4:-Homework-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Week 4: Homework</a></div><div class=\"lev2 toc-item\"><a href=\"#Face-Recognition/Verification\" data-toc-modified-id=\"Face-Recognition/Verification-101\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Face Recognition/Verification</a></div><div class=\"lev2 toc-item\"><a href=\"#Neural-Style-Transfer\" data-toc-modified-id=\"Neural-Style-Transfer-102\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>Neural Style Transfer</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 1: Convolutional Neural Networks\n",
    "\n",
    "## Vid: Computer Vision\n",
    "Applications of computer vision include image classification, object detection (locating types of objects in an image and drawing a bounding box around them), neural style transfer (repainting content images in the style of a different image). One problem with computer vision is the input data can get very large. A 1 MP image has 3 million data RGB pixel values, so if your first hidden layer has 1000 units then that is 3 billion parameters; not only is the compute slow, but it is difficult to gather enough data to prevent overfitting with so many parameters. The convolution operation is a fundamental building block of the CNN that will allow us to use such high-dimensional image inputs.\n",
    "\n",
    "## Vid: Edge Detection Example\n",
    "Consider a motivating example of detecting vertical and horizontal edges in a grayscale 6x6 pixel image. First construct a smaller 3x3 matrix called a *filter* (or sometimes *kernel*) where the values are specially chosen. We convolve the filter matrix with the image matrix to achieve a 4x4 output matrix that you can think of as a 4x4 image. The convolution proceeds by overlaying the filter into the top leftmost corner of the image, multiplying elementwise in the overlay region, then summing up to get a single number. This gives the top leftmost value in the output matrix (see the image below). You then shift the filter one column to the right and repeat to fill in the next element of the output matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic1.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that the convolution operation is accessed by conv_forward() in python, tf.nn.conv2d() in tensorflow, Conv2D() in Keras etc.\n",
    "\n",
    "This procedure using this particular filter outputs an \"image\" that draws the locations of vertical edges from the original image (in this example our drawn edge looks very thick because we are using very small input matrix relative to the filter size). The specific values in this filter are causing us to output non-zero values whenever we overlay onto a region where there are dark pixels on the left and light pixels on the right or viceversa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic2.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: More Edge Detection\n",
    "For edge filters, negative vs. positive values indicate a light -> dark transition (\"positive edge\") vs. a dark -> light transition (\"negative edge\"). If you don't care to distinguish, you could take absolute values of the output matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic3.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For edge filters there are lots of opinions on the best values to use in the matrix (\"Sobel filter\", \"Schorr filter\"), but it turns out we can just learn the best values as additional parameters of our model. This will result in filter matrices that are better at capturing the particular statisics of our data than any handcoded values would be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Padding\n",
    "*Padding* is an important modification to the basic convolutional operation discussed above. For an $n\\times n$ image convolved with an $f \\times f$ filter you will get an $n-f+1 \\times n-f+1$ output matrix. One downside is if you perform repeated convolutions your image will shrink each time! Also, pixels near the edge of the input image are \"touched\" much less by the filter, and so information from those regions is lost. A solution is to \"pad\" the image with an additional pixel border before applying the convolution. By convention, we pad with a border $p$ pixels thick containing all 0 values, so the output is $(n+2p)-f+1 \\times (n+2p)-f+1$.\n",
    "\n",
    "The choice of how much to pad has two common approaches:\n",
    "- *Valid Convolutions*: no padding\n",
    "- *Same Convolutions*: pad so that the output is exactly the same as the input size (achieved with $p=\\frac{f-1}{2}$). $f$ is usually odd by convention, 3x3 is common, 5x5 and some 7x7.\n",
    "\n",
    "## Vid: Strided Convolutions\n",
    "The stride parameter, $s$, for a convolution dictates how many pixels you step your filter in between convolution operations (i.e. how many locations for the filter you skip over); this will affect the output dimension. For $n \\times n$ image convolved with an $f \\times f$ filter using padding $p$ and and stride $s$ the output will be size $\\lfloor\\frac{n+2p-f}{s}+1 \\rfloor \\times \\lfloor \\frac{n+2p-f}{s} + 1 \\rfloor$, where we use the floor operator to round down to the nearest integer.\n",
    "\n",
    "Note regarding notation: in many math textbooks the convolution operation between two matrices is defined so that you first mirror-image the filter matrix (flip on both the $x$ and $y$ axis) before overlaying and multiplying. This ensures associativity of the operation which is nice for some applications. What we conventionally do in ML that we call the convolution operation (i.e. not executing the mirror imaging before multiplying) is often called \"cross-correlation\" in math textbooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Convolutions Over Volume\n",
    "Consider edge detection in an RGB image. The image can be thought of as a $6\\times6\\times3$ tensor (image height $\\times$ image width $\\times$ num. channels) and we will convolve it now with a $3\\times3\\times3$ filter. The size of the last dimension, \"num channels\" must match between the input and the filter, so that the result of the convolution will be a $4\\times4$ image i.e. no longer 3D! The math is exactly analagous to the 2D case: place the filter in the top leftmost position (there is no depth-wise flexibility in where we place it, since the \"num channels\" dimensions are equal), elementwise multiply and sum, move the filter and repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic5.png\" width=450/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Values for the filter can be chosen so that e.g. you find any edge regardless of color (the 2D layer for each channel has values for edge detection) or you only find red edges (the first 2D layer has values for edge detection while the last two layers are all zero).\n",
    "\n",
    "Consider applying a vertical edge filter and a horizontal edge filter, both of which give a $4 \\times 4$ output: we can take the two outputs and layer them to get a $4\\times 4 \\times 2$ volume, so we can think of the output has having a num. channels = num. filters = num. different features you are detecting. In general (without stride or padding) for an $n\\times n \\times n_c$ input convolved with $n'_c$ filters each with size $f\\times f \\times n_c$, the result will be an $n-f+1 \\times n-f+1 \\times n'_c$ volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">Decent resolution RGB images have tons of pixels, so NNs would need many many parameters (susceptible to overfitting) and take a long time to compute. The convolution operation of CNNs allows us to use such high-dimensional image inputs without these problems. An input matrix is convolved with a smaller matrix with specially chosen values called a *filter* (or sometimes *kernel*) by overlaying the filter into the top leftmost corner of the input, multiplying elementwise in the overlay region, and summing up to get a single number for the top leftmost value in the output matrix. The filter is shifted and this is repeated to fill in the next element of the output matrix etc. The input can be *padded* with a pixel border of thickness $p$ (usually pixel values of 0) which ensures that information at the edges of the image receives a fair sampling, and we can use a *stride* $s$ to control how many pixels to step the filter in between convolution operations. The output matrix will have dimension $\\lfloor\\frac{n+2p-f}{s}+1 \\rfloor \\times \\lfloor \\frac{n+2p-f}{s} + 1 \\rfloor$, where we use the floor operator to round down to the nearest integer. The values in the filter matrix can be chosen so that the output reflects the presence of different features e.g. vertical vs. horizontal edges. A 3D tensor (like an RGB image with $n_c$ number of color channels) can be convolved with a 3D filter in an analogous way, where the third dimension $n_c$ must be equal between the input and filter so that the output will always be 2D. Values for the filter can be chosen so that e.g. you find any edge regardless of color (the 2D layer for all channels has values for edge detection) or you only find red edges (the first 2D layer has values for edge detection while the last two layers are all zero). If we apply $n'_c$ of these different filters we can stack the resulting 2D outputs into a 3D volume with a depth of $n'_c$.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: One Layer of a Convolutional Network\n",
    "We can construct a layer of a CNN by adding a non-linear transformation to the output matrices of the different filters before stacking them into a volume. We add a single real number bias $b_j$ to each output matrix, and then apply an activation function elementwise. The resulting 2D matrices are stacked to give a volume that is passed to the next layer! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic6.png\" width=450/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A single filter in a layer is analagous to a single neuron in a layer; The input volume to the layer plays the role of $a^{[l]}$, while the filters play the role of the $W^{[l]}$ parameter.\n",
    "\n",
    "Assume in one layer you have 10 filters that are each $3\\times 3\\times 3$. How many parameters does the layer have? Each filter has $3^3$ learnable values in the matrix, plus a single bias term. So 28 parameters per filter for 10 filters = 280 total. Notice that with just a small number of parameters we can detect a variety of different features at locations all over the image.\n",
    "\n",
    "**If $l$ is a convolutional layer:**\n",
    "- $f^{[l]}$ is the filter size (dimension of the square matrix, often 3, 5 or 7)\n",
    "- $p^{[l]}$ is padding\n",
    "- $s^{[l]}$ is the stride\n",
    "- $n_c^{[l]}$ is the number of filters used in the layer (number of \"channels\")\n",
    "- Each filter must have depth matching the input volume, so filter dimensions are $f^{[l]} \\times f^{[l]} \\times n_c^{[l-1]}$\n",
    "\n",
    "\n",
    "- The size of the input volume from the previous layer is $n_H^{[l-1]} \\times n_W^{[l-1]} \\times n_c^{[l-1]}$\n",
    "- The size of the output volume for this layer is $n_H^{[l]} \\times n_W^{[l]} \\times n_c^{[l]}$\n",
    "    - $n_H^{[l]} = \\lfloor \\frac{n_H^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1 \\rfloor$ and likewise for the width\n",
    "    - Activations will have same dimension as output volume\n",
    "    \n",
    "\n",
    "- The \"weights\" of a layer is a tensor formed by stacking all the filters. It has dimension $f^{[l]} \\times f^{[l]} \\times n_c^{[l-1]} \\times n_c^{[l]}$\n",
    "- The \"bias\" of the layer is a vector of dimension $n_c^{[l]}$\n",
    "We can still implement a fully vectorized version of this which stacks the values for different *samples* side by side! In this case we will have activations $A^{[l]}$ that are 4D with size $m \\times n_H^{[l]} \\times n_W^{[l]} \\times n_c^{[l]}$, and we will represent the biases with a 4D tensor of dimension $1\\times1\\times1\\times n_c^{[l]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Simple Convolutional Network Example\n",
    "Consider a cat classifier that takes in $39\\times39\\times 3$ RGB images. The first layer uses 10 different $3\\times3$ filters to detect features and \"valid\" convolution (i.e. zero padding!) with stride of 1. The activation output of this first layer (which is the input to the next layer) will be a $37 \\times 37 \\times 10$ volume. If the second layer uses 20 different $5 \\times 5$ filters with a stride of 2 then the activation output from the second layer will be $17 \\times 17 \\times 20$. The third layer with 40 different $5 \\times 5$ filters and stride=2 outputs a $7 \\times 7 \\times 40$ volume. For the next layer we take this volume and flatten (\"unroll\") into a vector with 1960 components, and these are fed into a final sigmoid (or softmax) output layer for classification (see the image below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic7.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is a typical architecture in CNNs where the height and width of the input might stay relatively constant for a few layers then gradually trend down while the number of channels grows. Much of the work in designing CNNs is choosing good values for parameters like $f$, $s$, $p$ and $n_c$. Most CNNs incorporate three types of layers convolution (CONV), pooling (POOL) and fully connected (FC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Pooling Layers\n",
    "Pooling layers reduce reduce the size of intermediate representations (which speeds computation) and make some of the detected features more robust. Consider a $4 \\times 4$ input image: we can apply *Max pooling* to transform this into a $2 \\times 2$ output by a non-linear downsampling. We can think of this operation as proceeding similar to a convolution, but instead of applying an elementwise multiplication in each region of overlay, you instead pull the maximum value from the input image in that region. A pooling operation has hyperparameters $f$ (the size of the filter) and $s$ (the stride), but typically does not using padding and doesn't actually have an *parameters* to learn! Common parameter values of $f=2$ or 3, and $s=2$. This operation is like asking does the feature appear anywhere in this region (as indicated by a large pixel value), if so its presence will be preserved by the $max$ operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic8.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Unlike with convolution operations, for max pooling on a 3D input the filter remains 2D and the pooling is performed independently on each of the channels, so the output has the same depth as the input. The output height and width of pooling follows the same formula as for a convolution.\n",
    "\n",
    "Average pooling is exactly the same except that is applies an averaging operation instead of a max. This is much less commonly used except perhaps for very deep layers where it can be used to collapse the height and width to a single dimension (e.g. taking a $7 \\times 7 \\times 1000$ and collapsing to $7 \\times 7 \\times 1$\n",
    "\n",
    "**If $l$ is a pooling layer:**\n",
    "- $f^{[l]}$ is the filter size (dimension of the square matrix, typically 2 or 3)\n",
    "- $p^{[l]}$ is padding (almost always ZERO)\n",
    "- $s^{[l]}$ is the stride (typically 2)\n",
    "- There is only a single filter for the layer, it is 2D with dimension $f^{[l]} \\times f^{[l]}$\n",
    "\n",
    "\n",
    "- The size of the input volume from the previous layer is $n_H^{[l-1]} \\times n_W^{[l-1]} \\times n_c^{[l-1]}$\n",
    "- The size of the output volume for this layer is $n_H^{[l]} \\times n_W^{[l]} \\times n_c^{[l]}$\n",
    "    - Because the 2D filter is applied channel-by-channel we will always have $n_c^{[l]} = n_c^{[l-1]}$\n",
    "    - $n_H^{[l]} = \\lfloor \\frac{n_H^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1 \\rfloor$ and likewise for the width\n",
    "\n",
    "\n",
    "- There are no parameters to learn for the layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">We construct a convolutional layer in a CNN by taking the output matrices from all the filters, adding a different bias value $b_j$ to each one, and then applying a non-linear activation function elementwise; the resulting 2D matrices are stacked to give a volume that is passed to the next layer. Output dimensions for layers use the notation $n_H^{[l]} \\times n_W^{[l]} \\times n_c^{[l]}$. A single filter in a layer is analagous to a single neuron; The input volume to the layer plays the role of $a^{[l]}$, while the filters play the role of the $W^{[l]}$ parameter. We can still implement a fully vectorized version of this which stacks the values for different *samples* side by side! In this case we will have activations $A^{[l]}$ that are 4D with size $m \\times n_H^{[l]} \\times n_W^{[l]} \\times n_c^{[l]}$, and we will represent the biases with a 4D tensor of dimension $1\\times1\\times1\\times n_c^{[l]}$. A common architecture in CNNs has the height and width of the input relatively constant for a few layers (small stride) then gradually trend down while the number of channels grows; for instance a $39 \\times 39 \\times 3$ RGB image may be transformed to a $7 \\times 7 \\times 1000$ volume. In the second-to-last layer we typically flatten (\"unroll\") the input volume into a 1D vector, which is fed into a final sigmoid (or softmax) output layer for classification. Pooling layers reduce reduce the size of intermediate representations (which speeds computation) and make some of the detected features more robust. A pooling layer has a single 2D filter of size $f$ and stride $s$ (typically padding is zero), and there are no learning parameter values for the filter. Rather than an elementwise multiplication in each region of overlay as in convolution, you instead pull the maximum value from the input in that region; this is performed channel-by-channel with the 2D filter so that the output volume has the same depth as the input. This operation is like asking does the feature appear anywhere in this region (as indicated by a large pixel value), if so its presence will be preserved by the $max$ operation.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: CNN Example\n",
    "Consider hand-written digit recognition from $32 \\times 32 \\times 3$ RGB images. Note that conventionally we often combine a CONV and subsequent POOL and call them a single layer, because the POOL layers do not have any learnable parameters. In the image below we have CONV1+POOL1 layer followed by a CONV2+POOL2 layer. The output of POOL2 is a $5 \\times 5 \\times 16$ volume which we unroll into a vector to pass to the next layer FC3 which has 120 units all connected to every one of the 400 inputs: this is called a *Fully Connected Layer* and it behaves just like a layer of our classical NNs and has weights $W^{[3]}$ and bias $b^{[3]}$. FC4 is another densely connected layer with 84 units, which feeds into a softmax output layer for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic9.png\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notice again that as you go deeper $n_H$ and $n_W$ decrease while $n_c$ grows; overall the number of elements in the output volume (\"activation size\") will decrease, but if it drops too quickly that is typically bad for performance. The pattern of one or more CONV followed by POOL, repeated many times, then followed by several FC layers is a common architecture. Most of the learnable parameters come from the FC layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic10.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The best way to gain intuition for how to combine CONV, POOL and FC layers into a successful architecture is to see many example cases of succesful networks for different use-cases. This is the focus of the second week.\n",
    "\n",
    "## Vid: Why Convolutions\n",
    "Why are convolutions so useful? The two advantages over FC layers are\n",
    "- **Parameter sharing**: A feature detector (like vertical edge detector) that is useful in one part of the image is probably useful in other parts. A single 3x3 filter with learned parameters for edge detection can be applied to different locations all over your image so that the output values for the different location \"share\" the 9 parameters of the filter.\n",
    "- **Sparsity of connections**: In each layer, each output value (single element of the output tensor) depends on only a small number of inputs (a small region of pixels from the input volume).\n",
    "\n",
    "CNNs are very good at capturing translational invariance i.e. a cat shifted over a few pixels is still a cat. The image below is from an [article by Jefkine Kafunah](https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/) illustrates both concepts of parameter sharing and sparsity of connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic11.png\" width=350/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">Since POOL layers have no learnable parameters, conventionally we combine a CONV and subsequent POOL and call them a single layer. A common architecture is several sequential instances of one or more CONV followed by a single POOL, then these are followed by several *Fully Connected* (FC) layers ending with a final logistic or softmax unit. The input volume for the first FC layer is flattened into a vector, and every neuron in the FC layer is densely connected to all the input elements, just as in classical NNs. As you go deeper $n_H$ and $n_W$ decrease while $n_c$ grows; overall the number of elements in the output volume (\"activation size\") will decrease, but if it drops too quickly that is typically bad for performance. Convolutions are useful because of *parameter sharing* (a feature detector (like vertical edge detector) that is learned in one part of the image is probably useful in other parts) and *sparsity of connections* (each element of the output volume depends on only a small number of inputs... a small region of pixels from the input volume).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 1: Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If we don't worry about trying to vectorize the computations, then the forward pass computation for a single CONV layer can be nicely organized as follows:\n",
    "\n",
    "## Forward Pass\n",
    "\n",
    "**Inputs:** \n",
    "- `A_prev`: the 4D output volume of the previous layer (vectorized so that the data from all samples are stacked together)\n",
    "    - Shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "- `W`: a 4D tensor consisting of all the 3D filter weight tensors for the layer (there are $n_c$ of these) stacked together\n",
    "    - Shape (f, f, n_C_prev, n_C)\n",
    "- `b`: a 4D tensor consisting of all the real-number filter biases for the layer stacked together\n",
    "    - (1, 1, 1, n_C)\n",
    "- `hparameters`: a dictionary with the stride and pad for the layer\n",
    "\n",
    "**Outputs:**\n",
    "- `Z`: the 4D tensor output of the CONV layer (before applying a the non-linear activation)\n",
    "    - Shape (m, n_H, n_W, n_C)\n",
    "- `cache`: Cache of the input values which is needed for back-prop (A_prev, W, b, hparameters)\n",
    "\n",
    "**Function:**\n",
    "- We initialize the output volume `Z` with zeros of the correct shape\n",
    "- We zero-pad the vertical and horizontal dimensions of input volume to get `A_prev_pad`\n",
    "- Loop through the samples in `A_prev_pad` (i):\n",
    "    - Loop over the vertical axis of the output volume (h):\n",
    "        - Loop over the horizontal axis of the output volume (w):\n",
    "            - Loop over the channels i.e. depth axis of the output volume (c) <- THIS MEANS LOOP OVER THE FILTERS OF THE LAYER! \n",
    "                - Each index pair (h, w) in the output volume corresponds to a location of overlaying a filter onto the input volume i.e. defining a slice of the input volume\n",
    "                - Multiply and sum this particular 3D region/slice (determined by $h$ and $w$) of the $i^{th}$ sample with the $c^{th}$ 3D filter, the result is a real number which belongs at the [i, h, w, c] index of the output volume\n",
    "\n",
    "The actual code for this function is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape (≈1 line)\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)\n",
    "    n_H = int((n_H_prev + 2*pad - f)/stride + 1)\n",
    "    n_W = int((n_W_prev + 2*pad - f)/stride + 1)\n",
    "    \n",
    "    # Initialize the output volume Z with zeros. (≈1 line)\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(0, m):                               # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i, :, :, :]                               # Select ith training example's padded activation\n",
    "        for h in range(0, n_H):                           # loop over vertical axis of the output volume\n",
    "            for w in range(0, n_W):                       # loop over horizontal axis of the output volume\n",
    "                for c in range(0, n_C):                   # loop over channels (= #filters) of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = stride*h\n",
    "                    vert_end = stride*h + f\n",
    "                    horiz_start = stride*w\n",
    "                    horiz_end = stride*w + f\n",
    "                    \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    \n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:, :, :, c], b[:, :, :, c])\n",
    "                                            \n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We have chosen to construct the forward pass as: for each sample, carve out the different 3D regions/slices from the input volume of that sample and apply all the $n_c$ filters to each slice.\n",
    "\n",
    "In backprop we will need to update all the weights in every 3D filter for a layer. If $W_c$ is the $c^{th}$ 3D filter for a given layer, we can construct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Recall that in general backprop each computational unit expects to be told what is the derivative of $J$ with respect to its own output. That is, the $l^{th}$ layer expects $\\frac{dJ}{dA^{[l]}}$ to be passed backward to it. The job of that layer is then to combine this information with the *local gradients* to get $\\frac{dJ}{dW^{[l]}}$, $\\frac{dJ}{db}$ for updating its parameters, as well as $\\frac{dJ}{dA^{[l-1]}}$ to pass backward to the underlying layer to keep the party going. More explciitly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "- Back Prop in the $l^{th}$ layer:\n",
    "    - input $dA^{[l]}$ passed from the overlying layer and cached values of $A^{[l-1]}$, $Z^{[l]}$, $W^{[l]}$, and $b^{[l]}$ from forward prop\n",
    "    - compute \n",
    "        - $dZ^{[l]} = dA^{[l]}*g'^{[l]}(Z^{[l]})$ as an intermediary\n",
    "        - $dW^{[l]} = \\frac{1}{m} dZ^{[l]}A^{[l-1]T}$\n",
    "        - $db^{[l]} = \\frac{1}{m} \\textrm{np.sum($dZ^{[l]}$, axis=1, keepdims=True)}$\n",
    "        - $dA^{[l-1]} = W^{[l]T} dZ^{[l]}$\n",
    "    - output\n",
    "        - $dA^{[l-1]}$ to feed backward to next layer\n",
    "        - $dW^{[l]}$, $db^{[l]}$ to use for GD parameter update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A CONV layer is no exception to this. The task is still to figure out the local gradients of the unit, and combine them with the input $dA^{[l]}$ in order to calculate the derivatives for parameter update in that layer and the derivative $dA^{[l-1]}$ to be passed backward to the underlying layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In backprop we will need to update all the weights (pixels) in every 3D filter for a layer. Consider a single filter matrix from a single layer ($w$) and zero in on one particular pixel of that filter $w_{m,n,1}$; for simplicity let's take it to live in the first channel and thus abbreviate to $w_{m,n}$. Recall that each 3D filter convolved on the 3D input gives a 2D output; a bias term is added and an activation function applied to each 2D output to give a 2D feature map, and all the 2D feature maps from all the filters of a layer are stacked together to form the full output volume. **The single filter weight pixel $w_{m,n}$ will contribute to the value of every single pixel in the corresponding 2D slice of the output volume.** Let's call the 2D output formed by the convolution plus bias $x$ and let its pixels be indexed by $x_{i,j}$. If we want to know how the cost function changes as we change this particular weight pixel we can use the chain rule as follows: sum over all the $x$ pixels of how much $J$ changes with an $x$ pixel times how much the $x$ pixel changes with the particular weight pixel $w_{m,n}$. That is\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial w_{m,n}} = \\sum_i \\sum_j \\frac{\\partial J}{\\partial x_{i,j}}\\cdot \\frac{\\partial x_{i,j}}{\\partial w_{m,n}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Local Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If we think about the convolution plus bias operation, then we can see that the quantity $\\frac{\\partial x_{i,j}}{\\partial w_{m,n}}$ will depend only on the value of a single pixel in the input volume that the filter is convolving over! See the image below where I have set stride=2 for image clarity. This input volume is the output from the previous layer, so we denote it $o^{[l-1]}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic12.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the example pictured above (using the convention of indexing starting at 0) we have $m,n = 2, 0$ and $i, j = 0, 3$ corresponding to a particular input pixel $o^{[l-1]}_{2, 6}$. In general we can compute the particular input pixel indices based on $i, j, m, n$ and stride $s$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial x_{i,j}}{\\partial w_{m,n}} = o^{[l-1]}_{i\\cdot s+m,\\; j\\cdot s+n}\\\\[1em]\n",
    "\\implies \\frac{\\partial J}{\\partial w_{m,n}} = \\sum_i \\sum_j \\frac{\\partial J}{\\partial x_{i,j}}o^{[l-1]}_{i\\cdot s+m,\\; j\\cdot s+n}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    ">**The gradient w.r.t to a single weight pixel of a filter is thus a sum over all the incoming gradients from each pixel of the corresponding output slice, each weighted by a particular pixel in the input volume.** In fact, this is exactly a convolution of a matrix of gradients with a matrix of the subset of input pixels that were \"touched\" by the weight during the forward pass. The gradients flow in backwards from the overlying layer, and are convolved with the input from the underlying layer.\n",
    "\n",
    "(Note, you might think that the $mn^{th}$ pixel of every filter in the layer will have the same gradient, but remember that $x$ is the particular 2D slice of the output volume generated by the specific filter $w$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This picture also illustrates that the same approach holds true for computing $\\frac{\\partial J}{\\partial o^{[l-1]}_{m, n}}$, namely the quantity $\\frac{\\partial x_{i,j}}{\\partial o^{[l-1]}_{m,n}}$ will depend only on the value of a single pixel in the filter matrix!\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial x_{i,j}}{\\partial o^{[l-1]}_{m,n}} = o^{[l-1]}_{i\\cdot s+m,\\; j\\cdot s+n}\\\\[1em]\n",
    "\\implies \\frac{\\partial J}{\\partial w_{m,n}} = \\sum_i \\sum_j \\frac{\\partial J}{\\partial x_{i,j}}o^{[l-1]}_{i\\cdot s+m,\\; j\\cdot s+n}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now are tasked with understanding how to get the incoming gradients $\\frac{\\partial J}{\\partial x_{i,j}}$. In the terminology of this nice graphic illustrating backprop / chain rule on a computational graph, we have just found a \"local gradient\" and now we need to know how to find the backward flowing gradient coming in from the overlying node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic13.png\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our picture looks more like the below (where I have put a gray shaded box over the region of the computation graph that we haven't addressed yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic14.png\" width=350/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note, many of these quantities are tensors so for sloppy shorthand we might write $\\frac{\\partial J}{\\partial x} = \\frac{\\partial J}{\\partial o^{[l]}} \\frac{\\partial o^{[l]}}{\\partial x}$, but what this really means is if you look at any one particular element of the matrix, $x_{i',j'}$ then \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial x_{i',j'}} = \\sum_{p} \\sum_q \\frac{\\partial J}{\\partial o^{[l]}_{p, q}} \\frac{\\partial o^{[l]}_{p,q}}{\\partial x_{i',j'}}\\\\[1em] \n",
    "=\\frac{\\partial J}{\\partial o^{[l]}_{i', j'}} \\frac{\\partial o^{[l]}_{i',j'}}{\\partial x_{i',j'}} = \\frac{\\partial J}{\\partial o^{[l]}_{i', j'}} f'(x_{i',j'})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The last step follows from the nature of the relationship $o^{[l]} = f(x)$ where $f$ is just the RelU function applied elementwise. Thus for each element of $o^{[l]}$, only one element of $x$ (the one in the corresponding index position) has any impact. Remember, the overlying layer is going to pass us back the values of $\\frac{\\partial J}{\\partial o^{[l]}_{p, q}}$ so assuming you are comfortable taking the derivative of RelU then this step is taken care of! Next we need to pass these values $\\partial x_{i,j}$ back through the next node, where together with the *local derivatives* they will let us calculate the values of $\\partial w_{i,j}$, $\\partial b$, and $\\partial o^{[l-1]}_{i,j}$. The first two are needed for parameter update, the last is needed to pass back to the next layer to keep ths party going."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To compute the local gradients of the node we look at the mathematical operation it performs: $x = w * o^{[l-1]} + b$. In principle, any or all of the matrix elements $\\partial x_{i,j}$ could contribute to the total derivative of a single component $w_{i,j}$ etc. So for the complete chain rule we need to sum over all of them and thus we need the local derivatives $\\frac{\\partial x_{p,q}}{\\partial w_{i,j}}$ for ever combination of index pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Resources\n",
    "- A great article giving an [intuitive intro to CNNs by Christopher Olah](http://colah.github.io/posts/2014-07-Conv-Nets-Modular/)\n",
    "- An excellent article giving a derivation of the math for [backprop in a CONV layer for stride = 1](https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/)\n",
    "- An article detailing [backprop for CONV layer when stride > 1](https://medium.com/@mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710)\n",
    "- Some more articles I haven't read yet:\n",
    "    - https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/\n",
    "    - http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "    - https://grzegorzgwardys.wordpress.com/2016/04/22/8/\n",
    "    - http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tensorflow Organization\n",
    "\n",
    "As a refresher, writing and running programs in TensorFlow has the following steps:\n",
    "\n",
    "1. Create Tensors (variables) that are not yet executed/evaluated. \n",
    "2. Write operations between those Tensors. (This defines the computational graph)\n",
    "3. Initialize your Tensors. \n",
    "4. Create a Session. \n",
    "5. Run the Session. This will run the operations you'd written above. \n",
    "\n",
    "Some things to keep in mind:\n",
    "\n",
    "- The two main object classes in tensorflow are Tensors and Operators. \n",
    "- When you code in tensorflow you have to take the following steps:\n",
    "    - Create a graph containing Tensors (Variables, Placeholders ...) and Operations on tensors (tf.matmul, tf.add, ...)\n",
    "       - A placeholder is an object whose value you can specify only later by passing in values to a session using a \"feed dictionary\" (`feed_dict` kwarg)\n",
    "    - Create a session and then Initialize the session (initial values for tensor objects)\n",
    "    - Run the session to execute the graph\n",
    "        - You can execute the graph multiple times: think of the session as a block of code to train the model - each time you run the session on a minibatch, it trains the parameters.\n",
    "        - The backpropagation and optimization is automatically done when running the session on the \"optimizer\" object.\n",
    "\n",
    "\n",
    "### Code Organization\n",
    "\n",
    "In our vectorized version of a CNN, each sample is an RGB image (3D matrix), and all the samples in a minibatch are concatenated along the **first** dimension to form a 4D matrix which is the input to the first layer. For multiclass classification each sample has a corresponding $y$ vector where the number of components is the number of classes. We first create two `tf.placeholder`s for these input $X$ and $Y$ matrices. The first dimension corresponds to the number of samples, for which we specify a size of `None` so that different numbers of samples may be used as needed.\n",
    "- `X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)` returns the two placeholder objects\n",
    "    - `X = tf.placeholder(tf.float32, shape=[None, n_H0, n_W0, n_C0])`\n",
    "    - `Y = tf.placeholder(tf.float32, shape=[None, n_y])`\n",
    "\n",
    "The filters for a given layer are likewise concatenated together along the **last** dimension to create a single 4D matrix $W$ that contains all the weights for a CONV layer. Since these are dynamic values in our graph, we use `tf.get_variable` to create them. We also specify the way their values should be initialized. Here there are 8 filters in CONV1 and 16 filters in CONV2\n",
    "- `parameters = initialize_parameters()` returns a dictionary of the two variable objects\n",
    "    - `W1 = tf.get_variable(\"W1\", [4, 4, 3, 8], initializer=tf.contrib.layers.xavier_initializer(seed=0))`\n",
    "    - `W2 = tf.get_variable(\"W2\", [2, 2, 8, 16], initializer=tf.contrib.layers.xavier_initializer(seed=0))`\n",
    "    \n",
    "The network itself is specified by a single function that defines the forward prop chain of computation. Note that the convolution and the non-linear activations are called as separate steps. This model follows each CONV with a max pool. Both these types of layers have stride inputs that specify a stride in every dimension of the input size (since the first dimension corresponds to the concatenation of samples, we give it stride of 1). After the two CONV->POOL segments, we flatten the output and feed it into a fully connected layer with 6 neurons; this performs only the linear part of the computation for an FC layer to get Z3, the non-linear part to get A3 is actually combined with the loss function in TF.\n",
    "- `Z3 = forward_propagation(X, parameters)`\n",
    "    - `W1 = parameters['W1']`\n",
    "    - `W2 = parameters['W2']`\n",
    "    - `Z1 = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='SAME')`\n",
    "    - `A1 = tf.nn.relu(Z1)`\n",
    "    - `P1 = tf.nn.max_pool(A1, ksize=[1,8,8,1], strides=[1,8,8,1], padding='SAME')`\n",
    "    - `Z2 = tf.nn.conv2d(P1, W2, strides=[1,1,1,1], padding=\"SAME\")`\n",
    "    - `A2 = tf.nn.relu(Z2)`\n",
    "    - `P2 = tf.nn.max_pool(A2, ksize=[1,4,4,1], strides=[1,4,4,1], padding=\"SAME\")`\n",
    "    - `P2 = tf.contrib.layers.flatten(P2)`\n",
    "    - `Z3 = tf.contrib.layers.fully_connected(P2, num_outputs=6, activation_fn=None)`\n",
    "\n",
    "As mentioned, the forward prop ends with the linear computation for the FC layer which gives Z3. The non-linear part to get A3 is actually combined with the loss function in TF. To get a single cost number we average across the loss of all the samples.\n",
    "- `cost = compute_cost(Z3, Y)`\n",
    "    - `cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3, labels=Y))`\n",
    "    \n",
    "The wrapper to fully specify and train the model calls the above helper functions to create the model, defines the optimizer to use, then initializes a session and runs it on minibatches\n",
    "- `train_accuracy, test_accuracy, parameters = model(X_train, Y_train, X_test, Y_test, learning_rate, num_epochs, minibatch_size)`\n",
    "    - `X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)`\n",
    "    - `parameters = initialize_parameters()`\n",
    "    - `Z3 = forward_propagation(X, parameters)`\n",
    "    - `cost = compute_cost(Z3, Y)`\n",
    "    - `optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)`\n",
    "    - `init = tf.global_variables_initializer()`\n",
    "    - For each minibatch:\n",
    "        - `_ , temp_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})`\n",
    "        - `minibatch_cost += temp_cost / num_minibatches`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 2: Case Studies\n",
    "\n",
    "## Vid: Why Case Studies\n",
    "Analagous to how you improve at coding by reading other people's good code. Also sometimes you can take a pre-existing succesful architecture and apply it to your problem. We'll look first at \"classic networks\" like AlexNet which contain ideas that lay the foundation for modern computer vision. Then we look at ResNet and Inception.\n",
    "\n",
    "## Vid: Classic Networks\n",
    "### LeNet-5\n",
    "Goal: recognize gray-scale handwritten digits. Use average pooling after each CONV which is less common now, and always used VALID convolutions (which shrink the dimensions). LeNet had 60k parameters. Some principles that have held up over time: \n",
    "- As you go deeper the height and width shrink while the number of channels grows\n",
    "- A pattern of CONV+POOL repeated several times then followed by FC layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic15.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### AlexNet\n",
    "Goal: classify RGB images into one of 1000 classes. AlexNet has 60 million parameters in contrast to the 60k in LeNet! This network uses RelU, while LeNet was still using sigmoid/tanh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic16.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### VGG-16\n",
    "AlexNet architecture involved layers with quite different hyperparameter values; VGG simplifies this by making all CONV layers use 3x3 filters with SAME convolutions and stride=1, while all POOL layers use 2x2 filter with stride=2. VGG has 138 million parameters, but at least the architecture is very uniform. Notice that the number of filters roughly doubles from one CONV segment to the next. Again, $n_H$ and $n_W$ go down while $n_c$ goes up as you go deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic17.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: ResNets\n",
    "Very deep NNs are difficult to train because of vanishing/exploding gradient problems; ResNets let us get around this problem. In a normal net the \"main path\" for activations $a^{[l]}$ to influence downstream layers is to proceed through a sequence of linear plus non-linear transformations. In ResNets we give $a^{[l]}$ a \"shortcut\" (or \"skip connection\") by simply adding it in to the $z$ of a further downstream layer before applying the non-linearity. For instance, $a^{[l+2]} = g(z^{[l+2]} + a^{[l]})$; the group of layers is called a *Residual Block* and a ResNet is comprised of many such blocks. Below are two examples of residual blocks (skipping 2 and 3 layers, respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic18.png\" width=550/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic24.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In plain NNs, often as you add more and more layers you find $J_{\\textrm{train}}$ actually decreases to a minimum but then begins to increase (!) because the optimization algorithm has a much harder time due to e.g. vanishing/exploding gradients. With ResNets the skip-connections helps the gradient to backpropagate to earlier layers which addresses this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Why ResNets Work\n",
    "First let's show that adding a residual block on the end of a large network will not hurt performance. Consider a large network ending in output $a^{[l]}$. Append a residual block to this so that instead we output $a{[l+2]}$ where there is a skip connection from $a^{[l]}$ through to the $l+2^{th}$ layer. We will have $a^{[l+2]} = g(z^{[l+2]} + a^{[l]}) = g(W^{[l+2]}a^{[l+1]} + b^{[l+2]} + a^{[l]})$. Because we are using RelU activation, we know that $a \\geq 0$ for all layers. Since RelU is the identity for non-negative inputs, the network can very easily turn the residual block into simply an identity function $a^{[l+2]} = a^{[l]}$ by learning zero weights and bias for the $l+2^{th}$ layer. Thus we can always mimic the smaller network if that is optimal, yet we still have the option for the residual block to instead learn something more complex if that is helpful.\n",
    "\n",
    "Notice that in a residul block we need $z^{[l+2]}$ to have the same dimension as $a^{[l]}$, so in ResNets it is common to use a SAME convolution to avoid reshaping outputs. If the two quantities have different dimension, you can add an extra parameter matrix $W_s$ tha multiples $a^{[l]}$ in order to reshape it as needed: $a^{[l+2]} = g(z^{[l+2]} + W_s a^{[l]})$. The matrix $W_s$ can e.g. be implemented so that it simply pads out with zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">ResNets allow us to train deeper nets without encountering exploding/vanishing gradients. We give $a^{[l]}$ a \"shortcut\" (\"skip connection\") by adding it to the $z$ of a downstream layer e.g. $a^{[l+2]} = g(z^{[l+2]} + a^{[l]})$; the group of layers is called a *Residual Block* and a ResNet is comprised of many such blocks. It is easy for a residual block to learn the identity function by setting weights to zero, thus appending residual blocks can only *improve* performance relative to the shallower underlying net.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Networks in Networks and 1x1 Convolutions\n",
    "For volume inputs, a 1x1 convolution will take each depth-row of the input and multiply it elementwise by the single depth-row comprising the filter and then sum the values and RelU the sum. This is as though you have a single neuron taking the input volume depth-rows as fully-connected inputs. With multiple 1x1 filters you have many such fully-connected neurons in the CONV layer, so this is often called \"network in network\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic19.png\" width=450 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Pooling layers can be used to shrink just $n_H$ and $n_W$ while the 1x1 convolution can shrink the change of channels (depth) without changing height or width. Aside from this function, 1x1 layers just add nonlinearity to the network allowing you to learn more complex things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Inception Network Motivation\n",
    "An inception layer allows you to \"try\" a variety of different transformations for a single layer. If you want to try 1x1, 3x3 SAME, 5x5 SAME convolution options and a MAXPOOL option, an inception layer performs them all and then concatenates the individual output volumes! Note that this would need to be a modified pool operation that uses padding in order to make sure all the concatenated volumes retain the same height x width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic20.png\" width=450 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Inception layers have the disadvantage of high computational cost. For instance, for an input volume of 28x28x192 the 5x5 SAME convolution with 32 filters pictured above requires about 120 million multiplications to compute the individual output volume. Instead we can do a sequence of two convolutions: 1x1 with 16 filters to yield 28x28x16 followed by 5x5 SAME with 32 filters to yield 28x28x32. This achieves the same output shape as before with only 12 million (reduced by a factor of 10)! The 1x1 convolution is called a *Bottleneck Layer* since it yields the smallest volume in the pipeline. It turns out that the bottleneck effect of temporarily shrinking the representation doesn't really hurt performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic21.png\" width=450/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Inception Network\n",
    "An inception module takes as input the activation volume from the underlying layer and independently performs several different types of SAME CONVs and a SAME POOL, concatenating those individual outputs to form one large output. For the non-1x1 convolutions it uses a bottleneck layer (1x1 CONV with small number of filters) preceeding the normal CONV to help reduce computation cost. It also adds a 1x1 CONV after the pool operation to shrink the number of channels. The inception network stacks many such inception modules, ending in an FC and then softmax output. The architecture has one additional complication which is a few \"side branches\" (not pictured below) that attempt to make a prediction (FC plus softmax layers) from the activation output of an intermediate module; these predictions are incorporated into the cost function and the result is a regularizing effect the makes sure that earlier learned features remain useful for predicting the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic22.png\" width=850 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">A single 1x1 CONV filter behaves like a single neuron that is fully connected to each of the depth-rows of the input volume in turn. A 1x1 CONV layer is thus like having a \"network in a network\" and adds complexity, it is also used to adjust the number of channels in the volume without changing $n_H$ or $n_W$. A 1x1 CONV layer with a small number of filters can be used as a *bottleneck layer* preceeding a larger-filter CONV operation; by temporarily shrinking the depth of the volume it greatly reduces the computation cost to achieve the same output volume. A bottleneck layer can also be added after a SAME POOL operation in order to shrink the depth. An inception module is a layer that performs a variety of different CONVs and a POOL (all with bottlenecks to reduce computation) on the input volume, and then concatenates the output volumes of those individual operations; in this way you get to \"try out\" a variety of different operations for each layer. The inception network stacks many such inception modules.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 2: Practical Advice for Using CNNs\n",
    "\n",
    "## Vid: Using Open Source Implementation\n",
    "It can be difficult to replicate someone's architecture just based on a paper due to many details about tuning hyperparmeters etc. Many authors provide open-source implementation of their code online e.g. on GitHub. You can simply use `git clone <URL>` in your terminal with the URL you find on the GitHub page for the project. This will pull down a copy of all the files onto your PC. Often these implementations are pre-trained which can be helpful\n",
    "\n",
    "## Vid: Transfer Learning\n",
    "There are many very large open-source public datasets for images. Often open-source implementations of CNNs have been pre-trained extensively (with GPUs, for a long time) on these large datasets and you can apply transfer learning. For example, if you have two cats \"Tigger\" and \"Misty\" and you want a classifier to label images as one or the other or neither (3 classes). You can use an open source net trained on e.g. CIFAR which has 1000 classes, and replace just the last layer with a 3-class softmax and retrain only the new layer. Many open source implementations will have a keyword like `freeze` or `trainableParameter` that you can use to indicate whether the weights for a particular layer should be trained. You can actally run the net minus the softmax layer on all your images and save the outputs, then just train a shallow softmax model on those outputs. If you have a more substantial data set of \"Tigger\" and \"Misty\" pics, you can train (unfreeze) a larger number of deeper layers beyond just the output. With enough data you can train the entire network, where the downloaded weights just serve as initialization.\n",
    "\n",
    "## Vid: Data Augmentation\n",
    "More data improves performance for almost all computer vision tasks. Some common techniques:\n",
    "- Mirroring on the vertical axis (simplest method)\n",
    "- Random cropping to generate a few reasonably large subsets of the original image\n",
    "- Rotation, Shearing, Local warping are also possible, but not used as much as the first two\n",
    "- Color shifting by adding different values to the RGB channels e.g. +20/-20/+20. In pracice you randomly draw these additive values from some distribution. This reflects the fact that lighting strongly affects picture color, but doesn't change the underlying class.\n",
    "\n",
    "If you have a very large data set you don't want to just create augmented data and store it on disk. Instead you can use a CPU thread to load data from disk and also implement the distortions to form a minibatch. The minibatch can be passed to another thread (or GPU) for training.\n",
    "\n",
    "Notice that most data augmentation techniques involves choosing some hyperparameters for the transformation (e.g. how much to color shift). A good place to start is to use an open source implementation of data augmentation where someone has tuned these for you.\n",
    "\n",
    "## Vid: State of Computer Vision\n",
    "Compared to speech recognition, image recognition (classify image) has less data relative to the task complexity, and object detection (label and draw bounding box) has even less. When you have a lot of data you can use simpler algoritms with less hand-engineering of features and architectures, the lack of sufficient data is one reason computer vision field has spawned so many complicated architectures. \n",
    "\n",
    "In computer vision there is also a lot of emphasis on doing well on benchmarks - this is nice for reproducibility, but sometimes leads to techniques that you wouldn't use in practice. For instance, *Ensembling* (independently training several NNs and averaging their outputs) will often gain you a few percentage points in performance, but would be too computationally expensive for most real-world applications. Another perhaps impractical technique is *Multicrop* at test time which applies a set of crop transformations to each test image then averages your model's prediction on the set of crops to get a single prediction for each test sample. The 10-crop is a common implementation of this (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic23.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> Use architectures published in literature. Use open source implementations if possible. Use pre-trained models and fine-tune on your data set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">Use architectures published in literature, use open source implementations if possible, and use pre-trained models then fine-tune on your data set! For transfer learning, copy a pre-trained implementation and replace the softmax output with an output layer for your number of classes. With small amounts of data train only the weights in the new output layer, with larger amounts of data you can also retrain increasingly shallow layers. More data almost always improves computer vision performance: vertical axis mirroring, random cropping and random color shifting are the most common techniques for expanding your training set. To improve performance on benchmarks or competitions you can use *ensembling* (averaging the output of several independently trained NNs) and *multicrop at test time* (averaging the output on a set of cropped transformations of a test sample). When you have a lot of data you can use simpler algoritms with less hand-engineering of features and architectures, the lack of sufficient data is one reason computer vision field has spawned so many complicated architectures.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 2: Homework\n",
    "\n",
    "## Keras\n",
    "Keras uses a different convention with layer variable names: rather than creating a new variable on each step of forward prop (e.g. Z1, A1, Z2), each computation step just reassigns X to a new value e.g. `X = keras_func(kwargs)(X)`. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def HappyModel(input_shape):\n",
    "    \"\"\"\n",
    "    Implementation of the HappyModel.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Zero-Padding: pads the border of X_input with zeroes\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((2, 2), name='max_pool')(X)\n",
    "\n",
    "    # FLATTEN X (means convert it to a vector) + FULLYCONNECTED\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(1, activation='sigmoid', name='fc')(X)\n",
    "\n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = X, name='HappyModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For training / testing there are four steps:\n",
    "1. Create the model by calling the function above\n",
    "2. Compile the model by calling `model.compile(optimizer = \"...\", loss = \"...\", metrics = [\"accuracy\"])`\n",
    "3. Train the model on train data by calling `model.fit(x = ..., y = ..., epochs = ..., batch_size = ...)`\n",
    "4. Test the model on test data by calling `model.evaluate(x = ..., y = ...)`\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "happyModel = HappyModel((64,64,3))\n",
    "happyModel.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "happyModel.fit(x=X_train, y=Y_train, epochs=10, batch_size=64)\n",
    "preds = happyModel.evaluate(x = X_test, y = Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Two useful commands to inspect / visualize your achitecture are `happyModel.summary()` which gives your layers in table form and `SVG(model_to_dot(happyModel).create(prog='dot', format='svg'))` which gives a graphical representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ResNet Implementation\n",
    "An *identity block* is the standard residual block where the input ($a^{[l]}$) has the same shape as the output $a^{[l+N]}$ where $N$ is the number of skipped layers. A *convolution block* is a residual block where there is a shape mismatch between the input and output, so the shortcut path needs to involve a reshaping convolution. Note there is no non-linearity applied after the convolution on the shortcut path!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic25.png\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For instance, one specific implementation of a skip-3 identity block is as follows (a convolution block would be defined very similarly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: identity_block\n",
    "\n",
    "def identity_block(X, f, filters, stage, block):\n",
    "    \"\"\"\n",
    "    Implementation of the identity block as defined in Figure 4\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value. You'll need this later to add back to the main path. \n",
    "    X_shortcut = X\n",
    "    \n",
    "    # First component of main path\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1,1), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "        \n",
    "    # Second component of main path (≈3 lines)\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1,1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1,1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A full ResNet model called \"ResNet50\" is pictured below, followed by the necessary code to implement it (where hyperparameters of the layers are as given in the code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic26.png\" width=700/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ResNet50(input_shape = (64, 64, 3), classes = 6):\n",
    "    \"\"\"\n",
    "    Implementation of the popular ResNet50 the following architecture:\n",
    "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
    "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
    "    \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Stage 3 (≈4 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    # Stage 4 (≈6 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    # Stage 5 (≈3 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
    "    X = AveragePooling2D((2, 2), name='avg_pool')(X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 3: Object Detection\n",
    "\n",
    "## Vid: Object Localization\n",
    "In \"classification with localization\" we are responsible for labeling an image as e.g. \"car\" and also drawing a bounding box around the object within the image; typically there is one large object in the image. In \"object detection\" we may have multiple objects (possibly of different classes) within the same image and need a bounding box around each of them. \n",
    "\n",
    "For classification with localization we need to modify our network to have not only a softmax classification output node, but also nodes in the last layer that output the bounding box which is given by four numbers which specify the center point $(b_x, b_y)$ and the height $b_h$ and width $b_w$ of the box. We use the convention where the upper left of the image is $(0,0)$ and the lower right is $(1,1)$. For an example problem having three classes (e.g. \"car\", \"pedestrian\", \"motorcycle\")we define our label vector as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "y = \\big[p_c, b_x, b_y, b_h, b_w, c_1, c_2, c_3\\big]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here the $c_i$ are the typical binaries giving for whether the image is of each different class, while $p_c$ is a binary that reflects whether there is any object of those classes at all (\"any of the above classes\" i.e. \"background\"). Remember that for this problem only one object of one type will be in any of our image. For background images $y$ will have $p_c = 1$ and the rest of the components are irrelevant. To simplify thing we can use squared error loss for all components so that:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\hat{y}, y) = \\begin{cases} \n",
    "          \\sum_{i=1}^8 (\\hat{y}_i - y_i)^2 & \\textrm{if $p_c=1$} \\\\[1em]\n",
    "          (\\hat{y}_1 - y_1)^2 & \\textrm{if $p_c=0$}\n",
    "       \\end{cases}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In practice you could use e.g. squared error loss for the bounding box components, log likelihood loss for the $c_i$ components, and logistic regression loss for the $p_c$ component "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Landmark Detection\n",
    "In some cases you want a NN to output coordinates $(l_{ix}, l_{iy})$ of some \"landmarks\" in your image. For example, locating the four corners of a person's eyes in face images. You could even specify a large number of landmarks $l_i$ that would allow you to extract e.g. the shape of the eyes and shape of the mouth. This kind of algorithm is useful for e.g. instagram filters that distort a person's face or put a hat on them. Of course this requires a large dataset where someone has laboriously hand-annotated these landmarks onto images. Other algorithms look at \"pose detection\" based on landmarks across a person's body like center of chest, left shoulder, right shoulder etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic27.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Object Detection\n",
    "A popular technique for object detection is called *Sliding Window Detection*. First we start with a training set of images that are very closely cropped around the object of interest and we train a CNN to do image classification on this set. For object detection on a full-sized image with objects and background, you use a small window that defines a small patch on your image and you input just that patch into your object classifier, slide the window over a little and repeat until the window has moved over every region of the full image. You then repeat this process over the full image using increasingly large windows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic28.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we use CNN's for image tasks rather than simpler linear models, this method has a huge disadvantage of very high computation cost (and if you use a large stride for the window movement you may hurt performance). Luckily there is a solution using a convolutional implementation of this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Convolutional Implementation of Sliding Windows\n",
    "Consider a CNN terminating in two FC layers and then a softmax that outputs four numbers for four class probabilities. It is possible to turn the FC layers in CONV layers: for an FC layer with 400 nodes taking an input volume of 5x5x16, each individual node will multiply each pixel of the input volume by a weight and sum the results. The identical operation can be achieved with 400 CONV filters of size 5x5, which will output a 1x1x400 volume. The second FC layer can now be implemented with 400 CONV filters of size 1x1, and the final output layer can be implemented as 4 filters of size 1x1 followed by a softmax activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic29.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Consider a classifier implemented completely with CONVs that takes in 14x14x3 images, and you would like to use this for sliding window detection on 16x16x3 images. Using a stride of 2 you would want to slide your 14x14 window over four different positions on the full sized image, feeding each patch to your image classifier. But notice that a large part of the computation for each of these four positions is the same between them. It turns out that if you feed your full 16x16x3 image into your classifier the result will be a 2x2x4 volume rather than a 1x1x4 volume, but each of the four depth-rows will correspond to the result of running the classifier on the corresponding 14x14 window patch! Thus you can compute the probabilities for every position of the sliding window on a single forward pass through the CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic30.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This algorithm still has a weakness that it will not give very accurate positions of the bounding box because for larger strides, none of the window positions may match up perfectly with the position of the object, and the bounding box shape may be different than the window shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">For **Classification with Localization** we want to input an image containing only one or zero objects and output $\\hat{y} = \\big[p_c, b_x, b_y, b_h, b_w, c_1..., c_N\\big]$ which gives the probabilities for the possible classes (${c_i}$) and \"any of the above classes present\" ($p_c$), with the center point ($b_x, b_y$) and height/width ($b_h, b_w$) of the object's bounding box. A typical CNN can be trained to do this with a modified output layer and appropriate loss function and labeled dataset. For **Landmark Detection** we want to input an image containing one object of a specific type and output $\\hat{y} = \\big[l_{1x}, l_{1y}, l_{2x}, l_{2y}..., l_{Nx}, l_{Ny}\\big]$ which gives the coordinates for different types of landmarks in the image e.g. corners of eyes, center of chest. Again, a typical CNN can do this with an appropriate output layer and labeled dataset. If images can contain multiple objects we perform **Sliding Window Object Detection** where a small window is strided across the input image, and each window patch is fed in turn into an image classifier that has been trained on images that are closely cropped around the object. To avoid high computation cost of this approach we use a convolutional implementation: the patch classifier CNN is built with FC layers implemented as convolutions, and the final softmax ouput implemented as a 1x1xN output volume. We can then feed the full sized image into the patch classifier and the result will be an MxMxN output volume where each depth row gives the result of running the patch classifier at the corresponding window location (where there are MxM such locations for the window being strided across the input image).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Bounding Box Predictions\n",
    "You Only Look Once (YOLO) is an algorithm that improves on the bounding box problem. For labeling data to train this algorithm, first superimpose a somewhat fine square grid on the full input image. Use the classification with localization approach (remember, this uses a label vector like $y = \\big[p_c, b_x, b_y, b_h, b_w, c_1, c_2, c_3\\big]$) and apply this label scheme to each cell in the grid. **Here we only consider $p_c=1$ if the center point of the object falls in the grid cell.** Our bounding box coordinates are specified relative to the indvidual grid cell where the convention is top left corner of the grid cell is $(0,0)$ and bottom right corner is $(1,1)$. Because of our labeling rules, $b_x$ and $b_y$ will always be between 0 and 1, but $b_h$ and $b_w$ can span past the cell borders (there are some more advanced parameterizations for these values that work better than our basic approach). \n",
    "\n",
    "Using a 19x19 grid and having 3 object classes (giving 8 components in each label vector) we can arrange the label vectors into a 19x19x8 label volume. Now we just choose the layers of our CNN so that we input the full image size and end up with a 19x19x8 output volume, and then we train on this labeled data! This works well if you have just a single object within any grid cell. Notice that we now get precise coordinates of bounding boxes, while still using a convolutional implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic31.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Intersection Over Union\n",
    "How do we evaluate object localization performance? The intersection over union (IoU) is a measure of the overlap between two boxes: you divide the size of the intersection of the predicted vs. actual box by the union of the two. If the localization is perfect IoU = 1, but IoU > 0.5 is often considered as \"correct\" for mapping to accuracy metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic32.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Non-max Suppression\n",
    "One of the drawbacks of basic YOLO object detection is your algorithm may detect an object multiple times (i.e. within adjacent grid cells); non-max suppression ensures each objet is detected only once. Consider a 19x19 grid, several adjacent small cells in the region of a car object may have large $p_c$ values, and this may occur in multiple regions of the image that have other cars. First we ignore/discard all boxes with a $p_c < 0.6$. Then we locate the cell with the highest $p_c$ value, save it as a prediction, and discard any remaining boxes that have an $IoU > 0.5$ with it's box. We repeat this until every cell has been either discarded, or saved as a prediction. The result is we supress all boxes that do not have locally maximum $p_c$ values. If you have multiple possible classes then we perform this operation for each class in turn, examining $p_c\\cdot c_i$ as the relevant quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic33.png\" width=350/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Anchor Boxes\n",
    "Another issue with basic YOLO is that each grid cell can detect only one object. Anchor boxes are a way to encode the presence of multiple overlapping objects of different classes at the same location. We first define a set of anchor boxes, whose shapes are similar to the shapes of our different object classes. For a single grid cell, each anchor box is centered on the grid cell center, and we now look within each anchor box region to define a label vector $y$ for that anchor box. Specifically, in labeling images an object will be assigned to the grid cell that contains it's midpoint and the anchor box that has the highest IoU with it's ground truth bounding box. The label vectors for all the anchor boxes are concatenated together to give the full label vector for that grid cell. In the image below, in the $8^{th}$ grid cell's label vector the components corresponding to anchorbox1 will reflect the presence of a \"pedestrian\" class object, while the components corresponding to anchorbox2 will reflect the presence of a \"car\" class object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic34.png\" width=550 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note there are some cases that will break this method, e.g. two objects with midpoints in the same cell and having similar shape so they have the highest IoU with the same anchor box. You need to implement some default tie-breaker for these cases. In practice, with decently fine grids the chance of two object midpoints being in the same grid cell is pretty low. But in fact the anchor box approach allows your algorithm to specialize more e.g. some units can specialize in recognizing tall/skinny objects vs. other units specializing in wide objects. Often anchor boxes are chosen by hand, a more advanced way is to use KMeans to group together all the object shapes present in your dataset to get a sense of the few representative shapes.\n",
    "\n",
    "## Vid: Complete YOLO Algorithm\n",
    "Consider training an algorithm for multiple object detection among 3 possible classes (pedestrian, car, motorcycle), and you will use a 3x3 square grid with 2 anchor boxes for labeling. Each label vector associated with a single anchor box patch would have 8 components, so the full label vector for each grid cell will have 16 components, thus your label volume for an image will be 3x3x16. So you construct a CNN that inputs a 100x100x3 image and outputs a 3x3x16 volume. You take the output volume and apply non-max suppression independently for each class by considering $p_c$.\n",
    "\n",
    "\n",
    "## Vid: Region Proposals (Optional)\n",
    "In many images there are regions of the image that are obviously devoid of interesting objects. The R-CNN algorithm uses a *segmentation algorithm* to propose only a subset of regions in the image to run your full CNN on. This is actually still quite slow. It can be improved with a convolutional implementation that classifies all the \"proposed regions\" simultaneously (Fast R-CNN), and even by using a CNN to do the region proposal set (Faster R-CNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic35.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">The **YOLO** algorithm improves on the convolutional sliding window approach by also giving us precise object bounding boxes. First we label our dataset as follows: superimpose a relatively fine square grid on each full image and within each grid cell generate a label for classification with localization (where $p_c=1$ only if the object's midpoint falls in the cell) then arrange these label vectors into a volume. For example, with a 19x19 grid and 3 possible classes this would give a 19x19x8 label volume. Now simply choose the layers of the YOLO CNN such that we can input the full image, and output a 19x19x8 volume, and train on this labeled dataset. The complete YOLO algorithm incorporates two additional techniques. (1) **Non-max suppression** can be used class-by-class to clean up the output predictions of YOLO when multiple adjacent grid cells all strongly believe they contain an object midpoint: First discard all cells with $p_c c_i < 0.6$, then locate the cell with the highest $p_c c_i$ and save the associated bounding box prediction, now discard any cells that have an $IoU > 0.5$ with this saved box. Repeat this until every cell has been either discarded, or saved. (2) **Anchor Boxes** let us encode the presence of multiple overlapping objects of different classes at the same location. We choose a set of anchor boxes of different shapes, and when labeling images an object will be assigned to the grid cell that contains it's midpoint and the anchor box that has the highest IoU with it's ground truth bounding box. The label vectors for all the anchor boxes are concatenated together to give the full label vector for a grid cell.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 3: Homework\n",
    "Here we load a pretrained YOLO model in Keras and write functions that extend the model with layers that perform non-max suppression filtering to output our final best predictions. Then we write a predict function that will run a single image through our complete graph and output the final predictions. We call our extension functions within an interactive session, and pass that same session into the predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = K.get_session()  # Interactive session\n",
    "class_names = read_classes(\"model_data/coco_classes.txt\") \n",
    "anchors = read_anchors(\"model_data/yolo_anchors.txt\")\n",
    "image_shape = (720., 1280.)\n",
    "yolo_model = load_model(\"model_data/yolo.h5\")  # pretrained model\n",
    "yolo_outputs = yolo_head(yolo_model.output, anchors, len(class_names))  # extend the graph of the pretrained model \n",
    "scores, boxes, classes = yolo_eval(yolo_outputs, image_shape)  # extend the graph of the pretrained model some more\n",
    "out_scores, out_boxes, out_classes = predict(sess, \"test.jpg\")  # run the extended graph on a single image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 4: Face Recognition\n",
    "\n",
    "## Vid: What is Face Recognition\n",
    "The *Face Verification* task is given a name or ID and an input image, output whether the image does correspond to the name. The *Face Recognion* task is given a database of K persons with IDs and an input image, output the ID of the matching person (or \"not recognized\"). The latter is more difficult, because you are applying face verification $K$ times and thus have $K$ times the probability of making a mistake - you might require a face verification accuracy of e.g. 99.9%. \n",
    "\n",
    "## Vid: One Shot Learning\n",
    "Building face verification systems is difficult because you often need to solve a \"one-shot\" learning problem. This means you need to train an algo to recognize a person given just one example image of their face (say, their ID picture on file), and historically DL algos don't work well with limited examples. Instead we train an algorithm to learn a *similarity function* which takes in two images and outputs the degree of difference $d$ between them e.g. two images of the same person should output a small number. Then at recognition time if $d(im1, im2) < \\tau$ you predict these are the same person. \n",
    "\n",
    "## Vid: Siamese Network\n",
    "This is an architecture that can learn the similarity function $d$. We construct a typical CNN that takes in image1, $x^{(1)}$ and outputs a 128-vector $f(x^{(1)})$ which we think of as an \"encoding\" of image1. We feed image2 into the same CNN and define\n",
    "\n",
    "\\begin{align*}\n",
    "d(x^{(1)}, x^{(2)}) = \\Bigm\\lvert \\Bigm\\lvert f(x^{(1)}) - f(x^{(2)})\\Bigm\\rvert \\Bigm\\rvert _2^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Triplet Loss\n",
    "Generally we train a Siamese network by trying to make $d$ small when image 1 and 2 are of the same person, and large otherwise. This is achived with the **triplet loss** objective function which takes in three images: \"anchor\" image ($A$), a \"positive\" match to the anchor ($P$), and \"negative\" non-match to anchor ($N$). Our goal is to have $d(A, P) \\leq d(A, N)$. However a function that gives zero everywhere trivially satisfies this, so we modify our goal slightly to $d(A, P) - d(A, N) + \\alpha \\leq 0$ for some small *margin* value $\\alpha$. Formally the triplet loss is\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(A, P, N) = \\max\\bigg( \\bigm\\lvert \\bigm\\lvert f(A) - f(P)\\bigm\\rvert \\bigm\\rvert ^2 - \\bigm\\lvert \\bigm\\lvert f(A) - f(N)\\bigm\\rvert \\bigm\\rvert ^2 + \\alpha, 0 \\bigg)\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For this kind of cost function our \"number of samples\" $m$ is really number of distinct triplets. If we index each of the possible triplets by $i$ then the cost is \n",
    "\n",
    "\\begin{align*}\n",
    "J = \\sum_i^m \\mathcal{L}(A^{(i)}, P^{(i)}, N^{(i)})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For training you obviously now need a dataset that contains multiple pictures of the same person. Given your data set of e.g. 10k images of 1k different persons, how should you choose these triplets to form the training set? If you choose triplets randomly, the learning task is not very difficult because most people look very different. Instead you want to choose triplets that are \"hard\" to distinguish i.e. $d(A, P) \\approx d(A, N)$. Note that for commercial face recognition systems data sets with 10 million or even 100 million images are not uncommon. Luckily, some of these companies have open-sourced their pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Face Verification and Binary Classification\n",
    "An alternative to using the triplet loss is to run the siamese network (i.e. two identical CNNs) on two images to generate the two encodings $f(x^{(i)})$ and $f(x^{(j)})$, and then feed these encodings into a final logistic regression unit to make a simple binary prediction. You LR unit can have additional parameters $W$ and $b$ and compute the prediction as e.g. \n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{y} = \\sigma\\bigg( \\sum_k W_k \\bigm \\lvert f(x^{(i)})_k - f(x^{(j)})_k \\bigm \\rvert + b \\bigg)\n",
    "\\end{align*}\n",
    "\n",
    "There are some alternative formulations for forming the predictor. Notice that for this approach to training we take in samples that are pairs of images rather than triplets, and the target labels are 0 or 1 for each pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic36.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">The *Face Verification* task is given an ID and an input image, output whether the image corresponds to the ID. The *Face Recognion* task is given a database of $K$ IDs and an input image, output the ID of the matching person; this can be done by applying face verification $K$ times. Rather than requiring many images of our specific target persons in order to train an algo to recognize them, instead we train a *Siamese Network* that learns a *similarity function* which takes in two images and outputs the degree of difference $d$ between them. For training, we use triplets of images containing an \"anchor\", a \"positive\" match to the anchor, and a \"negative\" non-match to anchor, and to each one apply an identical CNN whose output layer gives a vector encoding $f$ for an image. The *triplet loss* seeks to make the similarity between A and P larger by some margin $\\alpha$ than the similarity between A and N, and is formalized as $\\mathcal{L}(A, P, N) = \\max\\bigg( \\bigm\\lvert \\bigm\\lvert f(A) - f(P)\\bigm\\rvert \\bigm\\rvert ^2 - \\bigm\\lvert \\bigm\\lvert f(A) - f(N)\\bigm\\rvert \\bigm\\rvert^2 + \\; \\alpha, \\;0 \\bigg)$. For training this algorithm you should select mostly \"hard\" triplets, and for commercial systems image sets of 100 million are not uncommon. Note that one alternative approach is to train the Siamese Network on pairs of images, where the two encodings $f$ are fed into a final logistic unit for a binary output indicating \"same person\" or not. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 4: Neural Style Transfer\n",
    "\n",
    "## Vid: What is Neural Style Transfer?\n",
    "Neural style transfer lets you take in a \"content\" image ($C$) and a \"style\" image ($S$) and output a new image ($G$) which contains the content repainted in the particular style. To implement this, we need to examine the features extracted by a CNN at both the shallow and deeper layers.\n",
    "\n",
    "## Vid: What are Deep CNNs Learning?\n",
    "How can we visualize what different hidden units in different layers in a CNN are doing? Let's consider the very first hidden layer: pick a single hidden unit (filter) and scan your training set to find the images that maximize that filters activations, and plot the small image patches of those images that gave those largest activation numbers. Repeat this for other hidden units. An example of this procedure is given below, where 9 image patches are plotted for each of 9 hidden units; obviously some units are responding to e.g. a diagonal edge in a particular location of the \"receptive field\", or the color green etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic37.png\" width=250/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can repeat this basic idea for deeper hidden layers (although the details of how to identify the \"patch\" of the original image that resulted in the actvation is non-trivial).\n",
    "\n",
    "## Vid: Cost Function\n",
    "To actally build a neural style transfer system we need to define a cost function for the generated image. The \"goodness\" of a generated image has two parts which can be weighted differently: the first part measures how similar $G$ is to $C$ and the second measures how similar $G$ is to $S$. \n",
    "\n",
    "\\begin{align*}\n",
    "J(G) = \\alpha J_{\\textrm{content}}(C, G) + \\beta J_{\\textrm{style}}(S, G)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To generate a new image you initialize $G$ randomly then use gradient descent to minimize $J(G)$ by updating the pixel values of $G$. The computation of $J$ involves running the three images $G$, $C$ and $S$ through a pre-trained image CNN such as VGG and comparing the activations at some chosen layer.\n",
    "\n",
    "## Vid: Content Cost Function\n",
    "To compute the content component of $J$ we select typically a middle layer $l$ of the pretrained CNN to examine. Let $a^{[l](C)}$, $a^{[l](G)}$ be the activations of that layer when the pretrained net is run on the two images $C$ and $G$. The cost should reflect how \"similar\" these two images are, which we achieve with\n",
    "\\begin{align*}\n",
    "J_{\\textrm{content}}(C, G) = \\frac{1}{2} \\bigm\\lvert \\bigm \\lvert a^{[l](C)} - a^{[l](G)}\\bigm \\rvert \\bigm \\rvert^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Style Cost Function\n",
    "What do we mean by \"style\" of an image? Consider a single layer $l$. We define style as the pairwise correlation between activations across different channels (filter outputs) in that layer's output volume. Pair-wise correlation of activations between two channels means that whenever a particular region of the image is activating (deactivating) one channel it is also activating (deactivating) the other channel. Consider visualizing the \"role\" of individual units (AKA filters AKA channels) as we did above. In the visualization example pictured below for a relatively deep layer, the unit visualized in the top-middle block is looking for vertical stripe patterns while the unit visualized in the in the left-center block is looking for the color orange. These two units (channels) would have a high correlation if patches of the image that have vertical stripe patterns also tend to have the color orange. So this measure of \"style\" (for a properly chosen $l$) tells us which types of \"textures\" tend to occur or not occur together in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic38.png\" width=250/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Given an image, the *style matrix* $G^{[l]}$ (also called \"Gram Matrix\") for a given layer is a symmetric square matrix ($n_c^{[l]} \\times n_c^{[l]}$) whose elements give the pairwise correlation between channels in $l$. Let $i, j, k$ index into the activation output volume of the layer so $a^{[l]}_{i,j,k}$ gives a single pixel number in the output volume, then\n",
    "\n",
    "\\begin{align*}\n",
    "G^{[l]}_{kk'} = \\sum_i \\sum_j a^{[l]}_{i,j,k} a^{[l]}_{i,j,k'}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We compute the style matrix from the pre-trained net for both $G$ and $S$, and we define\n",
    "\n",
    "\\begin{align*}\n",
    "J_{\\textrm{style}}^{[l]}(S, G) = \\frac{1}{2n_H^{[l]}n_W^{[l]}n_c^{[l]}}\\bigm\\lvert \\bigm \\lvert G^{[l](S)} - G^{[l](G)}\\bigm \\rvert \\bigm \\rvert^2_F\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In practice you get more visually pleasing results results if you define style at a variety of layers to take both lower and higher level features into account. So we define the full style component of the cost to be an average of layer style costs weighted by new hyperparameters $\\lambda^{[l]}$.\n",
    "\n",
    "\\begin{align*}\n",
    "J_{\\textrm{style}}(S, G) =\\sum_l \\lambda^{[l]} J_{\\textrm{style}}^{[l]}(S, G)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">We can visualize what a hidden unit is responding to by plotting an example panel of the image patches from the training set that generate the highest activations in that unit. Units in shallow layers usually look for simple features like edges in small patches on the image, while units in deeper layers look for more complex patterns in larger patches on the image. In neural style transfer we paint a new image $G$ from the content of image $C$ and in the style of image $S$ by randomly initializing $G$, and using GD to minimize $J(G)$ by updating $G$'s pixels, where $J$ has two (weighted) components to reflect the similarity of $G$ to $C$ and to $S$, respectively. Computing $J$ involves running the three images $G$, $C$ and $S$ each through a pre-trained image CNN and comparing the activations. $J_{\\textrm{content}}$ is just the squared norm of the difference between a chosen middle layer's activations on $G$ vs. $C$. Because deeper units typically respond to patterns in colors and shapes, the style of an image can be captured by the degree of correlation between the activations of different units of a deep layer. For example, a unit looking for vertical stripes may be highly positively correlated with a unit looking for the color orange if those two \"patterns\" often appear together in the same region of an image. The full style is captured by a style matrix of pairwise channel (unit) correlations for a chosen layer; this matrix is computed on both the generated and style images and $J_{\\textrm{style}}$ is defined as the squared norm of the difference between the two.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: 1D and 3D Generalizations\n",
    "Just as a 2D (3D) input volume can be convolved with a 2D (3D) filter, a 1D input can be convolved with a 1D filter. For example, EKG data which is electrical signal over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic39.png\" width=380/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Consider CAT scan data which gives a 2D image at successive depth slices in your brain to yield a 3D input data volume. In this case for our first layer we can convolve the input with a 3D filter whose depth dimension does NOT match, but rather is smaller than, the depth dimension of the input volume. This ensures that a small filter window will scan around to sample across the entire input volume and give a 3D volume output for each filter (channel). These 3D outputs from the full set of filters would be concatenated together to give a 4D output. For subsequent layers you would then use filters whose fourth dimension DID match to the input volume's fourth dimension, so that the volume passing through the net remained 4D. Video is another example of 3D data (snapshots over time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 4: Homework\n",
    "\n",
    "## Face Recognition/Verification\n",
    "\n",
    "We write functions to execute the tasks of face verification (given an input image and an identity encoding) and recognition (given an input image and a database of identity encodings) using a pretrained net for generating a vector encoding of the input image. The pretrained model is an inception architecture called FaceNet. We also write a function defining the triplet loss, though it is not needed with the pretrained net implementation.\n",
    "\n",
    "- Function `dist, door_open = verify(image_path, identity, database, model)` performs verification and returns the \"distance\" between the two encodings and whether or not you should open the door for that person\n",
    "- Function `min_dist, identity = who_is_it(image_path, database, model)` performs checking against a database to find the \"closest match\" within a distance of 0.7 (otherwise no match is found).\n",
    "\n",
    "## Neural Style Transfer\n",
    "We write functions to compute the content component of the cost and the style component of the cost (which is an average of the cost computed from several different layers), these are combined into one complete cost function. \n",
    "- `J = total_cost(J_content, J_style, alpha, beta)` computes the full cost by calling\n",
    "    - `J_content = compute_content_cost(a_C, a_G)`\n",
    "    - `J_style = compute_style_cost(model, STYLE_LAYERS)` which averages the individual layer style costs calculated with \n",
    "        - `J_style_layer = compute_layer_style_cost(a_S, a_G)`\n",
    "\n",
    "\n",
    "We then perform the style transfer using a pretrained VGG model according to the steps below:\n",
    "1. Create an Interactive Session\n",
    "2. Load the content image \n",
    "3. Load the style image\n",
    "4. Randomly initialize the image to be generated \n",
    "5. Load the VGG16 model\n",
    "7. Build the TensorFlow graph with an interactive session:\n",
    "    - Extend the graph to run the content image through the VGG16 model and compute the content cost\n",
    "    - Extend the graph to run the style image through the VGG16 model and compute the style cost\n",
    "    - Add a computation (layer) to compute the total cost\n",
    "    - Define the optimizer and the learning rate\n",
    "8. Initialize the TensorFlow graph and run it for a large number of iterations, updating the generated image at every step."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:scipybase_Apr2019]",
   "language": "python",
   "name": "conda-env-scipybase_Apr2019-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
