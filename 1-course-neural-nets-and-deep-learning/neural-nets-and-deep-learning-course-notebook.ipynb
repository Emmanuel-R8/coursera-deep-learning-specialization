{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Week-1:-Introduction-to-Deep-Learning\" data-toc-modified-id=\"Week-1:-Introduction-to-Deep-Learning-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Week 1: Introduction to Deep Learning</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-What-is-a-Neural-Network\" data-toc-modified-id=\"Vid:-What-is-a-Neural-Network-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Vid: What is a Neural Network</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Supervised-Learning-with-Neural-Networks\" data-toc-modified-id=\"Vid:-Supervised-Learning-with-Neural-Networks-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Vid: Supervised Learning with Neural Networks</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Why-is-Deep-Learning-Taking-Off?\" data-toc-modified-id=\"Vid:-Why-is-Deep-Learning-Taking-Off?-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Vid: Why is Deep Learning Taking Off?</a></div><div class=\"lev4 toc-item\"><a href=\"#Week-1-Learning-Objectives\" data-toc-modified-id=\"Week-1-Learning-Objectives-1301\"><span class=\"toc-item-num\">1.3.0.1&nbsp;&nbsp;</span>Week 1 Learning Objectives</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-2:-Logistic-Regression-as-a-Neural-Network\" data-toc-modified-id=\"Week-2:-Logistic-Regression-as-a-Neural-Network-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Week 2: Logistic Regression as a Neural Network</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Binary-Classification\" data-toc-modified-id=\"Vid:-Binary-Classification-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Vid: Binary Classification</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Logistic-Regression\" data-toc-modified-id=\"Vid:-Logistic-Regression-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Vid: Logistic Regression</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Logistic-Regression-Cost-Function\" data-toc-modified-id=\"Vid:-Logistic-Regression-Cost-Function-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Vid: Logistic Regression Cost Function</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Gradient-Descent\" data-toc-modified-id=\"Vid:-Gradient-Descent-24\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Vid: Gradient Descent</a></div><div class=\"lev2 toc-item\"><a href=\"#Computation-Graph\" data-toc-modified-id=\"Computation-Graph-25\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Computation Graph</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Derivatives-on-a-Computation-Graph\" data-toc-modified-id=\"Vid:-Derivatives-on-a-Computation-Graph-26\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Vid: Derivatives on a Computation Graph</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Logistic-Regression-Gradient-Descent\" data-toc-modified-id=\"Vid:-Logistic-Regression-Gradient-Descent-27\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Vid: Logistic Regression Gradient Descent</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Gradient-Descent-on-$m$-Examples\" data-toc-modified-id=\"Vid:-Gradient-Descent-on-$m$-Examples-28\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Vid: Gradient Descent on <span class=\"MathJax_Preview\">m</span><script type=\"math/tex\">m</script> Examples</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-2:-Python-and-Vectorization\" data-toc-modified-id=\"Week-2:-Python-and-Vectorization-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Week 2: Python and Vectorization</a></div><div class=\"lev4 toc-item\"><a href=\"#Week-2-Learning-Objectives\" data-toc-modified-id=\"Week-2-Learning-Objectives-3001\"><span class=\"toc-item-num\">3.0.0.1&nbsp;&nbsp;</span>Week 2 Learning Objectives</a></div><div class=\"lev4 toc-item\"><a href=\"#Homework-for-Week-2\" data-toc-modified-id=\"Homework-for-Week-2-3002\"><span class=\"toc-item-num\">3.0.0.2&nbsp;&nbsp;</span>Homework for Week 2</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-3:-Shallow-Neural-Networks\" data-toc-modified-id=\"Week-3:-Shallow-Neural-Networks-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Week 3: Shallow Neural Networks</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Neural-Networks-Overview\" data-toc-modified-id=\"Vid:-Neural-Networks-Overview-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Vid: Neural Networks Overview</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Neural-Network-Representation\" data-toc-modified-id=\"Vid:-Neural-Network-Representation-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Vid: Neural Network Representation</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Computing-a-Neural-Network's-Output\" data-toc-modified-id=\"Vid:-Computing-a-Neural-Network's-Output-43\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Vid: Computing a Neural Network's Output</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Vectorizing-Across-Multiple-Examples\" data-toc-modified-id=\"Vid:-Vectorizing-Across-Multiple-Examples-44\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Vid: Vectorizing Across Multiple Examples</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Explanation-for-Vectorized-Implementation\" data-toc-modified-id=\"Vid:-Explanation-for-Vectorized-Implementation-45\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Vid: Explanation for Vectorized Implementation</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Activation-Function\" data-toc-modified-id=\"Vid:-Activation-Function-46\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Vid: Activation Function</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Why-Do-You-Need-a-Non-Linear-Activation?\" data-toc-modified-id=\"Vid:-Why-Do-You-Need-a-Non-Linear-Activation?-47\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Vid: Why Do You Need a Non-Linear Activation?</a></div><div class=\"lev2 toc-item\"><a href=\"#Derivatives-of-Activation-Functions\" data-toc-modified-id=\"Derivatives-of-Activation-Functions-48\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>Derivatives of Activation Functions</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Gradient-Descent-for-Neural-Networks\" data-toc-modified-id=\"Vid:-Gradient-Descent-for-Neural-Networks-49\"><span class=\"toc-item-num\">4.9&nbsp;&nbsp;</span>Vid: Gradient Descent for Neural Networks</a></div><div class=\"lev4 toc-item\"><a href=\"#For-Sigmoid-Output-Layer-(and-Log-Loss-Cost)\" data-toc-modified-id=\"For-Sigmoid-Output-Layer-(and-Log-Loss-Cost)-4901\"><span class=\"toc-item-num\">4.9.0.1&nbsp;&nbsp;</span>For Sigmoid Output Layer (and Log-Loss Cost)</a></div><div class=\"lev4 toc-item\"><a href=\"#For-Activation-$g(z)$-Hidden-Layer-(and-Log-Loss-Cost)\" data-toc-modified-id=\"For-Activation-$g(z)$-Hidden-Layer-(and-Log-Loss-Cost)-4902\"><span class=\"toc-item-num\">4.9.0.2&nbsp;&nbsp;</span>For Activation <span class=\"MathJax_Preview\">g(z)</span><script type=\"math/tex\">g(z)</script> Hidden Layer (and Log-Loss Cost)</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Backpropagation-Intuition\" data-toc-modified-id=\"Vid:-Backpropagation-Intuition-410\"><span class=\"toc-item-num\">4.10&nbsp;&nbsp;</span>Vid: Backpropagation Intuition</a></div><div class=\"lev4 toc-item\"><a href=\"#Summary-Equations-(1-hidden-layer-with-activation-$g$,-sigmoid-output-layer)\" data-toc-modified-id=\"Summary-Equations-(1-hidden-layer-with-activation-$g$,-sigmoid-output-layer)-41001\"><span class=\"toc-item-num\">4.10.0.1&nbsp;&nbsp;</span>Summary Equations (1 hidden layer with activation <span class=\"MathJax_Preview\">g</span><script type=\"math/tex\">g</script>, sigmoid output layer)</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Random-Initialization\" data-toc-modified-id=\"Vid:-Random-Initialization-411\"><span class=\"toc-item-num\">4.11&nbsp;&nbsp;</span>Vid: Random Initialization</a></div><div class=\"lev4 toc-item\"><a href=\"#Week-3-Learning-Objectives\" data-toc-modified-id=\"Week-3-Learning-Objectives-41101\"><span class=\"toc-item-num\">4.11.0.1&nbsp;&nbsp;</span>Week 3 Learning Objectives</a></div><div class=\"lev4 toc-item\"><a href=\"#Homework-for-Week-3\" data-toc-modified-id=\"Homework-for-Week-3-41102\"><span class=\"toc-item-num\">4.11.0.2&nbsp;&nbsp;</span>Homework for Week 3</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-4:-Deep-Neural-Networks\" data-toc-modified-id=\"Week-4:-Deep-Neural-Networks-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Week 4: Deep Neural Networks</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Deep-L-layer-Neural-Network\" data-toc-modified-id=\"Vid:-Deep-L-layer-Neural-Network-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Vid: Deep L-layer Neural Network</a></div><div class=\"lev2 toc-item\"><a href=\"#Forward-Propagation-in-a-Deep-Network\" data-toc-modified-id=\"Forward-Propagation-in-a-Deep-Network-52\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Forward Propagation in a Deep Network</a></div><div class=\"lev2 toc-item\"><a href=\"#Getting-Your-Matrix-Dimensions-Right\" data-toc-modified-id=\"Getting-Your-Matrix-Dimensions-Right-53\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Getting Your Matrix Dimensions Right</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Why-Deep-Representations\" data-toc-modified-id=\"Vid:-Why-Deep-Representations-54\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Vid: Why Deep Representations</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Building-Blocks-of-Deep-NNs\" data-toc-modified-id=\"Vid:-Building-Blocks-of-Deep-NNs-55\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Vid: Building Blocks of Deep NNs</a></div><div class=\"lev4 toc-item\"><a href=\"#&quot;Function-Block&quot;-For-a-single-layer-$l$:\" data-toc-modified-id=\"&quot;Function-Block&quot;-For-a-single-layer-$l$:-5501\"><span class=\"toc-item-num\">5.5.0.1&nbsp;&nbsp;</span>\"Function Block\" For a single layer <span class=\"MathJax_Preview\">l</span><script type=\"math/tex\">l</script>:</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Forward-and-Backward-Propagation\" data-toc-modified-id=\"Vid:-Forward-and-Backward-Propagation-56\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Vid: Forward and Backward Propagation</a></div><div class=\"lev4 toc-item\"><a href=\"#For-full-net-computation:\" data-toc-modified-id=\"For-full-net-computation:-5601\"><span class=\"toc-item-num\">5.6.0.1&nbsp;&nbsp;</span>For full net computation:</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Parameters-vs.-Hyperparameters\" data-toc-modified-id=\"Vid:-Parameters-vs.-Hyperparameters-57\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Vid: Parameters vs. Hyperparameters</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-What-Does-This-Have-to-do-with-the-Brain?\" data-toc-modified-id=\"Vid:-What-Does-This-Have-to-do-with-the-Brain?-58\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Vid: What Does This Have to do with the Brain?</a></div><div class=\"lev4 toc-item\"><a href=\"#Week-4-Learning-Objectives\" data-toc-modified-id=\"Week-4-Learning-Objectives-5801\"><span class=\"toc-item-num\">5.8.0.1&nbsp;&nbsp;</span>Week 4 Learning Objectives</a></div><div class=\"lev4 toc-item\"><a href=\"#Homework-for-Week-4\" data-toc-modified-id=\"Homework-for-Week-4-5802\"><span class=\"toc-item-num\">5.8.0.2&nbsp;&nbsp;</span>Homework for Week 4</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 1: Introduction to Deep Learning\n",
    "> **Deep Learning**: training a very large neural network (i.e. having many layers)\n",
    "\n",
    "AI is the new electricity; it is transforming many industries!\n",
    "\n",
    "## Vid: What is a Neural Network\n",
    "\n",
    "A single neuron unit takes an input $\\vec{x}$ and outputs a single value $y$. Commonly these are *Rectifying Linear Units (ReLU)* where the transformation is linear except with a floor at 0. \n",
    "\n",
    "NNs just stack together sets of multiple neurons into layers. They are *densely connected* in that each unit takes input from every unit in the underlying layer. Units in intermediate layers may end up representing more complex features. The slide below illustrates this with a housing price example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic1.png\" width=550></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Supervised Learning with Neural Networks\n",
    "NNs are used most commonly for supervised tasks, and the optimal NN architecture depends on the task (CNNs for computer vision, RNNs for one dimensional 'sequence' data as in NLP). Data inputs can be *structured* (features with defined meaning in a table format) or *unstructured* (audio files, text documents, images, etc.), and NNs outperform other algorithms particularly in the unstructured realm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Why is Deep Learning Taking Off?\n",
    "- **More Data**: Amount of data collected has skyrocketed. Traditional algos and small NNs have a flat performance curve past a certain modest amount of data, whereas large NNs (DL) continue getting better with more data. In the small-data limit often algo performance is dictated by how good you are at feature engineering.\n",
    "\n",
    "\n",
    "- **More Compute**: Better computational ability to train large NNs with GPUs. (Makes the test cycle for new ideas much faster too).\n",
    "\n",
    "\n",
    "- **Better Algorithms**: Many improvements are related to making NNs run faster. For example, switching from sigmoid to ReLU activation because ReLU has a bigger gradient so gradient ascent is faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic2.png\" width=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">NNs are densely connected layers, each layer having multiple units, each unit performing a linear transform followed by activation function on its input ($\\vec{x} \\rightarrow y$). Deep Learning simply means training a very large NN on a lot of data, and DL really shines in supervised learning on unstructured data (images using Convolutional NNs, audio and text using Recurrent NNs). The rise of DL is due mostly to the explosion in training data because large NNs continue to get better with more data unlike other algos. Compute (GPUs) and algorithmic improvements also drive the rise of DL.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Week 1 Learning Objectives\n",
    "- Understand the major trends driving the rise of deep learning.\n",
    "- Be able to explain how deep learning is applied to supervised learning.\n",
    "- Understand what are the major categories of models (such as CNNs and RNNs), and when they should be applied.\n",
    "- Be able to recognize the basics of when deep learning will (or will not) work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 2: Logistic Regression as a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Binary Classification\n",
    "Logistic Regression is a binary classifier that takes in data $\\vec{x}$ and classifies the input as $y$ = 0 or 1 (e.g. if an image is a cat or non-cat). For image classification, the three matrices housing R, G and B intensity values for each pixel are unrolled and concatenated into one long vector to form $\\vec{x}$. \n",
    "\n",
    "Notationally, our data points are labeled as $(x^{(i)}, y^{(i)})$, where each $x$ is a vector in $\\mathbb{R}^n$ and we have $m$ such training points. In NN implementations, the data $\\vec{x}$s are placed side-by-side as column vectors in a matrix $\\mathbf{X}$ with dimension $n\\times m$, while the $y$s are placed side by side into a matrix $\\mathbf{Y}$ with dimension $1\\times m$ (a row vector). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Logistic Regression\n",
    "Logistic regression outputs the *probability* that the input has $y$=1. It does a linear transformation of the input $\\vec{x}$, followed by applying a sigmoid function which maps large negative numbers to 0, large positive numbers to 1 and interpolates linearly in between. \n",
    "\n",
    "The parameters of the linear transform are $\\vec{w}$ and $b$. Note that in NN implementation it is easier to keep $\\vec{w}$ and $b$ separate than to combine them as a single $\\vec{\\theta}$ (where $\\theta_0 = b$), but this notation can show up elsewhere. Our classifier is \n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = \\mathrm{P}(y=1 \\;| \\;x) = \\sigma(w^Tx + b) \\equiv \\sigma(z).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Logistic Regression Cost Function\n",
    "A *Loss* or *Error* Function ($\\mathcal{L}$) is the error the model makes on one single example, while the *Cost* Function ($J$) is the average of this error over the whole training set; the parameters of LR are trained by minimizing the Cost Function. For LR, squared error loss actually creates a non-convex (local minima) problem, so instead we define\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\hat{y}, y) = -\\big(y\\log \\hat{y} + (1-y)\\log(1-\\hat{y})\\big),\\\\[1em]\n",
    "J(\\vec{w}, b) = \\frac{1}{m}\\sum^m_{i=1}\\mathcal{L}(\\hat{y}^{(i)}, y^{(i)})\n",
    "\\end{align}\n",
    "Note the loss function reduces to only the first term when $y$=0 and only the second term when $y$=1.\n",
    "\n",
    "Sidenote: Minimizing this cost function is mathematically identical to maximimizing the log probability of observing the $y$-labels of your data set given their $x$ values (and assuming i.i.d. samples). This is the motivation for this specific odd-looking form, and means that LR is in fact a maximum likelihood estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Gradient Descent\n",
    "Gradient descent finds the minimum of the Cost Function by repeatedly stepping in the direction of greatest decrease in cost in the parameter space. Specifically, on each iteration the parameter $w$ is updated according to\n",
    "\\begin{align}\n",
    "w := w - \\alpha \\frac{\\partial J(w, b)}{\\partial w},\n",
    "\\end{align}\n",
    "and likewise for parameter $b$. Note that $w$ is properly a vector, so the derivative is actually a gradient and the equation is a vector equation. The learning rate $\\alpha$ affects how drastically the parameters are changed on each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** SKIPPED TWO VIDEOS ON REVIEWING DERIVATIVES **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">Logistic regression outputs the *probability* that the input has $y$=1. It does a linear transformation of the input $\\vec{x}$, followed by applying a sigmoid function: $\\hat{y} = \\mathrm{P}(y=1 \\;| \\;x) = \\sigma(w^Tx + b)$. A Loss Function ($\\mathcal{L}$) is the error the model makes on one example, while the Cost Function ($J$) is the average of this error over the whole training set; the parameters of LR are trained by minimizing $J$ with gradient descent. Gradient descent repeatedly steps in the direction of greatest decrease in $J$ over the parameter space: on each iteration we update $w := w - \\alpha \\frac{\\partial J(w, b)}{\\partial w}$.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Computation Graph\n",
    "NNs execute a *forward pass* which computes the output of the model and a *backward pass* which computes derivatives for gradient descent. A computational graph organizes a complex computation on a set of inputs into a progression of simpler computations which build upon themselves. The slide below gives a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic3.png\" width=350></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Derivatives on a Computation Graph\n",
    "Graphs are a good way to represent and organize the chain rule, which lets us calculate the derivative of the final output variable of the graph with respect to any of the input variables. Begin with the rightmost node and write down its derivatives with respect to each of its direct inputs; repeat this for each node moving backwards (leftward) through the graph. This supplies all the intermediate quantities for the full chain rule of d(output)/d(input).\n",
    "\n",
    "In code it is common to name variables representing derivatives with just the denominator of the derivative e.g. $\\frac{dJ}{dv}$ might be given the name \"dv\" in python. It is understood that these are all derivatives of the final output quantity of interest (the cost function). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Logistic Regression Gradient Descent\n",
    "The LR cost function can be represented as a computation graph in order to conduct gradient descent. The slide below shows this idea but for the *loss* function for a single example point, where $x$ and $w$ are in $\\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic4.png\" width=350></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In LR, the derivatives needed for gradient descent **for a single point** turn out to be:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{\"dz\"} \\equiv \\frac{\\partial\\mathcal{L}}{\\partial z} = a^{(i)} - y^{(i)}\\\\[1em]\n",
    "\\mathrm{\"dw1\"} \\equiv \\frac{\\partial\\mathcal{L}}{\\partial w_1} = x_1 \\cdot \\mathrm{\"dz\"} = x_1 (a^{(i)} - y^{(i)})\\\\[1em]\n",
    "\\mathrm{\"db\"} \\equiv \\frac{\\partial\\mathcal{L}}{\\partial b} = \\mathrm{\"dz\"} = a^{(i)} - y^{(i)}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Gradient Descent on $m$ Examples\n",
    "Because the Cost function for LR is just a sum over the single-point losses, we can implement one step of gradient descent as a \"for\" loop that loops over the $m$ training points and for each point cumulatively adds the values \"dw1\", \"dw2\" and \"db\" for that point, as computed with the computation graph approach in the previous section. After the \"for\" loop, $w_1$, $w_2$ and $b$ are updated according to the learning rule and using the cumulative values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic5.png\" width=350></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">A computational graph organizes a complex computation into a progression of simpler computations (e.g. $\\mathcal{L}$ as a function of data and parameters). In the graph picture, NNs execute a *forward pass* to compute the model output and a *backward pass* to compute derivatives for gradient descent. Graphs help to perform the chain rule to calculate the derivative of the output w.r.t. the inputs for gradient descent. For the rightmost node write down its derivatives w.r.t. each of its direct inputs; repeat for each node moving backwards (leftward) through the graph. This supplies all the intermediate quantities for the full chain rule of d(output)/d(input). Because $J$ is a sum over the single-point $\\mathcal{L}$, we implement one step of gradient descent as a \"for\" loop that loops over the $m$ training points and for each point cumulatively adds the values of the derivatives for that point, as computed with the chain rule on the graph.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 2: Python and Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    ">**Since I'm already quite familiar with python and vector operations, I am not breaking down notes by individual videos for this second part of Week 2.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Vectorized code is orders of magnitude faster than explicit \"for\" loops, on both CPUs and GPUs. Many built in functions of numpy for vector and matrix operations are inherently vectorized. The forward pass and the gradient descent algorithm for training logistic regression can actually be FULLY vectorized. \n",
    "\n",
    "Recall that $\\mathbf{X}$ is a matrix with all the data point vectors $\\vec{x}^{(1)}$, $\\vec{x}^{(2)}$ etc. stacked side by side. We can obtain a row vector of transforms, and a row vector of predictions as:\n",
    "\n",
    ">$Z$ = [$z^{(1)}$, $z^{(2)}$..., $z^{(m)}$] = $\\vec{w}^T\\mathbf{X}+\\vec{b}$, \n",
    "\n",
    ">$A$ = [$z^{(1)}$, $z^{(2)}$..., $z^{(m)}$] = $\\sigma(Z)$.\n",
    "\n",
    "where $\\vec{b}$ = [b, b, .... b] and we are using a vectorized version of the $\\sigma$ function.\n",
    "\n",
    "In the backward pass, obtaining derivatives for gradient descent can be fully vectorized as:\n",
    "\n",
    "> $dZ$ = $A-Y$\n",
    "\n",
    "> $db$ = $\\frac{1}{m}$ np.sum($dZ$)\n",
    "\n",
    "> $dw$ = $\\frac{1}{m} \\mathbf{X} dZ^T$\n",
    "\n",
    "The slide below compares the \"for\" loop versus the vectorized implementation of LR gradient descent. Note that you still need one outer for loop to conduct iterations (\"steps\") of gradient descent, but each step is fully vectorized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic6.png\" width=350></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Important Info on Numpy:\n",
    "- [**Broadcasting**](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) is the way numpy interprets arithmetic operations between matrices of different sizes. For instance, if you divide a 3x4 matrix by a 1x4 row vector, numpy will divide each row of the matrix by the row vector (elementwise). Internally, numpy has \"broadcast\" the row vector to form a matrix of the correct size to perform the division. The numpy \"reshape(m,n)\" command is helpful if you want to make sure you have a row vector (1xn) or column vector (mx1) specifically, so that you can ensure the broadcasting will occur along the right dimension.\n",
    "\n",
    "\n",
    "- **Broadcasting Protocol**: When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing dimensions, and works its way forward. Two dimensions are compatible when they are equal or one of them is 1. The size of the resulting array is the maximum size along each dimension of the input arrays. For example, if you have a 256x256x3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:\n",
    "    - Image  (3d array): 256 x 256 x 3\n",
    "    - Scale  (1d array): ........... 3\n",
    "    - Result (3d array): 256 x 256 x 3\n",
    "\n",
    "\n",
    "- Numpy has \"rank 1\" arrays which do not behave like either row or column vectors. They have one dimension so \"shape()\" returns (n,) rather than (n,1) or (1,m). They have unintuitive behavior so avoid doing operations with them. Some built-ins return rank 1 arrays, but can be forced to return a row or column vector by passing the correct shape. For example np.random.randn(5) returns a rank 1 array, while np.random.randn(5,1) returns a column vector.\n",
    "\n",
    "\n",
    "- Keeping track of and enforcing proper shape of arrays during mathematical operations for NNs helps prevent many bugs. When you write a function use the `assert` python statement to make sure your function halts if input dimensions are not as you expect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12288, 45)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.randn(12288, 150) # a.shape = (12288, 150)\n",
    "b = np.random.randn(150, 45) # b.shape = (150, 45)\n",
    "c = np.dot(a,b)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">Always vectorize your code to the fullest extent possible. For logistic regression as a NN, both the forward pass and the backward pass (backprop for obtaining derivatives) can be fully vectorized. The only \"for\" loop needed is to perform repeated steps of gradient descent in training the model. Numpy performs arithmetic between matrices of different sizes and dimensionality by \"broadcasting\" the inputs to form new sized matrices that permit the operation.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Week 2 Learning Objectives\n",
    "- Build a logistic regression model, structured as a shallow neural network\n",
    "- Implement the main steps of an ML algorithm, including making predictions, derivative computation, and gradient descent.\n",
    "- Implement computationally efficient, highly vectorized, versions of models.\n",
    "- Understand how to compute derivatives for logistic regression, using a backpropagation mindset.\n",
    "- Become familiar with Python and Numpy\n",
    "- Work with iPython Notebooks\n",
    "- Be able to implement vectorization across multiple training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Homework for Week 2\n",
    "The way Prof. Ng organized the code was with three independent functions:\n",
    "- `w, b = initialize(dimension)` returns initial values for parameters\n",
    "\n",
    "\n",
    "- `grads, cost = propagate(w, b, X, Y)` performs a single forward (cost) and backward pass (gradients) of the net based on the current values of w and b.\n",
    "\n",
    "\n",
    "- `params, grads, costs = optimize(w, b, X, Y, num_iterations, learning_rate)` performs a complete gradient descent. It has one for loop to iterate through the GD steps, and inside the loop at each step it calls `propagate()` and then updates the parameters based on the calculated gradients. It stores a running trace of the evolution of gradient values and cost function. It returns the final optimized parameter values and the gradient and cost traces.\n",
    "\n",
    "\n",
    "- `predict(w, b, X)` calculates the classifier prediction for the input X\n",
    "\n",
    "\n",
    "- `model(w, b, X, Y, num_iterations, learning_rate)` wraps everything together to initialize the parameters, run the optimization, and make predictions on the train and test set to evaluate accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 3: Shallow Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Neural Networks Overview\n",
    "Each neuron of the NN is actually two nodes in the computational graph: first node $z = w^Tx+b$ flows into the second node $a=\\sigma(z)$. In NNs the quantities associated with each layer neuron are indexed with *square* brackets where indices denote the layer and neuron number within the layer e.g. $a^{[1]}$ is a vector of activations output by the first layer of neurons. The parameters $W^{[1]}$ is now a matrix wherein each row vector is the transposed weights $w^T$ from a single neuron in the first layer. Individual quantities for neurons in a layer are further indexed with subscripts e.g. $z^{[1]}_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Neural Network Representation\n",
    "The first layer is called the \"input\" layer and we can think of it as neurons whose outputs (\"activations\") are just our feature values so notationally $a^{[0]}=x$. The intermediate layers are \"hidden\" layers whose outputs are activations $a^{[1]}$ etc. The final layer with a single neuron is the \"output\" layer and it's output is the actual prediction e.g. $\\hat{y}=a^{[2]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic8.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Computing a Neural Network's Output\n",
    "As mentioned, each neuron performs two computations. Specific quantities for a single neuron are indexed by square brackets giving the layer and subscripts giving the neuron position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic9.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The computations of each layer on a single training example can be fully vectorized. For the 1st layer $W^{[1]}$ is a matrix formed by stacking the row vectors of the neurons $w^{[1]}_i$, $b^{[1]}$ is a column vector formed by stacking the $b$ values of the neurons and the output is a column vector $z^{[1]}$. The vertical \"stacking\" to form these new quantities is reminiscent of how the neurons of a layer are depicted as a single stacked column.\n",
    "\n",
    "\\begin{align*}\n",
    "z^{[1]} = W^{[1]}a^{[0]} + b^{[1]}\\\\[1em]\n",
    "a^{[1]} = \\sigma(z^{[1]})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic10.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Vectorizing Across Multiple Examples\n",
    "In fact the NNs forward pass to compute the full cost $J$ across all samples can also be vectorized. Similar to before, we will line up the different samples as \"columns\" in a matrix. Notationally a layers operation on a *single* training example is indexed with an extra round bracket superscript e.g. $a^{[2](m)}$ is the activation of the second layer on the $m^{th}$ training example.\n",
    "\n",
    "The matrix $X$ is formed by lining up the column vectors $x^{(i)}$ comprising all our input data points so it has dimensions $n\\times m$. For any layer the matrices $Z^{[i]}$ and $A^{[i]}$ are similarly formed by lining up the column vectors for the corresponding quantities for each sample, but their dimensions are $\\textrm{(# neurons)}\\times m$. Remember that the matrix $W^{[i]}$ for a layer remains the same (it is not a function of the sample point!), and it's dimensions are $\\textrm{(# neurons)} \\times n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\\begin{align*}\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "    \\vert & \\vert & \\vert\\\\\n",
    "    x^{(1)}   & x^{(2)} & x^{(3)}  \\\\\n",
    "    \\vert & \\vert & \\vert\n",
    "\\end{bmatrix},\\;\\;\\;\n",
    "W^{[i]} = \n",
    "\\begin{bmatrix}\n",
    "    \\text{---} & w^{[i]}_1 & \\text{---} \\\\\n",
    "    \\text{---} & w^{[i]}_2 & \\text{---}\n",
    "\\end{bmatrix},\\;\\;\\;\n",
    "Z^{[i]} = \n",
    "\\begin{bmatrix}\n",
    "    \\vert & \\vert & \\vert\\\\\n",
    "    z^{[i] (1)}   & z^{[i] (2)} & z^{[i] (3)}  \\\\\n",
    "    \\vert & \\vert & \\vert\n",
    "\\end{bmatrix}\\\\[1em]\n",
    "Z^{[i]} = W^{[i]}X + b^{[i]}\\\\[1em]\n",
    "A^{[i]} = \\sigma(Z^{[i]})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Explanation for Vectorized Implementation\n",
    "Just going into the vectorized math in more detail to prove that it does what we want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">Notationally hidden layers are indexed with *square* bracket superscripts, individual neuron location within a layer by subscripts, and individual training examples by *round* bracket superscripts e.g. $a^{[1](29)}_5$ is the activation (single real number) output by the 5th neuron in the 1st layer when operating on the 29th training point. The computations of each full layer on the complete set of training points can be vectorized. Matrix $X$ lines up column vectors $x^{(i)}$ of all our samples and has dimension $n\\times m$. For each layer: Matrices $Z^{[i]}$ and $A^{[i]}$ line up the column vectors for the corresponding quantities computed on each sample and both have dimension $\\textrm{(# neurons)}\\times m$. Matrix $W^{[i]}$ vertically stacks up row vectors of the weights $w_j^T$ of the neurons and has dimension $\\textrm{(# neurons)} \\times n$. Column vector $b^{[i]}$ stacks the $b_j$ values of the neurons. The full vectorized operation of a layer on all the training points is then $Z^{[i]} = W^{[i]}X + b^{[i]}$ followed by $A^{[i]} = \\sigma(Z^{[i]})$.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Activation Function\n",
    "In general we can use some other activation function $g(z^{[i]})$ as long as it's nonlinear. The **tanh** function almost always works better because the output values are closer to having a mean of zero (which helps analogous to how centering data helps the algorithm). Sigmoid for the output layer is still appropriate to get probabilities on [0, 1]. Both sigmoid and tanh have the issue of having very small derivative out at the tails, which slows down the GD algorithm (parameters get updated by smaller increments). The **RelU** function is equivalent to $f(z) = \\max(0, z)$ only has this problem on the negative-$z$ side, and the **leaky RelU** corrects that somewhat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic11.png\" width=350/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Why Do You Need a Non-Linear Activation?\n",
    "If you do away with the activation (or use a linear activation) it turns out the full computation of the net (however complex) reduces to just a simple linear transformation of the input! In a regression problem it can be appropriate to let the output layer neuron have a linear activation (although RelU also works if the targets are all positive numbers) so that the output values span the real line rather than restricted to [0, 1]. \n",
    "\n",
    "Sidenote: Because of its very different behavior for negative vs. positive inputs, RELU is considered non-linear as a whole. The weights can be trained to generate a negative Z value to effectively \"turn off\" a node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Derivatives of Activation Functions\n",
    "- *sigmoid*: $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$ is close to zero for points away from $z=0$\n",
    "- *tanh*: $\\tanh'(z) = 1-\\tanh(z)^2$ is close to zero for points away from $z=0$\n",
    "- *RelU*: $g'(z) = 1$ for $z>0$ and zero elsewhere.\n",
    "- *Leaky RelU*: $g'(z) = 1$ for $z>0$ and small but non-zero elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">A nonlinear activation for hidden units is needed to make the net computation more complex than a simple linear transform of the input. RelU or leaky RelU are usually best, then tanh, then sigmoid; this stems from the small-derivative issue of the latter two functions, which limits GD operation. For the output layer, sigmoid is appropriate for binary classification while linear or RelU are appropriate for regression problems on the real line.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Gradient Descent for Neural Networks\n",
    "We denote the number of nodes in a layer with $n$ (including the \"nodes\" in the input layer i.e. features). A NN with one hidden layer will have values $n^{[0]}=n_x$, $n^{[1]}$, and $n^{[2]}=1$, and these numbers determine the dimensions of the $W^{[i]}$ matrices. The cost is a function of all the parameters $J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]})$ so for GD we need the partial derivatives of $J$ with respect to these four objects, these can be fully vectorized. For back-prop we start computing at the rightmost layer and work backwards.\n",
    "\n",
    "#### For Sigmoid Output Layer (and Log-Loss Cost)\n",
    "\\begin{align*}\n",
    "Y = [y^{(1)}, y^{(2)}..., y^{(m)}]\\;\\;\\textrm{(a row vector)}\\\\[1em]\n",
    "dZ^{[i]} = A^{[i]} - Y\\\\[1em]\n",
    "dW^{[i]} = \\frac{1}{m}\\big(dZ^{[i]}A^{[i]\\;\\textrm{T}}\\big)\\\\[1em]\n",
    "db^{[i]} = \\frac{1}{m}\\;\\textrm{np.sum($dZ^{[i]}$, axis=1, keepdims=True)}\n",
    "\\end{align*}\n",
    "\n",
    "#### For Activation $g(z)$ Hidden Layer (and Log-Loss Cost)\n",
    "\\begin{align*}\n",
    "dZ^{[i]} = \\big(W^{[i+1]}dZ^{[i+1]}\\big) * \\big(g'^{[i]}(Z^{[i]})\\big)\\;\\;\\;\\textrm{ where * is elementwise multiplication}\\\\[1em]\n",
    "dW^{[i]} = \\frac{1}{m}\\big(dZ^{[i]}X^{\\textrm{T}}\\big)\\\\[1em]\n",
    "db^{[i]} = \\frac{1}{m}\\;\\textrm{np.sum($dZ^{[i]}$, axis=1, keepdims=True)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here $g'^{[i]}(Z^{[i]})$ is the derivative of the activation function for the $i^{th}$ layer evaluated on the $Z$ inputs of that layer. The keyword `keepdims` means the operation will not *eliminate* a dimension/axis that we have reduced along, so e.g. we will get back an array with shape ($n^{[i]}$, 1) rather than a strange rank 1 object with shape ($n^{[i]}$,). You could instead use `reshape()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Backpropagation Intuition\n",
    "Intuition is a stretch...mostly focused on how the vectorized forms of the back-prop equations are built analagously to the fully vectorized forward-prop equations, namely by lining up the column vectors of individual samples into a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Summary Equations (1 hidden layer with activation $g$, sigmoid output layer)\n",
    "On the left are vectorized quantities for computations on a single training point, on the right are fully matricized quantities for computations on the entire batch of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic12.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Random Initialization\n",
    "For NNs it's very important to randomly initialize weights (rather than all zeros). If all the $W^{[i]}$ begin at zero, then all the $dz^{[i]}_j$ for individual neurons in a given layer will be identical on the first back-prop pass. By symmetry, successive GD steps will continue to evolve the neurons identically so that they all continue to compute exactly the same output - which completely limits the fitting power of the net. In python we do $W^{[i]} = \\textrm{np.random.randn($W^{[i]}$.shape)}*0.01$ to achieve very small random values. We want small values because we don't want our $Z$ values to be out in the small-derivative regions of the activation function. Note that the exact small-value multiplier that is appropriate depends on the NN. The $b^{[i]}$ vectors can still be initialized to zero with no issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">Back-prop equations can be fully vectorized by lining up column vectors for all the sample points into a matrix. It's very important to randomly initialize weights. If all the $W^{[i]}$ begin at zero, then by symmetry successive GD steps will continue to evolve the neurons identically so that they all continue to compute exactly the same output.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Week 3 Learning Objectives\n",
    "- Understand hidden units and hidden layers\n",
    "- Be able to apply a variety of activation functions in a neural network.\n",
    "- Build your first forward and backward propagation with a hidden layer\n",
    "- Apply random initialization to your neural network\n",
    "- Become fluent with Deep Learning notations and Neural Network Representations\n",
    "- Build and train a neural network with one hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Homework for Week 3\n",
    "The way Prof. Ng organized the code was with 6 independent functions:\n",
    "- `parameters = initialize(n_x, n_h, n_y)` takes in the number of \"neurons\" in each layer (input, hidden and output) and returns a dictionary of matrices (with correct dimensions) with random initial values for the parameters of the hidden and output layers.\n",
    "\n",
    "\n",
    "- `A2, cache = forward_propagation(X, parameters)` performs a single forward pass to compute the predictions `A2` based on the current parameter values passed in. The Z values and A values for each layer computed on that pass are stored in the `cache` dictionary.\n",
    "\n",
    "\n",
    "- `cost = compute_cost(A2, Y, parameters)` computes the cost function for the current predictions by the net (doesn't actually use `parameters` input).\n",
    "\n",
    "\n",
    "- `grads = backward_propagation(parameters, cache, X, Y)` performs a single backward pass to compute the derivatives of the cost w.r.t. all the parameters which are returned in the dictionary `grads`.\n",
    "\n",
    "\n",
    "- `parameters = update_parameters(parameters, grads)` simply performs the update rule to return the new parameter values based on this step of GD\n",
    "\n",
    "\n",
    "- `parameters = nn_model(X, Y, n_h, num_iterations, print_cost)` wraps everything together to initialize the parameters and then execute a for loop on each step of which we call `forward_propagation`, `compute_cost`, `backward_propagation` and `parameters` in that order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 4: Deep Neural Networks\n",
    "\n",
    "\n",
    "## Vid: Deep L-layer Neural Network\n",
    "A deep NN just means a significant number of hidden layers. It's difficult to predict for a particular problem how many layers you need, so start by trying shallow options and consider the number of layers as a hyperparameter. In labeling NNs we only consider hidden and output layers, so e.g. logistic regression is a \"1 layer\" NN. We let $L$ denote the number of layers and $n^{[l]}$ the number of units in $l^{th}$ layer. The input layer is indexed by $l=0$ so e.g. $a^{[0]} = x$.\n",
    "\n",
    "\n",
    "## Forward Propagation in a Deep Network\n",
    "With our terminology we can write a single set of equations that can technically apply to forward-prop computations on *all* layers: input, hidden and output. In code we put these equations inside a for loop that loops through the layers. A key to implementing this is to think carefully about what the matrix dimensions should be in each layer.\n",
    "\n",
    "\\begin{align*}\n",
    "Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}\\\\[1em]\n",
    "A^{[l]} = g^{[l]}(Z^{[l]}\\\\[1em]\n",
    "\\textrm{(where $A^{[0]} = X$ and $A^{[L]} = \\hat{Y}$)}\n",
    "\\end{align*}\n",
    "\n",
    "## Getting Your Matrix Dimensions Right\n",
    "For the fully matricized math with $m$ training examples:\n",
    "\n",
    "\\begin{align*}\n",
    "Z^{[l]} = W^{[l]} \\cdot A^{[l-1]} + b^{[l]}\\\\[1em]\n",
    "(n^{[l]}, m) \\;= \\;(n^{[l]}, n^{[l-1]}) \\; \\cdot \\; (n^{[l-1]}, m) \\; + \\; (n^{[l]}, 1)\\;\\;\\; \\textrm{where $b$ is broadcast to $(n^{[l]}, m)$}\n",
    "\\end{align*}\n",
    "\n",
    "The various derivative values e.g. $dZ1$ should have the same dimensions as the variable itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">$L$ is a tunable hyperparameter that denotes the number of layers (not counting input), and $n^{[l]}$ denotes the number of units in the $l^{th}$ layer (input layer is indexed by $l=0$). In implementing deep NNs it's critical to get the correct matrix dimensions: $Z^{[l]} = W^{[l]} \\cdot A^{[l-1]} + b^{[l]}$ implies $(n^{[l]}, m) \\;= \\;(n^{[l]}, n^{[l-1]}) \\; \\cdot \\; (n^{[l-1]}, m) \\; + \\; (n^{[l]}, 1)$, where $b$ is broadcast to $(n^{[l]}, m)$.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Why Deep Representations\n",
    "Why *deep* as opposed to just a giant single layer? We can think of earlier layers as learning simpler functions like \"vertical edge\" and \"horizontal edge\" and subsequent layers as composing these functions into more complex functions like \"nose detection\", \"eye detection\" and then eventually types of faces. Early layers might be looking at only small regions of a picture while later layers might look at the whole pixel set. Or for audio data early layers might detect low level waveform features e.g. \"tone up\", \"tone down\", \"pitch\" while a later layer might compose these to recognize phonemes, which can be composed to recognize words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic13.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can also prove (based on circuit theory) that some logical functions that can be computed with \"smaller\" deep NNs would require exponentially more units in a single layer implementation. An example is the XOR of all your features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Building Blocks of Deep NNs\n",
    "\n",
    "Computations in the NN can be thought of in terms of sequential function blocks representing layers. In forward and back prop the blocks pass some inputs along the next block in the sequence. The forward pass also caches additional information needed by the back prop calculations.\n",
    "\n",
    "#### \"Function Block\" For a single layer $l$:\n",
    "\n",
    "- Parameters of the layer $W^{[l]}$, $b^{[l]}$\n",
    "\n",
    "\n",
    "- Forward Pass:\n",
    "    - input $A^{[l-1]}$ passed from the underlying layer\n",
    "    - compute:\n",
    "        - $Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$\n",
    "        - $A^{[l]} = g^{[l]}(Z^{[l]})$\n",
    "    - output $A^{[l]}$ to pass forward to next layer\n",
    "    - cache $A^{[l-1]}$, $Z^{[l]}$, $W^{[l]}$, and $b^{[l]}$... we will pass the cache to the backprop step\n",
    "\n",
    "\n",
    "- Back Prop:\n",
    "    - input $dA^{[l]}$ passed from the overlying layer and cached values of $A^{[l-1]}$, $Z^{[l]}$, $W^{[l]}$, and $b^{[l]}$ from forward prop\n",
    "    - compute \n",
    "        - $dZ^{[l]} = dA^{[l]}*g'^{[l]}(Z^{[l]})$ as an intermediary\n",
    "        - $dW^{[l]} = \\frac{1}{m} dZ^{[l]}A^{[l-1]T}$\n",
    "        - $db^{[l]} = \\frac{1}{m} \\textrm{np.sum($dZ^{[l]}$, axis=1, keepdims=True)}$\n",
    "        - $dA^{[l-1]} = W^{[l]T} dZ^{[l]}$\n",
    "    - output\n",
    "        - $dA^{[l-1]}$ to feed backward to next layer\n",
    "        - $dW^{[l]}$, $db^{[l]}$ to use for GD parameter update\n",
    "\n",
    "Note in the computations for back-prop if you plug in the last listed equation for $dA^{[l-1]}$ into the first listed equation for $dZ^{[l]}$ you will recover the expression for $dZ^{[l]}$ that we had previous been using. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic14.png\" width=200/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Forward and Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### For full net computation: \n",
    "\n",
    "- Initialize the forward pass by inputing $A^[0] = X$ to the first layer function node\n",
    "- Compute and cache the cost once we have $A^{[L]}$ (for debugging purposes)\n",
    "- Initialize the backprop pass by feeding $dA^{[L]}$ into the $L$-layer (last layer) function node. Exact expression for $dA^{[L]}$ depends on choice of $J$.\n",
    "    - Log-Loss (Cross-Entropy) for Binary Classification: $da^{[L]} = -\\frac{y}{a} + \\frac{1-y}{1-a}$ (sum over these terms for all the sample points to compute $dA^{[L]}$)\n",
    "\n",
    "Note: in the below diagrams $A^{[l-1]}$ is not shown as being in the cache, but you do need it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic15.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Parameters vs. Hyperparameters\n",
    "Parameters are the $W$ and $b$ for all layers. Basic hyperparameters are learning rate, number of iterations of GD, number of hidden layers, units per layer, choice of activation and they control the final values of $W$ and $b$ that your algorithm outputs. There are even more hyperparameters for more complicated implementations: momentum, mini-batch size, regularization strength. Values for all hyperparameters need to be \"tried out\" to find good values; to evaluate you can monitor $J$ vs. the iteration step number to see the evolution of cost as the algorithm runs. The best hyperparameters can be very different depending on the application e.g. vision vs speech vs NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: What Does This Have to do with the Brain?\n",
    "This analogy is generally breaking down as deep learning field evolves.\n",
    "\n",
    "There is a very simplistic analogy between a single logistic unit and a single biological neuron (multiple inputs from other neurons, a thresholding computation in the body of the neuron based on these inputs, and then a single 0 or 1 output in the form a pulse not-fired/fired down the axon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic16.png\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">\"Deep\" is superior to simply \"Big\" because earlier layer can learn simpler functions like \"vertical edge\" and subsequent layers can compose these into more complex functions like \"nose detection\", and then eventually \"types of faces\". Computations in the NN can be thought of as sequential function blocks representing layers; in forward and back prop blocks pass inputs along the next block in the sequence and the forward pass caches extra info needed by the back pass. To evaluate the many hyperparameter configurations of NNs you can monitor $J$ vs. the iteration step number to see the evolution of cost as the algorithm runs.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Week 4 Learning Objectives\n",
    "- See deep neural networks as successive blocks put one after each other\n",
    "- Build and train a deep L-layer Neural Network\n",
    "- Analyze matrix and vector dimensions to check neural network implementations.\n",
    "- Understand how to use a cache to pass information from forward propagation to back propagation.\n",
    "- Understand the role of hyperparameters in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Homework for Week 4\n",
    "\n",
    "The code organization for an L-layer NN is somewhat more complicated.\n",
    "\n",
    "**Initialization**: The same as usual, just looping through the layers.\n",
    "\n",
    "- `parameters = initialize_parameters_deep(layer_dims)` takes in a list of the number of \"neurons\" in each layer (input, hidden1..., hiddenL-1, and output) and returns a dictionary of matrices (with correct dimensions) with random initial values for the parameters $W$ and $b$ of the hidden and output layers.\n",
    "\n",
    "\n",
    "**Forward Pass**: \n",
    "\n",
    "Each layer computes a linear transform followed by activation so these two functions are written separately and each stores a cache of values that are used by backprop.\n",
    "\n",
    "- `Z, linear_cache = linear_forward(A_prev, W, b)` where `linear_cache: (A_prev, W, b)`. Performs the linear part of the computation of a layer to output $Z$ and cache the passed in $A$ and layer parameters.\n",
    "\n",
    "- `A, activation_cache = relu(Z)` where `activation_cache: (Z)`. Performs the non-linear part of the computation of a layer to output $A$ and cache the passed in $Z$ value. An analogous function is defined `sigmoid(Z)`.\n",
    "\n",
    "\n",
    "The linear and activation pieces of a single layer computation are combined into one function, and then a wrapper function executes the full sequential forward pass.\n",
    "\n",
    "- `A, cache = linear_activation_forward(A_prev, W, b, activation_type)` where `cache: (linear_cache, activation_cache)`. Performs the linear + activation computation of a layer to output `A` for passing forward and to collect the two caches.\n",
    "\n",
    "- `AL, caches = L_model_forward(X, parameters)` where `caches: [(lin_cache_1, act_cache_1)..., (lin_cache_L, act_cache_L)]`. Loops through the layers calling `linear_activation_forward`, adding the two caches into a list and passing its output of $A$ forward into the overlying layer. The loop is initialized with $A^{[0]}=X$, and the output of the last layer is `AL` which gives our predictions $\\hat{Y}$.\n",
    "\n",
    "\n",
    "**Cost**: This follows the same structure as the forward pass, and relies on cached values from that pass.\n",
    "\n",
    "- `cost = compute_cost(AL, Y)` computes the cost function for the current predictions by the net (doesn't actually use `parameters` input).\n",
    "\n",
    "\n",
    "**Backward Pass**: \n",
    "\n",
    "This follows the same structure as the forward pass, however since we are working backwards the activation component of a single layer's computation is considered *before* the linear component. These functions rely on cached values from the forward pass.\n",
    "\n",
    "- `dZ = relu_backward(dA, activation_cache)` Receives a passed in values of `dA` computed by the overlying node and uses it to compute `dZ` (also relying on the cached value of $Z$.\n",
    "\n",
    "- `dA_prev, dW, db = linear_backward(dZ, linear_cache)` Uses the value of `dZ` and the cached values of `W` and `A_prev` to compute derivatives. Note that the computed `dA_prev` is now available to pass backward into the next (underlying) node.\n",
    "\n",
    "The activation and linear pieces of a single layer computation are combined into one function, and then a wrapper function executes the full sequential backward pass.\n",
    "\n",
    "- `dA_prev, dW, db = linear_activation_backward(dA, cache, activation_type)` where `cache: (linear_cache, activation_cache)`. Performs the activation and linear components of the derivative computations of a layer to output `dA_prev` for passing backward and to collect the derivatives needed for parameter updates.\n",
    "\n",
    "- `grads = L_model_backward(AL, Y, caches)` where `caches: [(lin_cache_1, act_cache_1)..., (lin_cache_L, act_cache_L)]`. Loops backward through the layers calling `linear_activation_backward`, adding the derivates into a dictionary and passing its output of `dA_prev` backward into the underlying layer. The loop is initialized with `dAL` which is computed from the form of the cost function.\n",
    "\n",
    "\n",
    "**Parameter Updates**:\n",
    "\n",
    "- `parameters = update_parameters(parameters, grads, learning_rate)` simply performs the update rule to return the new parameter values based on this step of GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic17.png\" width=450/>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
