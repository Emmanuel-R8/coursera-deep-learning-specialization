{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Week-1:-Setting-Up-Your-Machine-Learning-Application\" data-toc-modified-id=\"Week-1:-Setting-Up-Your-Machine-Learning-Application-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Week 1: Setting Up Your Machine Learning Application</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Train-/-Dev-/-Test-Sets\" data-toc-modified-id=\"Vid:-Train-/-Dev-/-Test-Sets-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Vid: Train / Dev / Test Sets</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Bias/Variance\" data-toc-modified-id=\"Vid:-Bias/Variance-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Vid: Bias/Variance</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Basic-Recipe-for-Machine-Learning\" data-toc-modified-id=\"Vid:-Basic-Recipe-for-Machine-Learning-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Vid: Basic Recipe for Machine Learning</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-1:-Regularizing-Your-Neural-Network\" data-toc-modified-id=\"Week-1:-Regularizing-Your-Neural-Network-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Week 1: Regularizing Your Neural Network</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Regularization\" data-toc-modified-id=\"Vid:-Regularization-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Vid: Regularization</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Why-Regularization-Reduces-Overfitting\" data-toc-modified-id=\"Vid:-Why-Regularization-Reduces-Overfitting-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Vid: Why Regularization Reduces Overfitting</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Dropout-Regularization\" data-toc-modified-id=\"Vid:-Dropout-Regularization-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Vid: Dropout Regularization</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Understanding-Dropout\" data-toc-modified-id=\"Vid:-Understanding-Dropout-24\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Vid: Understanding Dropout</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Other-Regularization-Techniques\" data-toc-modified-id=\"Vid:-Other-Regularization-Techniques-25\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Vid: Other Regularization Techniques</a></div><div class=\"lev3 toc-item\"><a href=\"#Data-Augmentation\" data-toc-modified-id=\"Data-Augmentation-251\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Data Augmentation</a></div><div class=\"lev3 toc-item\"><a href=\"#Early-Stopping\" data-toc-modified-id=\"Early-Stopping-252\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Early Stopping</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-1:-Setting-Up-Your-Optimization-Problem\" data-toc-modified-id=\"Week-1:-Setting-Up-Your-Optimization-Problem-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Week 1: Setting Up Your Optimization Problem</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Normalizing-Inputs\" data-toc-modified-id=\"Vid:-Normalizing-Inputs-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Vid: Normalizing Inputs</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Vanishing/Exploding-Gradients\" data-toc-modified-id=\"Vid:-Vanishing/Exploding-Gradients-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Vid: Vanishing/Exploding Gradients</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Weight-Initialization-for-Deep-Networks\" data-toc-modified-id=\"Vid:-Weight-Initialization-for-Deep-Networks-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Vid: Weight Initialization for Deep Networks</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Numerical-Approximation-of-Gradients\" data-toc-modified-id=\"Vid:-Numerical-Approximation-of-Gradients-34\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Vid: Numerical Approximation of Gradients</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Gradient-Checking\" data-toc-modified-id=\"Vid:-Gradient-Checking-35\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Vid: Gradient Checking</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Gradient-Checking-Implementation-Notes\" data-toc-modified-id=\"Vid:-Gradient-Checking-Implementation-Notes-36\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Vid: Gradient Checking Implementation Notes</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-1-Learning-Objectives:\" data-toc-modified-id=\"Week-1-Learning-Objectives:-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Week 1 Learning Objectives:</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-1-Homework\" data-toc-modified-id=\"Week-1-Homework-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Week 1 Homework</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-2:-Optimization-Algorithms\" data-toc-modified-id=\"Week-2:-Optimization-Algorithms-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Week 2: Optimization Algorithms</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:Batch-vs.-Mini-batch-Gradient-Descent\" data-toc-modified-id=\"Vid:Batch-vs.-Mini-batch-Gradient-Descent-61\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Vid:Batch vs. Mini-batch Gradient Descent</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Understanding-Mini-batch-Gradient-Descent\" data-toc-modified-id=\"Vid:-Understanding-Mini-batch-Gradient-Descent-62\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Vid: Understanding Mini-batch Gradient Descent</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Exponentially-Weighted-Averages\" data-toc-modified-id=\"Vid:-Exponentially-Weighted-Averages-63\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Vid: Exponentially Weighted Averages</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Understanding-Exponentially-Weighted-Averages\" data-toc-modified-id=\"Vid:-Understanding-Exponentially-Weighted-Averages-64\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Vid: Understanding Exponentially Weighted Averages</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Bias-Correction-in-Exponentially-Weighted-Averages\" data-toc-modified-id=\"Vid:-Bias-Correction-in-Exponentially-Weighted-Averages-65\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Vid: Bias Correction in Exponentially Weighted Averages</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Gradient-Descent-with-Momentum\" data-toc-modified-id=\"Vid:-Gradient-Descent-with-Momentum-66\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>Vid: Gradient Descent with Momentum</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-RMSprop\" data-toc-modified-id=\"Vid:-RMSprop-67\"><span class=\"toc-item-num\">6.7&nbsp;&nbsp;</span>Vid: RMSprop</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Adam-Optimization-Algorithm\" data-toc-modified-id=\"Vid:-Adam-Optimization-Algorithm-68\"><span class=\"toc-item-num\">6.8&nbsp;&nbsp;</span>Vid: Adam Optimization Algorithm</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Learning-Rate-Decay\" data-toc-modified-id=\"Vid:-Learning-Rate-Decay-69\"><span class=\"toc-item-num\">6.9&nbsp;&nbsp;</span>Vid: Learning Rate Decay</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-The-Problem-of-Local-Optima\" data-toc-modified-id=\"Vid:-The-Problem-of-Local-Optima-610\"><span class=\"toc-item-num\">6.10&nbsp;&nbsp;</span>Vid: The Problem of Local Optima</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-2-Objectives\" data-toc-modified-id=\"Week-2-Objectives-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Week 2 Objectives</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-2-Homework\" data-toc-modified-id=\"Week-2-Homework-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Week 2 Homework</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-3:-Hyperparameter-Tuning\" data-toc-modified-id=\"Week-3:-Hyperparameter-Tuning-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Week 3: Hyperparameter Tuning</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Tuning-Process\" data-toc-modified-id=\"Vid:-Tuning-Process-91\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Vid: Tuning Process</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Using-an-Appropriate-Scale-to-Pick-Hyperparameters\" data-toc-modified-id=\"Vid:-Using-an-Appropriate-Scale-to-Pick-Hyperparameters-92\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Vid: Using an Appropriate Scale to Pick Hyperparameters</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Hyperparmeter-tuning-in-practice:-pandas-vs.-caviar\" data-toc-modified-id=\"Vid:-Hyperparmeter-tuning-in-practice:-pandas-vs.-caviar-93\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Vid: Hyperparmeter tuning in practice: pandas vs. caviar</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-3:-Batch-Normalization\" data-toc-modified-id=\"Week-3:-Batch-Normalization-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Week 3: Batch Normalization</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Normalizing-Activations-in-a-Network\" data-toc-modified-id=\"Vid:-Normalizing-Activations-in-a-Network-101\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Vid: Normalizing Activations in a Network</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Fitting-Batch-Norm-into-a-NN\" data-toc-modified-id=\"Vid:-Fitting-Batch-Norm-into-a-NN-102\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>Vid: Fitting Batch Norm into a NN</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Why-Does-Batch-Norm-Work?\" data-toc-modified-id=\"Vid:-Why-Does-Batch-Norm-Work?-103\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;</span>Vid: Why Does Batch Norm Work?</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Batch-Norm-at-Test-Time\" data-toc-modified-id=\"Vid:-Batch-Norm-at-Test-Time-104\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;</span>Vid: Batch Norm at Test Time</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-3:-Multi-Class-Classification\" data-toc-modified-id=\"Week-3:-Multi-Class-Classification-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Week 3: Multi-Class Classification</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Softmax-Regression\" data-toc-modified-id=\"Vid:-Softmax-Regression-111\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Vid: Softmax Regression</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Training-a-Softmax-Classifier\" data-toc-modified-id=\"Vid:-Training-a-Softmax-Classifier-112\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>Vid: Training a Softmax Classifier</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-3:-Introduction-to-Programming-Frameworks\" data-toc-modified-id=\"Week-3:-Introduction-to-Programming-Frameworks-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Week 3: Introduction to Programming Frameworks</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Deep-Learning-Frameworks\" data-toc-modified-id=\"Vid:-Deep-Learning-Frameworks-121\"><span class=\"toc-item-num\">12.1&nbsp;&nbsp;</span>Vid: Deep Learning Frameworks</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-TensorFlow\" data-toc-modified-id=\"Vid:-TensorFlow-122\"><span class=\"toc-item-num\">12.2&nbsp;&nbsp;</span>Vid: TensorFlow</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-3-Objectives\" data-toc-modified-id=\"Week-3-Objectives-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Week 3 Objectives</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-3-Homework\" data-toc-modified-id=\"Week-3-Homework-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Week 3 Homework</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-2:-Case-Studies\" data-toc-modified-id=\"Week-2:-Case-Studies-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Week 2: Case Studies</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Why-Case-Studies?\" data-toc-modified-id=\"Vid:-Why-Case-Studies?-151\"><span class=\"toc-item-num\">15.1&nbsp;&nbsp;</span>Vid: Why Case Studies?</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 1: Setting Up Your Machine Learning Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Train / Dev / Test Sets\n",
    "Applied ML is highly iterative because it is impossible to guess what hyperparameter configurations will work best for a given task, and intuition for hyperparameters often doesn't translate between application fields. It's crucial to quickly be able to execute \"idea -> code -> experiment -> idea\" cycles, which requires splitting the data. The *training set* is used to train different algorithms, the *hold-out cross validation* or *dev set* is used to compare the performance of those different algorithms, and the (optional) *test set* is used to get an unbiased estimate of the performance of your chosen final algorithm. With a small number of samples (up to 10,000) standard data splits are 60/20/20 or 70/30 without a test set. However with big data it is possible to get robust evaluation with just a small fraction of the data because there is so much of it, so e.g. with 1 million samples you might use 98/1/1. Another trend is mismatched train and test distributions because DL algorithms require so much data e.g. an app that trains on cat images found on the web, but wants to classify cat images uploaded by their users. You should however make sure the dev and test sets do come from the same distribution.\n",
    "\n",
    "## Vid: Bias/Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic3.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To recognize these problems look at both the training set and dev set errors. For these comparisons you should keep in mind what a \"human\" level of error would be on the task, that will dictate what is \"high\" vs. \"low\" error in the below rules:\n",
    "\n",
    "- Train very low and < Dev | overfitting / high variance\n",
    "- Train ~ Dev but both very high | underfitting / high bias\n",
    "- Train very high and < Dev | high bias AND high variance?\n",
    "- Train ~ Dev and both very high | JUST RIGHT\n",
    "\n",
    "With some high dimensional problems it is possible to have too much bias in one region of the feature space and too much variance in other. The below classifier (purple line) is predominately linear but has too much flexibility in the middle region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic1.png\" width=200/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Basic Recipe for Machine Learning\n",
    "Knowing whether you are dealing with high bias or high variance lets you systematically improve performance.\n",
    "\n",
    "1) Look at the training data error to decide if you have high bias. If so, try:\n",
    "    - using a 'bigger' network\n",
    "    - training for longer\n",
    "    - possibly a different NN architecture\n",
    "\n",
    "\n",
    "2) Once bias is reduced, check/compare the dev set performance to decide if you have high variance. If so, try:\n",
    "    - get more data (best way!)\n",
    "    - try regularization\n",
    "    - again, consider a different NN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic2.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Pre-DL era we discussed the bias/variance tradeoff because we didn't have as many techniques for tuning them independently. Now using a bigger network almost always helps bias without hurting variance as long as you regularize properly, and getting more data always helps variance without hurting bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">Use an iterative process to tune hyperparameters. The *training set* is used to train different algorithms, the *hold-out cross validation* or *dev set* is used to compare the performance of those different algorithms, and the (optional) *test set* is used to get an unbiased performance estimate of your final algorithm. With < 10k samples standard splits are 60/20/20 or 70/30, however with big data (> 1 million) we might use 98/1/1. Always make sure the dev and test sets do come from the same distribution. To diagnose bias vs. variance look at how the train and dev set errors compare to each other and to what you expect the baseline human error for the task to be i.e. minimum possible error rate. For high bias consider a bigger network/different architecture or longer training. For high variance consider gather more data (top choice), regularization techniques or a different architecture.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 1: Regularizing Your Neural Network\n",
    "\n",
    "## Vid: Regularization\n",
    "If you have high variance and adding more training data is infeasible, try regularization! You can do this by adding a term to the cost function that increases the cost in proportion to how large (non-zero) the parameters are, this will push the algorithm toward smaller values of the parameters thereby \"turning off\" certain features (LR) or nodes (NN) and making your model simpler.  \n",
    "\n",
    "For NN the specific terms are:\n",
    "- L2 regularization: $\\frac{\\lambda}{2m}\\sum_{l=1}^L||W^{[L]}||^2 = \\sum_{i=1}^{n^{[l-1]}} \\sum_{j=1}^{n^{[l]}}(W_{ij}^{[l]})^2$\n",
    "\n",
    "\n",
    "- L1 regularization: $\\frac{\\lambda}{2m}\\sum_{l=1}^L||W^{[L]}|| = \\sum_{i=1}^{n^{[l-1]}} \\sum_{j=1}^{n^{[l]}}|W_{ij}^{[l]}|$\n",
    "\n",
    "The parameter $\\lambda$ tunes the strength of the regularizing effect and is a hyperparameter to be tuned. We omit regularization of $b$ because most of the parameters are contained in the weights. Using L1 will tend to make $W$ sparse. In back prop we need to adjust the math for our derivatives of $J$ with respect to the weights to account for the new term. For L2 we can simply do:\n",
    "\n",
    "\\begin{align*}\n",
    "dW^{[l]} = \\big(\\textrm{expression from unregularized $J$}\\big)\\; + \\; \\frac{\\lambda}{m}W^{[l]}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "L2 reg is often called \"weight decay\" because this new derivative term will always push $W$ to be updated to a smaller value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Why Regularization Reduces Overfitting\n",
    "The intuition that the reg term will push the weights $W^{[l]}$ towards zero and thereby \"turn off\" hidden units making the NN effectively smaller/simpler is not quite accurate. The NN will still use all the hidden units but each will have a smaller effect. Further intuition is that pushing the weights to be small keeps $Z$ in the range around zero where the tanh and sigmoid activations $g(Z)$ are roughly linear, pushing the NN closer towards being simply a composition of linear functions.\n",
    "\n",
    "## Vid: Dropout Regularization\n",
    "Dropout is a powerful alternative to L2 penalty regularization, most commonly used in computer vision. You set some probability for eliminating nodes in a layer and then eliminate nodes (all their ingoing and outgoing links) with a coin flip governed by that probability. The result is a much sparser NN. You train the diminished net on one training example and then repeat the random dropout protocol to achieve a different diminished net for training on the next sample. On multiple passes through the data different NN configurations will be trained on the same data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic4.png\" width=550/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The most common way of implementing dropout is \"Inverted Dropout\". Note step (3) below is designed to eliminate the need to do scaling to your NN parameters at test time.\n",
    "\n",
    "- Define a *droput vector* for the $l^{th}$ layer, that holds a binary 0/1 for every node in the layer indicating whether it should be kept. For ($l=3$) example, `d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob`. \n",
    "\n",
    "\n",
    "- Elementwise multiply this \"mask\" to the activations of that layer `a3 = a3*d3` to eliminate the dropped nodes out of the net's computation (for both the forward and backward computations).\n",
    "\n",
    "\n",
    "- Note that in for our fully vectorized implementation (where individual training samples correspond to matrix columns) we actually use a matrix mask for each layer:  $D^{[1]} = [d^{[1](1)} d^{[1](2)} ... d^{[1](m)}] $. This means the neurons which are randomly chosen for dropping will be different in each column, thus each training sample sees a different sparsified net.\n",
    "\n",
    "\n",
    "- We would like to keep the expectation of $z$ for all the layers unchanged relative to the non-dropout NN. Recall $z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$. Since we are turning `1-keep_prob` fraction of the $a$ values in a layer to zero, we want to scale up the remaining non-zeroed $a$ to compensate. This is achieved with `a3 /= keep_prob`.\n",
    "\n",
    "\n",
    "- At test time you use the full NN (no dropout) but you no longer need the `a3 /= keep_prob` scaling factor (since all the nodes are present). This approximates taking the geometric mean of all the possible sparsified NN predictions (an extreme form of \"bagging\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here an additional [lecture video from Hinton](https://www.youtube.com/watch?v=kAwF--GJ-ek) on dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Understanding Dropout\n",
    "The simplest intuition is that dropping out units reduces the size of the NN similar to the intuition for L2 regularization. Another intuition is that it prevents the NN from relying to heavily on any single feature (input feature or learned/composed feature), since any of the inputs to any node could go away at random) so each node has to spread out the weights more. Spreading out the weights in fact has the effect of shrinking the L2 norm $||W||^2$. In fact dropout is shown to be equivalent to an \"adaptive\" L2 penalty, where the penalty on a weight scales in response to the activations being multiplied to that weight.\n",
    "\n",
    "It is reasonable to vary `keep_prob` by layer, where perhaps layers with more units have a lower `keep_prob` since they are more likely to be contributing to overfitting problems, and small layers (and usually the input layer) have `keep_prob = 1`. Note each `keep_prob` is a hyperparameter that needs to be tuned. The downside of dropout is that it is no longer as informative to monitor $J$ as a function of GD iteration, since each iteration will involve a different set of randomly diminished NNs. So begin by setting `keep_prob = 1` and plotting $J$ to ensure sensible behavior before turning on dropout.\n",
    "\n",
    "## Vid: Other Regularization Techniques\n",
    "\n",
    "### Data Augmentation\n",
    "Data augmentation helps to give you a larger training set without the true pain/expense of gathering more data. For instance, you can take an image sample and apply some random transformations and distortions to generate \"new\" images. These new points do not add as much information as gathering actual new samples, but it is simple. In some sense you are telling the algorithm that e.g. if something is a cat, a left-right mirror image is also a cat (but for example you wouldn't add in upside down transformations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic5.png\" width=450/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Early Stopping\n",
    "As you train you plot $J$ on the training and dev set error as a function of the number of GD iterations. You may see that while $J_{train}$ decreases monotonically, $J_{dev}$ begins to go back up after some number of iterations. This is because the weights $W$ are randomly initialized to very small values thus preventing overfitting early on, but they can grow as GD proceeds. In early stopping you halt GD at the inflection point of $J_{dev}$. \n",
    "\n",
    "The *Orthoganlization* principle says that we have one set of tools for the task of minimizing $J_{train}$ (GD and other numerical optimization algorithms) and a completely different set of tools for the task of reducing overfitting (regularization etc.), and we can think about and optimize these two tasks separately. However early stopping mixes the two. As an alternative to early stopping you can use L2 reg, but the downside is the need to try various values for $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic6.png\" width=450/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">Regularization is a technique to reduce overfitting when gathering more data is infeasible. *L2 and L1 regularization* work by adding a penalty term to the cost function which tends to push the weights to small values; L2 is more common, L1 often results in a sparse matrix. Intuitively, we think of smaller weights as \"turning off\" some units (simplifying the net), or as keeping $Z$ in the range around zero where the tanh and sigmoid activations $g(Z)$ are roughly linear (making the NN more like a composition of linear functions). In *dropout* regularization, nodes are eliminated at random to generate a sparser net for training each sample. Since any of the inputs to any node could go away at random), each node has to spread out its weights more, effectively shrinking the L2 norm $||W||^2$. Other ways to reduce overfitting include *data augmentation* to enlarge your training set (transforming/distorting your points to artificially generate new points), and early stopping (stopping GD when $J_{dev}$ begins to increase). However early stopping violates *orthogonalization*: the principle that we have one set of tools for the task of minimizing $J_{train}$ (GD and other numerical optimization algorithms) and a completely independent set of tools for the task of reducing overfitting (regularization etc.).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 1: Setting Up Your Optimization Problem\n",
    "\n",
    "## Vid: Normalizing Inputs\n",
    "We normalize our input data points by subtracting the mean and dividing by the variance for every feature: $x^{(i)} = (x^{(i)} - \\mu)/\\sigma^2$. This centers the data around zero and makes the variance of every feature equal to 1. Make sure to apply the exact same transformation (use the training $\\mu$ and $\\sigma$ values) to the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic7.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you use non-normalized features then the cost function over the parameter space might have a very elongated shape, so a single learning rate might be too small for effectively traversing in $w_1$ dimension, but too large for the $w_2$ dimension, and the gradient at any point does not really point toward the \"center\" where the $J$ minimum is. With normalized features $J$ will be more symmetric and the GD path will be more direct. In the first case,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic8.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Vanishing/Exploding Gradients\n",
    "In deep networks gradients of $J$ may get either tiny or huge, which makes GD difficult. Consider a very deep $L$-layer NN with linear activation. Because $g(z) = z$, the final activation $\\hat{y} = a^{[L]} = \\Pi_{l=1}^{L} W{[l]}x$. Thus if the weights in all the $W^{[l]}$ are < 1 (> 1) then the final output will be exponentially small (large) as $L$ becomes large. A similar argument applies to the gradients. This was a barrier to training large $L$ NNs for a long time, but careful initialization of weights can help.\n",
    "\n",
    "## Vid: Weight Initialization for Deep Networks\n",
    "Consider a single node receiving $n^{[l-1]}$ inputs, where $z = \\sum^n w_i x$ (where $x$ could be the activation of the underlying layer). As $n$ gets large we need to diminish the weights to prevent $z$ from blowing up. Conventionally we constrain $\\textrm{Var}(w_i) = \\frac{2}{n}$ so we initialize as $W^{[l]}$ = `np.random.randn(shape)*np.sqrt(2/n_l-1)`. Note if you are using `tanh` activation it is better to scale with `np.sqrt(1/n_l-1)`. In general this scaling value is a hyperparameter you can tune (but it is typically not super important).\n",
    "\n",
    "## Vid: Numerical Approximation of Gradients\n",
    "\"Gradient checking\" can help you catch bugs in your back-prop implementation. Recall we have a simple linear approximation to the gradient of a function at any point: $f`(\\theta) \\approx \\big( f(\\theta+\\epsilon) - f(\\theta - \\epsilon)\\big) / 2\\epsilon$. The error of this approximation is $\\mathcal{O}(\\epsilon^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic9.png\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Gradient Checking\n",
    "For implementing gradient checking, take all your parameter matrices and vectors, $W^{[1]}$, $b^{[1]}$...,$W^{[L]}$, $b^{[L]}$, unravel the $W$ into vectors, and then concatenate all these vectors into a single vector $\\Theta$. Then do likewise for all the derivative quantities to form a big vector $d\\Theta$. We want to check that $d\\Theta$ is indeed the slope of the cost function $J$ evaluated at the parameter values in $\\Theta$. For any of our parameters $\\Theta_i$ if we nudge it by $\\pm\\epsilon$ and keep the other parameters $\\Theta_{j\\neq i}$ the same then we can use the above approximation to estimate $\\frac{\\partial J}{\\partial \\Theta_i}$ and check that this is close to the value $d\\Theta_i$. This can be implemented in a simple for loop through the parameters.\n",
    "\n",
    "\\begin{align*}\n",
    "d\\Theta_i = \\frac{\\partial J}{\\partial \\Theta_i} \\approx \\frac{J(\\Theta_1, \\Theta_2...,\\Theta_i+\\epsilon,...) - J(\\Theta_1, \\Theta_2...,\\Theta_i-\\epsilon,...)}{2\\epsilon} \\equiv d\\Theta_{approx}\\\\[1em]\n",
    "\\textrm{set $\\epsilon \\approx 10^{-7}$ and check that}\\; \\frac{||d\\Theta_{approx} - d\\Theta||}{||d\\Theta_{approx}|| + ||d\\Theta||} \\approx 10^{-7}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Values of $10^{-5}$ might be OK, anything around $10^{-3}$ needs to be investigated: check individual components of the vectors to see if one component has an outsized mismatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Gradient Checking Implementation Notes\n",
    "- Only use grad check for debugging, not while training your algorithm\n",
    "- If an algorithm fails the grad check, look at the components of the vectors to identify the bug e.g. maybe the bug is only in computing the $db$ values.\n",
    "- Don't forget the regularization term in $J$ for computing the grad approximation.\n",
    "- Grad check doesn't work with dropout regularization, because $J$ is ill-defined (since a different, sparsified NN is used for computingon each sample). So first use grad check with `keep_prob=1`.\n",
    "- Grad check may pass when $W, b \\approx 0$ even with an incorrect back-prop implementation, so you can run grad check once after random initialization and then again after training for a while."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">Normalize input data points (feature-by-feature) to achieve mean of zero and variance of 1: $x^{(i)} = (x^{(i)} - \\mu)/\\sigma^2$, then apply the exact same transformation to the test set. This makes $J$ more symmetric so that GD can progress toward the minimum in fewer steps. In very deep NNs if all the weights are < 1 (> 1) then the gradient may become exponentially small (large). Help prevent this with proper random initialization: we constrain $\\textrm{Var}(w_i) = \\frac{2}{n}$ so we initialize as $W^{[l]}$ = `np.random.randn(shape)*np.sqrt(2/n_l-1)`. Gradient checking is important to catch bugs in back-prop implementation. After a forward and backward pass, unravel and concatenate all your weights (and weight derivatives) to form two long vectors $\\Theta$ and $d\\Theta$. Check that for each parameter the linear approximation to the partial derivative $\\frac{\\partial J}{\\partial \\Theta_i}\\rvert_\\Theta$ with an $\\epsilon \\approx 10^{-7}$ gives you a value very close to the corresponding component $d\\Theta_i$. Note that with dropout $J$ is ill-defined, so first set `keep_prob=1` to perform grad check.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 1 Learning Objectives:\n",
    "- Recall that different types of initializations lead to different results\n",
    "- Recognize the importance of initialization in complex neural networks.\n",
    "- Recognize the difference between train/dev/test sets\n",
    "- Diagnose the bias and variance issues in your model\n",
    "- Learn when and how to use regularization methods such as dropout or L2 regularization.\n",
    "- Understand experimental issues in deep learning such as Vanishing or Exploding gradients and learn how to deal with them\n",
    "- Use gradient checking to verify the correctness of your backpropagation implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 1 Homework\n",
    "\n",
    "**Initialization**\n",
    "- Initializing all the $W$ weights to zero results in the network failing to break symmetry: every neuron in each layer will learn the same thing, and the network is no more powerful than a linear classifier such as logistic regression.\n",
    "- Random weights breaks symmetry, but if the values are too large then the $Z^{[L]}$ will be large and $\\sigma(Z^{[L]}$ will be far out onto the flat tails, making GD less effective and potentially giving a vanishing or exploding gradient.\n",
    "- The best approach is \"He Initialization\" which multiples the random weights by $\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}$ (for RelU units) to keep them appropriately small in proportion to the number of input features from the underlying node.\n",
    "\n",
    "**Regularization**\n",
    "- With L1/L2 regularization an extra term is added to the cost, extra terms are added in back-prop for the $dW$s, and weights are pushed to smaller values (\"weight decay\")\n",
    "- With dropout, on each sample of each iteration you train a different model that uses only a subset of your neurons, thus each neuron becomes less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time.\n",
    "- Dropout implementation details:\n",
    "    - `D1 = np.random.rand(*A1.shape) < keep_prob`\n",
    "    - `A1 = (D1*A1)/keep_prob`\n",
    "    - Store the $D^{[l]}$ masks in the cache during the forward pass\n",
    "    - In backprop, apply the masks and the scaling factor to their corresponding derivative quantities e.g. `dA1 = (D1*dA1)/keep_prob`\n",
    "    \n",
    "**Gradient Checking**\n",
    "- The function relies on three helper functions: \n",
    "    - `parameter_values = dictionary_to_vector(parameters)` takes in your dictionary of parameters and unravels then concatenates them into one long vector\n",
    "    - `parameters = vector_to_dictionary(parameter_values)` reverses this process\n",
    "    - `grad = gradients_to_vector(gradients)` performs the same process on the dictionary of gradients\n",
    "- `difference = gradient_check_n(parameters, gradients, X, Y, epsilon)` calls the two above vector-making helper functions and then loops through the elements of parameter_values. For each parameter value it $\\pm$ increments only that value and calls `forward_propagation` to get $J_+$ and $J_-$ for computing the approximate partial derivative, which is added to the vector `gradapprox`. After the loop `gradapprox` is compared to `grad` and the difference (as defined in lecture notes) is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic10.png\" width=350/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 2: Optimization Algorithms\n",
    "\n",
    "## Vid:Batch vs. Mini-batch Gradient Descent\n",
    "Applied ML is iterative so being able to quickly excecute a development cycle is important, but deep NNs require lots of data to train; thus good optimization techniques are critical! In our fully vectorized implementation of NNs, if $m$ is extremely large (e.g. 5 million) than the calculations for a single iteration can still be quite slow. We can improve on this if we let GD start to make some steps before having to process all our samples. \n",
    "\n",
    "We split our training data into *mini-batches* of e.g. 1000 points, held in the matrices $X^{{t}}$ and $Y^{{t}}$ (note the curly braces!). To implement GD we add an extra loop inside the outer GD loop where we loop through the mini-batches and each $X^{{t}}$, $Y^{{t}}$ and $m_t$ play the exact same role as the full $X$, $Y$ and $m$ do in batch GD, and the parameter update is performed as usual after each mini-batch. A single pass through all your mini-batches (i.e. the full data set) is called an *epoch* and the term *iteration* is sometimes used to refer to a pass through a single mini-batch.\n",
    "\n",
    "## Vid: Understanding Mini-batch Gradient Descent\n",
    "In mini-batch GD, $J$ might not decrease after each mini-batch (since the data is different) but the overall trend will still be downward over multiple epochs. The batch size $m_t$ is a parameter to be chosen:\n",
    "- $m_t = m$: This is just normal batch GD and you spend too long computing on each iteration (blue trajectory)\n",
    "- $m_t = 1$: This called Stochastic GD (SGD); it loses all the speedup of vectorization and \"wanders\" a lot (purple)\n",
    "- $m_t$ somewhere in the middle is ideal; the path to the minimum might be \"noisier\" but it will arrive there in less time (green)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic11.png\" width=200/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For a small data set (< 2000) just use batch GD. For larger sets typical mini-batch sizes are 64, 128, 256, 512 and sometimes as high as 1024. It is important that the mini-batch size is small enough that the matrices can fit in CPU/GPU memory, otherwise the computation speed will fall off a cliff due to swapping. You can treat $m_t$ as a hyperparameter and search over it.\n",
    "\n",
    "\n",
    "## Vid: Exponentially Weighted Averages\n",
    "Consider some noisy data with a larger trend present: temperature $\\theta$ vs. day over one year. If you want to \"smooth\" the data a bit to reveal the larger pattern you can use a *moving average*. Define $v_t = \\beta v_{t-1} + (1-\\beta)\\theta_t$ where $\\beta$ is between 0 and 1 and $v_0 := 0$. These $v$ values are effectively providing an average over $\\approx \\frac{1}{1-\\beta}$ days. As $\\beta$ gets closer to 1 you average over a larger window. In the pic below original data is blue, $\\beta=0.5$ is yellow and $\\beta = 0.98$ is green."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic12.png\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Understanding Exponentially Weighted Averages\n",
    "This averages are a key component of many optimization algorithms for NNs. If we consider the form of the expression we see that the average on any day is constructed from an exponentially decaying weighting of the previous days, for example\n",
    "\n",
    "\\begin{align*}\n",
    "v_{100} = 0.1\\cdot \\big[\\theta_{100} + 0.9\\cdot \\theta_{99} + (0.9)^2\\cdot \\theta_{98} + (0.9)^3\\cdot \\theta_{97}...\\big]\n",
    "\\end{align*}\n",
    "\n",
    "To implement this you have a single `v` variable that begins at zero and then is updated on each day: `v = beta*v + (1-beta)*theta[i]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Bias Correction in Exponentially Weighted Averages\n",
    "This is a minor correct to weighted average that makes it slightly more accurate. The problem is that for the early days where there are not very many preceeding points, the initialization of $v_0 = 0$ has a disproportionate effect that biases the $v$ values toward 0. We can correct this by dividing by a factor that is small for small $t$ (thus inflating the early $v$ values) but goes to 1 for larger $t$ values (thus losing its impact):\n",
    "\n",
    "\\begin{align*}\n",
    "v_t \\rightarrow \\frac{v_t}{1-\\beta^t}\n",
    "\\end{align*}\n",
    "\n",
    "In the figure below the purple line is the average without bias correction, the green line is with bias correction. Note that many people do not bother with this correction when implementing moving averages for optimization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic13.png\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Gradient Descent with Momentum\n",
    "This method almost always works faster than standard GD: you compute an exponentially weighted average of your gradients over several iterations and use the average for the parameter update. Consider a $J$ with elliptical contours: the fact that the gradient at each step is not pointed right at the center means you cannot use too large of a learning rate, and you must take many steps oscillating back and forth perpendicular to the most direct path to the minimum. To implement this in mini-batch:\n",
    "\n",
    "- Initialize $v_{dw}$ and $v_{db}$ with `np.zeros` using `dW.shape` and `db.shape`\n",
    "- On iteration $t$:\n",
    "    - Compute $dW$, $db$ on current mini-batch\n",
    "    - Compute moving average update $v_{dw} = \\beta v_{dw} + (1-\\beta)dW$ and likewise for $v_{db}$\n",
    "    - (OPTIONAL) Bias-correct the moving average $v_{dw} = \\frac{v_{dw}}{1-\\beta^t}$ and likewise for $v_{db}$\n",
    "    - Execute parameter update using the moving average $W = W - \\alpha v_{dw}$ and likewise for $b$\n",
    "\n",
    "This works fine too for batch GD. The effect of the average is to \"damp out\" the extraneous oscillations in the direction perpendicular to the desired path. In the figure below normal GD is shown in blue (with too large of a learning rate in purple), and GD with momentum is shown in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic14.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The weighting average parameter $\\beta$ is now a hyperparameter in addition to the learning rate $\\alpha$. The most common values is $\\beta = 0.9$ which means averaging over gradients computed on the last 10 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: RMSprop\n",
    "Root Mean Square prop is another algorithm that can also speed up GD with the same idea of speeding up the movement in the direction of the desired straight path to the center, while slowing down the movement in the perpendicular direction i.e. damping oscillations. We modify the term in the moving average to $(1-\\beta_2)dW*dW$ where the multiplication is element-wise and we have renamed the parameter $beta_2$, and we also modify the update rule:\n",
    "\n",
    "- Initialize $s_{dw}$ and $s_{db}$ with `np.zeros` using `dW.shape` and `db.shape`\n",
    "- On iteration $t$:\n",
    "    - Compute $dW$, $db$ on current mini-batch\n",
    "    - Compute moving average update $s_{dw} = \\beta_2 s_{dw} + (1-\\beta_2)dW*dW$ and likewise for $s_{db}$\n",
    "    - (OPTIONAL) Bias-correct the moving average $s_{dw} = \\frac{s_{dw}}{1-\\beta_2^t}$\n",
    "    - Execute parameter update using the moving average $W = W - \\alpha \\frac{dW}{\\sqrt{s_{dw}+10^{-8}}}$ and likewise for $b$\n",
    "    \n",
    "The idea is that in the directions in which $J$ is more steeply sloped we will have large $s$ values, and thus in the update rule by dividing by $s$ we diminish the size of the update. Thus we can keep a large learning rate without generating large oscillations/divergence in the steeply sloped directions. The value of $10^{-8}$ is just to prevent dividing by zero!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Adam Optimization Algorithm\n",
    "*Adam* (\"Adaptive Moment Estimation\" because $v_{dw}$ is first moment of the gradients, $s_{dw}$ is the second moment) is essentially a combination of momentum and RMSprop, and it has been shown to be effective across a wide range of architectures and applications.\n",
    "\n",
    "- Initialize $s_{dw}$, $s_{db}$, $v_{dw}$ and $v_{db}$ with `np.zeros` using `dW.shape` and `db.shape`\n",
    "- On iteration $t$:\n",
    "    - Compute $dW$, $db$ on current mini-batch\n",
    "    - Momentum average update $v_{dw} = \\beta_1 v_{dw} + (1-\\beta_1)dW$ and likewise for $v_{db}$\n",
    "    - Momentum bias correction: $v_{dw} = \\frac{v_{dw}}{1-\\beta_1^t}$ and likewise for $v_{db}$\n",
    "    - RMSprop average update $s_{dw} = \\beta_2 s_{dw} + (1-\\beta_2)dW*dW$ and likewise for $s_{db}$\n",
    "    - RMSprop bias correction: $s_{dw} = \\frac{s_{dw}}{1-\\beta_2^t}$ and likewise for $s_{db}$\n",
    "    - Execute parameter update using both $W = W - \\alpha \\frac{v_{dw}}{\\sqrt{s_{dw}+10^{-8}}}$ and likewise for $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Typically people will use common default choices $\\beta_1 = 0.9$ and $\\beta_2 = 0.999$ and then tune the learning rate $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[Here](http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html) you can find an animated comparison of the trajectories of various optimization algoritms (on several different $J$ surfaces), and [here](http://louistiao.me/notes/visualizing-and-animating-optimization-algorithms-with-matplotlib/) you can find a blog post detailing how to build these kinds of trajectory animations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Learning Rate Decay\n",
    "Learning rate decay means slowly reducing your learning rate as GD proceeds. In mini-batch your algorithm will converge to an oscillation around the minimum of $J$ rather than exactly to the point $J_{min}$. These oscillations are tighter with a smaller $\\alpha$ (green vs. blue curve in the figure below), but we don't want to sacrifice convergence speed earlier on prior to arrive near the center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic15.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Recall that an *epoch* is one pass through the full training data. There are several options to achieve decaying $\\alpha$:\n",
    "- $\\alpha = \\frac{1}{1 + \\; \\textrm{decay rate * epoch number}}\\cdot\\alpha_0$\n",
    "\n",
    "\n",
    "- $\\alpha = 0.95^{\\textrm{epoch number}}\\cdot \\alpha_0$\n",
    "\n",
    "\n",
    "- $\\alpha = \\frac{k}{\\sqrt{\\textrm{epoch number}}}\\cdot \\alpha_0$\n",
    "\n",
    "\n",
    "- discrete staircase of values where $\\alpha$ steps down after some number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Learning rate decay can help, but it is relatively low on the list of things to try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: The Problem of Local Optima\n",
    "In high-dimensional NNs most *local optima* points where the gradient is zero are actually saddle points rather than local minimal or maxima. The reason is that for a local minimum the function must be convex in every direction, which becomes increasingly unlikely as the number of dimensions grow. Enjoy this image of a saddle point :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic16.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It turns out that *plateaus* are a much bigger problem for GD in NNs than local minima; these are large regions where the gradient is close to zero and take many steps to traverse. This problem is ameliorated by the ADAM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">With millions of points vectorized computation can still be slow. Mini-batch GD splits the training data into batches $X^{{t}}$ and $Y^{{t}}$ and loops through them, performing a parameter update after each mini batch; common batch sizes are $m_t= 64$ up to 512. With learning rate decay we slowly reduce the learning rate as GD proceeds, so that it terminates in tighter/smaller oscillations around the minimum without sacrificing too much speed early on. Several formulas can achieve this e.g. $\\alpha = \\frac{1}{1 + \\; \\textrm{decay rate * epoch number}}\\cdot\\alpha_0$, where an *epoch* is a single pass through all the mini-batches.\n",
    "</span>\n",
    "\n",
    "</span> <span style=\"background-color:#FFFFbb\"> Consider a $J$ with very elliptical contours: a GD steps will be very large in one direction (the more steeply sloped direction) and very small in another. Thus GD will not move along a direct path to the minimum but rather oscillate widely along the more steeply-sloped direction, meaning we cannot use too large of a learning rate. The Momentum algorithm averages out such oscillations prior to taking a step, while the RMSprop algorithm adjusts the step-size for a direction in proportion to how steeply sloped the gradient is in that direction; they both allow us to more safely use a larger $\\alpha$ for faster convergence. Both these algorithms also help the problem of *pleaeaus* (large regions of flat slope that take GD a long time to traverse), and both rely on computing *exponentially weighted moving averages* (EWMA) of $J$ gradient values. An EWMA $v$ is a smoothed curve approximation to noisy $\\theta$ achieved by $v_t = \\beta v_{t-1} + (1-\\beta)\\theta_t$ where $\\beta$ is between 0 and 1 and $v_0 := 0$. For early points the initialization of $v_0 = 0$ has a disproportionate effect that biases the $v$ values toward 0, this can be corrected with $v_t \\rightarrow \\frac{v_t}{1-\\beta^t}$. The ADAM algorithm combines the effects of Momentum and RMSprop and is implemented as:\n",
    "</span>\n",
    "- Initialize $s_{dw}$, $s_{db}$, $v_{dw}$ and $v_{db}$ with `np.zeros` using `dW.shape` and `db.shape`\n",
    "- On iteration $t$:\n",
    "    - Compute $dW$, $db$ on current mini-batch\n",
    "    - Momentum average update $v_{dw} = \\beta_1 v_{dw} + (1-\\beta_1)dW$ and likewise for $v_{db}$\n",
    "    - Momentum bias correction: $v_{dw} = \\frac{v_{dw}}{1-\\beta_1^t}$ and likewise for $v_{db}$\n",
    "    - RMSprop average update $s_{dw} = \\beta_2 s_{dw} + (1-\\beta_2)dW*dW$ and likewise for $s_{db}$\n",
    "    - RMSprop bias correction: $s_{dw} = \\frac{s_{dw}}{1-\\beta_2^t}$ and likewise for $s_{db}$\n",
    "    - Execute parameter update using both $W = W - \\alpha \\frac{v_{dw}}{\\sqrt{s_{dw}+10^{-8}}}$ and likewise for $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 2 Objectives\n",
    "- Remember different optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam\n",
    "- Use random minibatches to accelerate the convergence and improve the optimization\n",
    "- Know the benefits of learning rate decay and apply it to your optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 2 Homework\n",
    "\n",
    "**Mini-batch GD**\n",
    "- The mini-batches are created by `mini_batches = random_mini_batches(X, Y, mini_batch_size = 64, seed = 0)` which returns a list of tuples of the mini batches `mini_batches = [(X1, Y1), (X2, Y2)...]`.\n",
    "- Creating the mini-batches occurs by first randomly permuting the full data set, then partitioning it. Permuting is succinctly achieved with `permutation = list(np.random.permutation(m))` followed by `shuffled_X = X[:, permutation]`\n",
    "\n",
    "**Momentum**\n",
    "- Momentum is implemented by modifying just the function which updates parameters, and adding a new function which initializes the momentum moving average variable ($v$) i.e. \"velocity\" to zero.\n",
    "- `v = initialize_velocity(parameters)` creates a dictionary of variables just like the `parameters` dictionary, where the matrices now hold the velocities associated with each parameter, which are initialized to zero.\n",
    "- `parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)` is called after each mini-batch computation. It updates the velocity values in `v` using the new gradient values in `grads`, then updates the `parameters` using the momentm update rule.\n",
    "\n",
    "**Adam**\n",
    "- Adam is implemented identically to momentum, except we now have `v, s = initialize_adam(parameters)` and `parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2,  epsilon)`. Inside the update function we have the extra calculations for computing `s` and adding bias corrections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 3: Hyperparameter Tuning\n",
    "\n",
    "## Vid: Tuning Process\n",
    "In order of priority for tuning we have:\n",
    "- learning rate $\\alpha$\n",
    "    - $\\beta$ for momentum (if using ADAM, typically don't tune $\\beta_1$, $\\beta_2$)\n",
    "    - num. of hidden units\n",
    "    - mini-batch size\n",
    "        - num. of hidden layers\n",
    "        - learning rate decay strength\n",
    "        \n",
    "Previously it was common to check parameter values systematically over a grid, but with so many hyperparameters it is now more efficient to **sample randomly over the hyperparameter space**. One intuition for this is that in a 2D space you may have one hyperparameter that turns out to be MUCH more important than the other; with systematic checking you will only try out say 5 values of the very important hyperparameter, whereas with random checking you will sample many more. A good method is to start with a coarse sampling over a larger space and let the results guide you to a smaller subspace where you do finer-grained sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic17.png\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Using an Appropriate Scale to Pick Hyperparameters\n",
    "Sampling *randomly* over the hyperparameter space does NOT necessarily mean sampling *uniformly* over the range; for some hyerparameters it is more appropriate to use a different scale. Consider two cases:\n",
    "- *num. of hidden units*: reasonable values are between 50 and 100, so uniform sampling makes sense\n",
    "- *learning rate*: reasonable values are between 0.0001 and 1, if you sample uniformly at random over this range almost all of your values will be between 0.1 and 1, which underrepresents the smaller values. Instead we should samply uniformly over the $\\log_{10}(\\alpha)$. To implement, we could do e.g. `alphas = 10**(-4*np.random.rand())`.\n",
    "- *momentum $\\beta$*: Recall that $\\beta$ corresponds non-linearly to averaging the gradients over a certain number of days e.g. $\\beta = 0.9$ is a 10-day average while $\\beta = 0.999$ is a 1000-day average. For this reason a log scale is more appropriate here.\n",
    "\n",
    "The guiding principle is that you want to sample more densely in regions where a small $\\Delta$ in the parameter value results in more dramatic change to the behavior of the algorithm.\n",
    "\n",
    "## Vid: Hyperparmeter tuning in practice: pandas vs. caviar\n",
    "Intuitions for hyperparameter settings do get stale as your data grows and changes, so try to re-evaluate every few months. There are two approaches for how to tune hyperparameters:\n",
    "1. *Babysit one model (\"panda approach\").* If you have lots of data and limited compute you can watch the evolution of $J_{train}$ and $J_{dev}$ over time and interrupt periodically to adjust parameters as you see fit. Then potentially restart this process with a different model (using what you have learned).\n",
    "2. *Train many models in parallel (\"fish approach\").* With plenty of compute you can concurrently train models with many different hyperparameter settings and then just pick the one that worked best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">In order of priority we tune $\\alpha$ > ($\\beta$, num. of hidden units, mini-batch size) > (num. of hidden layers, LR decay strength). It is important to randomly (not grid-wise) sample hyperparameter configurations, which ensures that if one hyperparameter is disproportionately imporant then you will have tried many different values for it. The random sample needs to be taken on an appropriate scale. For instance, for testing the num. of hidden units between 50 and 100 we can sample linearly, but for testing $\\alpha$ between 0.0001 and 1 we should sample uniformly from the $\\log($\\alpha$)$ so as not to underrepresent small values. In general we want to sample more densely in regions where a small $\\Delta$ in the parameter value results in more dramatic change to the behavior of the algorithm. In the \"panda\" approach (lots of data, limited compute) you watch the evolution of $J_{train}$ and $J_{dev}$ over time and interrupt periodically to adjust parameters as you see fit. In the \"caviar\" approach (plenty of compute) you concurrently train models with many different hyperparameter settings and then just pick the one that worked best.<span style=\"background-color:#FFFFbb\">.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 3: Batch Normalization\n",
    "\n",
    "## Vid: Normalizing Activations in a Network\n",
    "Batch normalization is a technique that makes your NN more robust to the choice of hyperparameters (a wider range of acceptable performance) and speeds up training. Recall that normalizing your feature inputs to a NN improves GD convergence by reshaping $J$ contours to be more symmetric / less elliptical. Analogously, normalizing the inputs to deeper layers, $a^{[l]}$, can make their training more efficient.\n",
    "\n",
    "In practice we actually achieve this by normalizing a layer's $z^{[l]}$ values:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu = \\frac{1}{m} \\sum_i z^{(i)}\\;\\; , \\;\\; \\sigma^2 = \\frac{1}{m}\\sum_i (z_i - \\mu)^2\\\\[1em]\n",
    "z^{(i)}_{\\textrm{norm}} = \\frac{z^{(i)}-\\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\\\\[1em]\n",
    "\\tilde{z}^{(i)} = \\gamma z^{(i)}_{\\textrm{norm}} + \\beta\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here $\\epsilon$ is some very small number to prevent a divide-by-zero situation. The final transformation is to allow for different mean and variance of the $z^{[l](i)}$ distribution for any layer; for instance under the $\\tanh$ activation it might be better for the spread of $z$ to be wider about zero to extend out into the linear regime. **Note that $\\gamma$ and $\\beta$ are single real numbers for each layer**, but they are implemented as vectors $\\gamma^{[l]}$ and $\\beta^{[l]}$ for computation. They are both learnable parameters (NOT hyperparameters) of the model which are updated in GD just as you update the components of $W$ and $b$ (with momentum and RMSprop if you are using them)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Fitting Batch Norm into a NN\n",
    "We can think of batch norm (BN) as adding a third intermediate step to the computation of a single neuron: compute $z$, compute $\\mu$ and $\\sigma^2$ and use them to normalize $z$ which is then scaled to $\\tilde{z}$, apply activation to $\\tilde{z}$.\n",
    "\n",
    "\\begin{align*}\n",
    "a^{[l-1]} \\xrightarrow{W^{[l]}, b^{[l]}} z^{[l]} \\xrightarrow[\\text{Batch Norm}]{\\beta^{[l]}, \\gamma^{[l]}} \\tilde{z}^{[l]} \\xrightarrow{} a^{[l]} = g^{[l]}(\\tilde{z}^{[l]})\n",
    "\\end{align*}\n",
    "\n",
    "Careful not to confuse this $\\beta$ *parameter* (a different value for each layer) with the $\\beta$ *hyperparameter* (typically one global value) used in momentum. With most programming frameworks you don't need to implement batch normalization by hand. Note that BN is typically applied with mini-batch GD, where the **normalizing values $\\mu$ and $\\sigma^2$ for a mini-batch are computed from only the samples in that mini-batch.**\n",
    "\n",
    "During the BN normalization step, we first center the $z^{(i)}$ by subtracting $\\mu$; this means the value of $b^{[l]}$ is actually subtracted away! So in fact when using BN we do not need to include $b^{[l]}$ parameters in our NN, and it is effectively replaced by the single $\\beta^{[l]}$ parameter for a layer. In our vectorized implementation the parameters $\\gamma^{[l]}$ and $\\beta^{[l]}$ will have dimension $(n^{[l]}, 1)$ (but every component will be identical), and they are broadcast as needed to in order to apply the transformation to the matrix $Z^{[l] \\{j\\}}$ across all the samples for a single mini-batch.\n",
    "\n",
    "- for $t$ = 1..., number of mini-batches:\n",
    "    - Forward prop on $X^{{t}}$\n",
    "        - (within each hidden layer, use BN to replace $z^{[l]}$ with $\\tilde{z}^{[l]}$)\n",
    "    - Backprop to compute $dW^{[l]}$, $d\\beta^{[l]}$, $d\\gamma^{[l]}$ for each layer\n",
    "    - Update all parameters $W$, $\\beta$ and $\\gamma$ (using momentum and/or RMSprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Why Does Batch Norm Work?\n",
    "One reason BN is helpful is that it makes parameters of deeper layers more robust to changes in parameter values of underlying layers. Consider training cat vs. non-cat image classifier but using only black cat training examples. If you then run your classifier on colored cats, it might not generalize well (see image below). This kind of shift in your data distribution is called \"Covariate Shift\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic18.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Consider this idea in the context of a deep NN. From the perspective of any deep layer, it's input values ($a^{[l-1]}$) are shifting all the time as training proceeds. BN in fact reduces that amount by which the distribution of these input values shifts around as GD progresses and causes them to become more stable, so each layer is able to learn a little more independently of what earlier layers are doing.\n",
    "\n",
    "BN also has a small regularizing effect. Because the normalizing $\\mu$ and $\\sigma^2$ are computed on only a single mini-batch their values have some \"noise\", so that the $z^{[l]}$ values have some added noise. This encourages downstream hidden units not too rely to heavily on the output of any *single* underlying hidden unit, because their values are noisy. This is somewhat similar to the regularizing effect of dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Batch Norm at Test Time\n",
    "Commonly you use mini-batch GD with BN so a $\\mu$ and $\\sigma^2$ is computed from a single mini-batch, while at test time often we are computing on e.g. only a single test point. The solution is to come up with a separate, fixed estimate of these values to be used for applying the NN after training. This estimate is typically an exponentially weighted average across the mini-batches, which is implemented by using a variable to keep track of a running weighted average as training proceeds through mini-batches. The exact implementation of this idea might vary by programming framework, but the performance of the trained NN is usually pretty robust to what specific approach is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">Normalizing feature inputs improves GD convergence by making $J$ contours less elliptical; *Batch Normalization* analogously normalizes inputs to deeper layers to make their training more efficient. For a single layer $l$ we modify the neuron's computation to: $z^{(i)} = W^{[l]}a^{[l-1]} \\longrightarrow z^{(i)}_{\\textrm{norm}} = \\frac{z^{(i)}-\\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\longrightarrow \\tilde{z}^{(i)} = \\gamma z^{(i)}_{\\textrm{norm}} + \\beta \\longrightarrow a^{[l]} = g^{[l]}(\\tilde{z}^{[l]})$. Here $\\mu$ and $\\sigma^2$ are the mean and variance calculated for the current mini-batch. The parameter $\\beta^{[l]}$ has replaced the $b^{[l]}$ values and together with $\\gamma^{[l]}$ gives us two *new learnable parameters* for each layer which are updated just like $W^{[l]}$. **Note that $\\gamma$ and $\\beta$ are implemented as vectors to perform operations, but for each layer there is a single real number value for $\\gamma$ and likewise for $\\beta$.** BN reduces the amount by which the distribution of input $z^{[l] (i)}$ values to a layer shifts around as GD progresses, so each layer is able to learn more independently of earlier layers. BN also has a slight regularizing effect similar to dropout: because $\\mu$ and $\\sigma^2$ are computed by mini-batch their values have some \"noise\", which encourages downstream hidden units not too rely too heavily on any single underlying unit because its activation has some noise. At test time the values for $\\mu$ and $\\sigma$ are usually fixed to be an exponentially weighted average of the values over all the mini-batches, which can be tracked in an updated variable during training.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 3: Multi-Class Classification\n",
    "\n",
    "## Vid: Softmax Regression\n",
    "If you have multiple possible classes we can generalize with softmax regression. Assume we have classes 1, 2, 3, and a none-of-the-above class we denote with 0. Let $C$ be the the number of possibilities (4). We build a NN where the output layer has $n^{[L} = C$, and the $j^{th}$ node outputs the probability of the sample being in the $j^{th}$ class: $P(y=y_j | x)$. Note $\\hat{y}$ is now a $C$-vector and its values should sum to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic19.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We accomplish this using a \"softmax\" output layer, which generalizes the sigmoid (logistic) activation function to $C$ classes: \n",
    "\n",
    "\\begin{align*}\n",
    "z^{[L]} = W^{[L]}a^{[L-1]} + b^{[L]}\\\\[1em]\n",
    "t = e^{z^{[L]}} \\; \\; \\textrm{(also a $C$-vector)}\\\\[1em]\n",
    "a^{[L]} = \\frac{1}{\\sum^C_{j=1} t_i} e^{z^{[L]}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A softmax output layer with no hidden layers can represent multiple intersecting linear binary decision boundaries to demarcate $C$ regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic20.png\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Training a Softmax Classifier\n",
    "With a softmax output layer, where $\\hat{y}$ is a $C$-vector, the loss function becomes $\\mathcal{L}(\\hat{y}, y) = -\\sum_{j=1}^C y_j \\log \\hat{y}_j$, but note that since all the components of $y$ are zero except one, this sum will only have one non-zero term which will be the log probability of the true class. The cost $J$ is defined as usual from the loss. In our vectorized implementation our ground truths $Y$ and predictions `AL` i.e. $\\hat{Y}$ will both be matrices with dimension $(C, m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">We can perform multi-class classification using a \"softmax\" output layer, which generalizes a sigmoid (logistic) activation function to $C$ classes (where the $0^{th}$ class corresponds to the prediction \"none-of-the-above\"). The softmax activation computes $t = e^{z^{[L]}}$ (a $C$-vector)\n",
    "and then $\\hat{y} = a^{[L]} = \\frac{1}{\\sum^C_{j=1} t_i} e^{z^{[L]}}$, so that each element of the output vector corresponds to the probability of the sample being in that class. The loss function to use with softmax amounts to maximizing the log probability of each sample being in it's correct class: $\\mathcal{L}(\\hat{y}, y) = -\\sum_{j=1}^4 y_j \\log \\hat{y}_j$, where only one term in the sum will be non-zero due to the multiplication by $y_j$.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 3: Introduction to Programming Frameworks\n",
    "\n",
    "## Vid: Deep Learning Frameworks\n",
    "\n",
    "It is more practical to use a framework rather than code more complicated architectures / algorithms by hand. In choosing a framework, consider (1) ease of programming (including development *and* deployment), (2) running speed, and (3) truly open source with good governance (do you trust it will remain open source?).\n",
    "\n",
    "## Vid: TensorFlow\n",
    "Consider a cost function $J(w) = w^2 - 10w + 25$. We can write a program in TF to minimize this function over the space of $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define a parameter with tf.Variable and give it an initial value\n",
    "w = tf.Variable(0.0, dtype=tf.float32)\n",
    "\n",
    "# You can compose basic arithmetic function with e.g. tf.add, tf.multiply etc.\n",
    "cost = tf.add(tf.add(w**2, tf.multiply(-10.0, w)), 25.0)\n",
    "# TF has also overloaded the normal python arithmetic operators, so you can more simply do e.g. \n",
    "# cost = w**2 - 10*w + 25\n",
    "\n",
    "# Define an optimizer algorithm with a learning rate of 0.01 and a target function of \"cost\"\n",
    "# (you can use different optimizers)\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following three lines of code are very idiomatic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Start and then run a TF \"session\"\n",
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "\n",
    "# Sometimes people will instead use\n",
    "# with tf.Session() as session:\n",
    "#     session.run(init)\n",
    "#     print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# We haven't done anything yet, so \"w\" still has it's initial value of 0.\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.099999994\n"
     ]
    }
   ],
   "source": [
    "# Run one step of GD, see how w is updated\n",
    "session.run(train)\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9999886\n"
     ]
    }
   ],
   "source": [
    "# Run 1000 iterations of GD, w is very close to the true minimizing value of 5\n",
    "for i in range(1000):\n",
    "    session.run(train)\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notice we didn't have to explicitly define anything about back-prop! Tensorflow takes the function you defined e.g. `cost` and constructs a computational graph of it, and the framework has the functions built-in for doing back-prop through a graph.\n",
    "\n",
    "What if our cost was also a function of some input data (which can potentially change, but is not a \"variable\" of the model). Let's let our data `x` be the coefficients in our quadratic function for this example. The TF `placeholder` is a good way to define a quantity like your data, which can change (e.g. on each mini-batch) and which you want to pass the specific values for at run time. You pass them in to `session.run()` with a dictionary (kwarg `feed_dict`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Input data with shape [3, 1], the \"placeholder\" tells TF we will provide the values later\n",
    "x = tf.placeholder(tf.float32, [3, 1])\n",
    "cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "# Make some dummy data for x, choose them so that we expect to minimize with w=10\n",
    "coefficients = np.array([[1.], [-20.], [100.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.999977\n"
     ]
    }
   ],
   "source": [
    "# Run 1000 iterations of GD, w is very close to the true minimizing value of 10\n",
    "for i in range(1000):\n",
    "    session.run(train, feed_dict={x: coefficients})\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">In choosing a framework consider ease of programming, running speed, and whether it is truly open source (and expected to remain so). TF allows you to define functions (like \"cost\") and inputs to those functions (like data) with `tf.placeholder`, and parameters/variables of those functions (like $W$) with `tf.Variable`. TF constructs a computational graph for your function, and knows how to perform back-prop through your graph.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 3 Objectives\n",
    "- Master the process of hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 3 Homework\n",
    "\n",
    "Writing and running programs in TensorFlow has the following steps:\n",
    "\n",
    "1. Create Tensors (variables) that are not yet executed/evaluated. \n",
    "2. Write operations between those Tensors. (This defines the computational graph)\n",
    "3. Initialize your Tensors. \n",
    "4. Create a Session. \n",
    "5. Run the Session. This will run the operations you'd written above. \n",
    "\n",
    "\n",
    "\n",
    "**Key Points**\n",
    "\n",
    "- The two main object classes in tensorflow are Tensors and Operators. \n",
    "- When you code in tensorflow you have to take the following steps:\n",
    "    - Create a graph containing Tensors (Variables, Placeholders ...) and Operations on tensors (tf.matmul, tf.add, ...)\n",
    "        ` A placeholder is an object whose value you can specify only later by passing in values to a session using a \"feed dictionary\" (`feed_dict` kwarg)\n",
    "    - Create a session and then Initialize the session (initial values for tensor objects)\n",
    "    - Run the session to execute the graph\n",
    "        - You can execute the graph multiple times: think of the session as a block of code to train the model - each time you run the session on a minibatch, it trains the parameters.\n",
    "        - The backpropagation and optimization is automatically done when running the session on the \"optimizer\" object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 2: Case Studies\n",
    "\n",
    "## Vid: Why Case Studies?\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:scipybase_Apr2019]",
   "language": "python",
   "name": "conda-env-scipybase_Apr2019-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "45px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
