{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Week-1:-Introduction-to-ML-Strategy\" data-toc-modified-id=\"Week-1:-Introduction-to-ML-Strategy-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Week 1: Introduction to ML Strategy</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Why-ML-Strategy?\" data-toc-modified-id=\"Vid:-Why-ML-Strategy?-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Vid: Why ML Strategy?</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Orthogonalization\" data-toc-modified-id=\"Vid:-Orthogonalization-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Vid: Orthogonalization</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-1:-Setting-Up-Your-Goal\" data-toc-modified-id=\"Week-1:-Setting-Up-Your-Goal-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Week 1: Setting Up Your Goal</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Single-Number-Evaluation-Metric\" data-toc-modified-id=\"Vid:-Single-Number-Evaluation-Metric-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Vid: Single Number Evaluation Metric</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Satisficing-and-Optimizing-Metrics\" data-toc-modified-id=\"Vid:-Satisficing-and-Optimizing-Metrics-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Vid: Satisficing and Optimizing Metrics</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Train/dev/test-Distributions\" data-toc-modified-id=\"Vid:-Train/dev/test-Distributions-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Vid: Train/dev/test Distributions</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Size-of-dev-and-test-sets\" data-toc-modified-id=\"Vid:-Size-of-dev-and-test-sets-24\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Vid: Size of dev and test sets</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-When-to-Change-dev/test-Sets-and-Metrics\" data-toc-modified-id=\"Vid:-When-to-Change-dev/test-Sets-and-Metrics-25\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Vid: When to Change dev/test Sets and Metrics</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-1:-Comparing-to-Human-Level-Performance\" data-toc-modified-id=\"Week-1:-Comparing-to-Human-Level-Performance-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Week 1: Comparing to Human-Level Performance</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Why-Human-level-Performance?\" data-toc-modified-id=\"Vid:-Why-Human-level-Performance?-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Vid: Why Human-level Performance?</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Avoidable-Bias\" data-toc-modified-id=\"Vid:-Avoidable-Bias-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Vid: Avoidable Bias</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Understanding-Human-level-Performance\" data-toc-modified-id=\"Vid:-Understanding-Human-level-Performance-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Vid: Understanding Human-level Performance</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Surpassing-Human-Level-Performance\" data-toc-modified-id=\"Vid:-Surpassing-Human-Level-Performance-34\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Vid: Surpassing Human-Level Performance</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Improving-your-Model-Performance\" data-toc-modified-id=\"Vid:-Improving-your-Model-Performance-35\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Vid: Improving your Model Performance</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-2:-Error-Analysis\" data-toc-modified-id=\"Week-2:-Error-Analysis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Week 2: Error Analysis</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Carrying-Out-Error-Analysis\" data-toc-modified-id=\"Vid:-Carrying-Out-Error-Analysis-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Vid: Carrying Out Error Analysis</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Cleaning-Up-Incorrectly-Labeled-Data\" data-toc-modified-id=\"Vid:-Cleaning-Up-Incorrectly-Labeled-Data-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Vid: Cleaning Up Incorrectly Labeled Data</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Build-Your-First-System-Quickly,-Then-Iterate\" data-toc-modified-id=\"Vid:-Build-Your-First-System-Quickly,-Then-Iterate-43\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Vid: Build Your First System Quickly, Then Iterate</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-2:-Mismatched-Training-and-Dev/Test-Set\" data-toc-modified-id=\"Week-2:-Mismatched-Training-and-Dev/Test-Set-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Week 2: Mismatched Training and Dev/Test Set</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Training-and-Testing-on-Different-Distributions\" data-toc-modified-id=\"Vid:-Training-and-Testing-on-Different-Distributions-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Vid: Training and Testing on Different Distributions</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Bias-and-Variance-with-Mismatched-Data-Distributions\" data-toc-modified-id=\"Vid:-Bias-and-Variance-with-Mismatched-Data-Distributions-52\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Vid: Bias and Variance with Mismatched Data Distributions</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Addressing-Data-Mismatch\" data-toc-modified-id=\"Vid:-Addressing-Data-Mismatch-53\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Vid: Addressing Data Mismatch</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-2:-Learning-From-Multiple-Tasks\" data-toc-modified-id=\"Week-2:-Learning-From-Multiple-Tasks-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Week 2: Learning From Multiple Tasks</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Transfer-Learning\" data-toc-modified-id=\"Vid:-Transfer-Learning-61\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Vid: Transfer Learning</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Multi-task-Learning\" data-toc-modified-id=\"Vid:-Multi-task-Learning-62\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Vid: Multi-task Learning</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-2:-End-to-End-Deep-Learning\" data-toc-modified-id=\"Week-2:-End-to-End-Deep-Learning-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Week 2: End-to-End Deep Learning</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-What-is-End-to-End-DL?\" data-toc-modified-id=\"Vid:-What-is-End-to-End-DL?-71\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Vid: What is End-to-End DL?</a></div><div class=\"lev2 toc-item\"><a href=\"#Whether-to-Use-End-to-End-DL\" data-toc-modified-id=\"Whether-to-Use-End-to-End-DL-72\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Whether to Use End-to-End DL</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 1: Introduction to ML Strategy\n",
    "\n",
    "## Vid: Why ML Strategy?\n",
    "Consider a cat image classifier with a 90% accuracy which is simply not good enough for your application. You have many ideas of what to try: collect more data or more diverse data, train longer, try different optimizer, use a different NN architecture, use some techniques like dropout. How do we recognize which of all of these options are the most likely to help?\n",
    "\n",
    "## Vid: Orthogonalization\n",
    "It is important to have clear delineation between sets of tools designed to achieve different specific effects. Old school TVs were designed with many different tuning knobs, which were designed to tune orthogonal, distinct aspects of the picture e.g. horizontal vs. vertical stretching. Another example is a car designed with steering, accelerating and braking \"knobs\" which are completely independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic1.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There is a chain of requirements / assumptions for a good model. We want each of these goals to be responsive to a different, independent set of knobs:\n",
    "1. Fit the training set well (as compared to human-level performance) i.e. strongly minimize $J_{train}$\n",
    "    - network architecture, different optimizers\n",
    "2. Fit the development (cross-validation) set well\n",
    "    - regularization techniques, bigger / more diverse training set\n",
    "3. Fit the hold-out test set well\n",
    "    - bigger / more diverse dev set (you have \"overfitted\" to the dev set)\n",
    "4. Perform well in the real world (happy users!)\n",
    "    - different dev set or different cost function\n",
    "    \n",
    "Note that early stopping is a technique (\"knob\") that mixes effect between (1) and (2) above.\n",
    "\n",
    "**The goal of this course is to diagnose which of the above goals you are failing on, and understand what \"knobs\" you can tune to fix the problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 1: Setting Up Your Goal\n",
    "\n",
    "## Vid: Single Number Evaluation Metric\n",
    "It's important to quickly evaluate whether a new approach is doing better or worse - a single number evaluation metric makes this possible. For example, we often care about both the precision and recall of a classifier but there is a tradeoff between the two so that one model may have better precision but worse recall. So instead we should combine these two metrics into a single number based on how important they are (the standard way to do this is the \"F1 score\"). Another example is a classifier that has different error rates on examples from different geographic markets, here you should construct some average that accounts for how important each market is.\n",
    "\n",
    "## Vid: Satisficing and Optimizing Metrics\n",
    "Sometimes it is difficult to bundle all the things we care about into a single number, in this case it is useful to distinguish satisficing vs. optimizing metrics. For example, if we care about accuracy and running time for our model we might create a metric of \"maximizing accuracy (optimizing metric) subject to the constraint that run-time must be less than 100 ms (satisficing metric)\".\n",
    "\n",
    "## Vid: Train/dev/test Distributions\n",
    "Proper set up of different data sets is important to maximally efficient development cycles. Think of creating your dev set and defining your evaluation metric as setting up a Bullseye target: these are the two things which will dictate what is considered success vs. failure for your team.\n",
    "> Thus it is crucial that your dev and test sets come from the same distribution, which reflects data you expect to get  in the future and consider it important to perform well on.\n",
    "\n",
    "## Vid: Size of dev and test sets\n",
    "For data sets of less than 10,000 examples, a common way to split the data was 60/20/20 (train/dev/test). In the modern era we often have more data so with a million samples it might be reasonable to instead use 98/1/1! \n",
    "> Your dev set needs to be big enough to confidently evaluate the performance of different ideas/approaches. Your test set should be big enough to give high confidence in the overall performance of your system.\n",
    "\n",
    "If you don't need a very accurate estimate of the final performance, or if you have a large dev set such that you are less concerned about overfitting to it with hyperparameter tuning, then sometimes you don't need a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: When to Change dev/test Sets and Metrics\n",
    "Sometimes part-way through a project you realize you haven't set your \"target\" in exactly the right place. For example, you may realize that a classifier with lower accuracy will also let through more pornographic images: this is completely unacceptable to users and so it needs be incorporated into the evaluation metric. In this case you could modify the cost function so that it more heavily weights the loss terms associated with pornographic images.\n",
    "\n",
    "This also falls on the orthoganalization principle: first define the right metric to evaluate models (place the target in the right place), then worry separately about how to achieve good performance on this metric (shooting at the target).\n",
    "\n",
    ">If doing well on your metric + dev/test sets does not correspond to doing well on your application after launching the model into the real world, then change your metric and/or dev/test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">Orthogonalization means having completely independent sets of tools or knobs to tune in order to achieve different goals. For an ML project there is a sequence of four goals with their associated independent tools: Fit the training set well (network architecture, different optimizers) -> Fit the dev set well (regularization techniques, bigger / more diverse training set) -> Fit the test set well (bigger / more diverse dev set to minimize overfitting to it) -> Perform well in the real world (different dev set or evaluation metrics). It is best to construct a single real number evaluation metric that incorporates all the things you care about, this lets you quickly compare the \"goodness\" of different ideas / approaches. In some cases you can combine an optimizing metric with satisficing metric(s), like \"maximize accuracy subject to the constraint that run-time must be less than 100 ms\". **Think of creating your dev set and defining your evaluation metric as setting up a Bullseye target, so make sure your dev and test sets come from the same distribution, which reflects data you expect to get in the future and consider it important to perform well on.** Don't be afraid to change your metric or dev set midway if you realize it does not accurately reflect what you consider important.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 1: Comparing to Human-Level Performance\n",
    "\n",
    "## Vid: Why Human-level Performance?\n",
    "With deep learning, it becomes feasible to approach/exceed human-level performance in more applications. Improvement is typically rapid early in development, but often slows down when you exceed human-level performance. This is because human-level performance is often close to the *Bayes Rate* (the minimal possible error on a task limited by e.g. noise baseline), but also because we lose tools like:\n",
    "- get more labeled data from humans which is correctly labeled in a higher fraction than your algorithm could achieve\n",
    "- gain insight from manual error analysis\n",
    "- get a better analysis of bias / variance\n",
    "\n",
    "## Vid: Avoidable Bias\n",
    "Knowing how well humans can do on a task helps you understand how much you should try to address bias vs. variance issues. Consider a cat image classifier with training/dev errors of 8%/10%. If human-level error on the task is 1% you should probably try to reduce bias, whereas if it is 7.5% you in fact are doing quite well on the training set and you might want to try to reduce the variance to bring your dev error closer to the training error. We tend to think of human-level error as a decent proxy for the Bayes error rate (this is an especially good assumption for computer vision). \n",
    "\n",
    "\"Avoidable bias\" is the difference between the Bayes error rate (or your human-error approximation of it) and your training error, while the \"Variance\" is the difference between your training and dev error. \n",
    "\n",
    "## Vid: Understanding Human-level Performance\n",
    "Consider radiology images with the goal of making a medical diagnosis. We might have several different forms of \"human-level error\" e.g. untrained human, mid-career doctor, team of very experienced doctors etc. If we want a proxy for Bayes error then probably the latter is the best choice, while if we want to know whether our tool can improve upon current standard practice, maybe we should compare to the error rate by a typical doctor.\n",
    "\n",
    "It is important to choose a very good human-error proxy for the Bayes error, because as discussed above that can dictate whether you should focus on decreasing bias (to bring training error closer to Bayes error) or variance (to bring dev error closer to training error). Consider the figure below where there are three different possible ways of defining the human-error which give 1% vs. 0.7% vs. 0.5 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic2.png\" width=450/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Surpassing Human-Level Performance\n",
    "Surpassing human-level performance is increasingly common with deep learning. Consider a problem with a breakdown of human proxy for bayes / training / dev = 0.5% / 0.3% / 0.4% error rates. Once you have surpassed human-level performance it is difficult to know whether the true Bayes error is lower than your estimate, thus you should reduce bias, or if in fact your performance is a red-flag that you have overfitted and should address variance.\n",
    "\n",
    "Some problems where ML surpasses human-level performance: online advertising, product recommendations, logistics, loan approvals. These are all structured data problems rather than \"natural perception\" problems, and have huge amounts of data (far more than a single human could consume in their lifetime). \n",
    "\n",
    "## Vid: Improving your Model Performance\n",
    "Doing well in a supervised ML task requires two assumptions:\n",
    "1. You can fit the training set pretty well (minimize your \"Avoidable bias\"; the difference between your Bayes error estimate and your training error)\n",
    "    - Train a bigger model\n",
    "    - Train longer or use a better optimization algorithm\n",
    "    - Try different architecture (CNN, RNN) or search for better hyperparameter values\n",
    "2. The training performance generalizes pretty well to the dev/test performance (minimize your variance; the difference between your dev error and training error)\n",
    "    - Expand your training data (bigger set, more diverse set)\n",
    "    - Regularization (L2, dropout, data augmentation)\n",
    "    - Try different architecture (CNN, RNN) or search for better hyperparameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">Human-level performance is often close to the *Bayes Error Rate*: the minimal possible error on a task limited by noise. When human performance is superior to your algorithm we have options like getting more accurately labeled data, gaining insight from manual error analysis, and knowing whether to focus on bias or variance. Having the best human-error proxy for Bayes error is important for knowing whether to focus on decreasing bias (bring training error closer to Bayes error, the delta is called \"avoidable bias\") or variance (bring dev error closer to training error). Under the orthogonalization principle there are a different set of tools minimizing avoidable bias than for minimizing variance.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 2: Error Analysis\n",
    "\n",
    "## Vid: Carrying Out Error Analysis\n",
    "Look at examples of misclassification in your dev set to guide your development. Consider a cat classifier that you discover tends to misclassify some dogs as cats; should you devote resources to trying to make your model do better on dog pictures (add more dog images to training, engineer dog specific features etc.)? First gather ~100 mislabeled dev set examples and examine them manually to see what percentage are dogs, this gives you an idea of how much gain you could possibly have from solving the dog problem. You can do this analysis for multiple iteration ideas in parallel (see table below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic3.png\" width=450/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Cleaning Up Incorrectly Labeled Data\n",
    "During error analyis you may realize some of your data is incorrectly labeled by the human labeler. DL algorithms are actually very robust to *random* (NOT systematic) errors in the training set. If you're worried about the impact of random incorrectly labeled examples on your dev/test performance metric, simply add a column in your error analysis spreadsheet (above) to investigate how large this impact might be on your error rates as compared to other areas you could focus on. **If you decide to correct labels, apply the same process to both dev and test sets, to ensure they remain identically distributed.** Also consider looking also at examples your algorithm labeled accurately. Keep in mind that after correction your training set now comes from a slightly different distribution than your dev/test set, but this is typically not a problem for DL algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Build Your First System Quickly, Then Iterate\n",
    "For a given system there are many different areas of focus for potentially improving your algorithm. For example in speech recognition we have tools for working on noisy backgrounds, accented speech, young children's speech etc. For a brand new application it is best to build your first model quickly and then iterate.\n",
    "1. Set up dev/test set and evaluation metric\n",
    "2. Build your initial system quickly/simply\n",
    "3. Use Bias/Variance analysis and manual error analysis to prioritize next steps\n",
    "\n",
    "This may be less applicable if you have significant experience in the specific area, or there is a very large body of literature to draw from e.g. face recognition algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">Manual error analysis is very helpful for guiding development. Select 100 mislabeled dev examples and inspect them, keeping a spreadsheet to track the category of each image in terms of potential areas to work on (e.g. for a cat classifier blurry images, dog images, great cats, incorrectly labeled by human labeler) and decide what area gives the highest ceiling for improvement. If you decide to correct incorrectly labeled data, make sure to apply the same process to both dev and test sets, and consider looking also to correct labels where your classify was correct. DL algorithms are typically quite robust to *random* (NOT systematic) errors in training set labels. In general it is best to build your initial model quickly and simply, then use bias/variance and error analyses to prioritize directions for iterative development.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 2: Mismatched Training and Dev/Test Set\n",
    "\n",
    "## Vid: Training and Testing on Different Distributions\n",
    "DL algorithms are data-hungry so many teams will now gather data from diverse sources to feed for training, while keeping the dev set more reflective of the specific use-case. For example, you have 10k cat images uploaded from your mobile app (which is what your classifier will ultimately need to classify) and 200k images scraped from webpages. Two options:\n",
    "- ***BAD**:Randomly shuffle all the images into train/dev/test.* Conveniently all of your sets now come from the same distribution, but your estimate of performance on dev/test is no longer largely reflective of the use-case you care about.\n",
    "- ***GOOD**:Keep the dev and test sets purely mobile app images, let the train set be a mixture.* Now your \"target\" for your team is placed where it should be, but the training set is very different from the dev and test sets - luckily there are technique for dealing with this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Bias and Variance with Mismatched Data Distributions\n",
    "Should you always use all the data you can amass from various sources for training? If your training and dev set come from very different distributions it is more subtle to analyze bias/variance. Consider a cat classifier with ~0% Bayes error, 1% training error and 10% dev error: if train/dev are identically distributed then obviously there is a variance problem, but if not identically distributed we cannot conclude this, the dev set may just be much harder e.g. more blurry images. **To isolate bias/variance effects from *data mismatch* effects we carve out a new dev set called the *Training-dev set* which is randomly pulled from the training set; now we train on the diminished training set and look at errors on training, training-dev and dev sets to analyze bias/variance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic4.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In some cases your dev/test error may be smaller than your training/training-dev error; this reflects the dev/test distribution being \"easier\" for the classifier. It may be worthwhile to do a more granular error analysis as shown below for the example of a voice-activated rearview mirror where training data has been augmented with general speech recognition data sets. It can be worthwhile to fill in the two missing error rates (by paying someone to get an estimate of human error on the mirror speech data, and by adding some of the mirror speech data into the training set and pulling the error on that specific subset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic5.png\" width=450 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Addressing Data Mismatch\n",
    "If your training and dev/test sets come from different distributions, and your error rate analysis reveals you have a \"data mismatch\" issue, it is helful to conduct manual error analysis on the dev set to understand the differences between the two distributions that are making your dev set \"harder\" for the classifier. For the voice-activated rearview mirror example, perhaps your dev set has more car noise, or contains more street number utterances. You can then try to make your training data more similar (or collect more similar data) to the dev set on the dimensions that matter. For instance you can use ***artificial data synthesis*, such as overlaying car noise onto training set examples. In this technique beware of synthesizing data only in a very restricted subspace of your full space of application.** For instance, if you have only 1 hour of car noise that you overlay onto 10,000 hours of clean utterances, you run the risk that your model will overfit to the specific car sounds in your 1 hour. Another example is generating car images from video games, which typically only contain a limited number of car models and have much less diversity than real-world car images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">DL algorithms are data-hungry so it is common to acquire extra data from related sources. You can augment your training data but should use only your use-case-specific data for your dev/test sets. Since the augmented training data now has a different distribution, we randomly carve out from it a small holdout \"training-dev\" set and compare error rates for human-level vs. training vs. training-dev vs. dev to analyze the presence of avoidable bias, variance and data mismatch, respectively. For a data mismatch problem (dev error > training-dev error), do a manual analysis to understand the key differences between the two distributions and try to address it by gathering more training data similar to the dev set, or using artifical data synthesis (but beware of synthesizing data in only a restricted subspace of your full space of application).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 2: Learning From Multiple Tasks\n",
    "\n",
    "## Vid: Transfer Learning\n",
    "Sometimes knowledge a NN has learned from one task can be \"adapted\" to a different use-case. You do this by taking a trained NN from a related task and replacing the output layer (with a single new output layer or multiple new layers) using random initial weights, then resume training the net using the data from your new task. With limited new data you can retrain only the output layer or last few layers weights. With more data you can retrain all the weights (in this case the terminology is \"fine-tuning\" a \"pre-trained\" net). This works because early layers often act as general feature extractors e.g. detecting edges, shapes etc. Consider a speech recognition model that takes in audio and outputs a transcript, and you want want to retrain it to be a \"wakeword\" detector (e.g \"OK google\", \"hey Siri\"). \n",
    "\n",
    "Transfer learning works and is useful when:\n",
    "- The two tasks have the same kind of input data\n",
    "- You have a lot of data for the problem you are transferring from and little data for the new application\n",
    "- You think low-level features from the first task may be informative for the second task\n",
    "\n",
    "## Vid: Multi-task Learning\n",
    "This is an alternative to sequential transfer learning where you ask the NN to simultaneously do different tasks. Consider a self-driving car application where we need a classifier to take in an image and detect the presence of different objects like pedestrians, stop signs, other cars etc. We can label each image with a vector $y$ that has a 0 or 1 at each component indicating the presence of each of these individual things, and train a NN to predict these vector values. This is like multi-class classification with softmax regression, except each sample can belong in multiple classes. The loss function for any example $\\mathcal{L}(\\hat{y}, y) = -\\sum_{j=1}^C y_j \\log \\hat{y}_j$ can now have multiple (rather than just one) non-zero terms. This is more efficient and effective than just training multiple binary classifier NNs because lower-level features can be shared between the different object recognition tasks. Note that this method works even if some of your examples are \"unlabeled\" on some dimensions, in the loss function you just omit terms where there is a missing label.\n",
    "\n",
    "Multi-task learning makes sense when:\n",
    "- The individual tasks could benefit from having shared lower-level features\n",
    "- Usually: you have similar amounts of data for each task (or rather, if you expect improvement on one task then the aggregate amount of data from the other tasks needs to be much larger than for that one task)\n",
    "- You have the compute resources to train a large enough NN to do well on all the tasks (otherwise it can hurt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\"> **Transfer Learning** takes a trained NN from a task with lots of data, replaces the output layer with one or multiple new layers using random initial weights, then resumes training with the smaller amount of data from a new, related task. With very limited new data you retrain only the output or last few layer weights, otherwise you can retrain all the weights (\"fine-tuning\" a \"pre-trained\" net). This works because early layers often act as general feature extractors e.g. detecting edges, shapes etc. Multi-task learning trains a NN to do multiple tasks on the same input simultaneously using vector labels $y$ that can have multiple non-zero components. This is more effective than training several binary classifier NNs if the tasks can benefit from having shared lower-level features.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 2: End-to-End Deep Learning\n",
    "\n",
    "## Vid: What is End-to-End DL?\n",
    "End-to-End DL replaces multiple different stages of input processing with a single NN. Consider the audio processing example below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic6.png\" width=450 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "E2E requires a lot of data to work well, because you are removing a lot of information that was added by human intervention/engineering at the different stages. A multi-step approach can work better if the data sets for the individual tasks can be more easily augmented with external data than for the E2E task. For example, an automated security turnstile looking at video footage of an approaching person (developed at Baidu) was easier to develop as a two step process of locating a face on an image followed by a NN for identity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Whether to Use End-to-End DL\n",
    "Pros:\n",
    "- Lets the data speak for itself and determine the most appropriate direct mapping from $x \\rightarrow y$ rather than having human prejudice forced on the process\n",
    "- Less work hand-designing intermediate representations\n",
    "\n",
    "Cons:\n",
    "- Requires a lot of labeled data for the full task\n",
    "- Excludes potentially useful hand-designed components (which can be helpful by injecting manual knowledge in the absence of tons of data)\n",
    "\n",
    ">The key question is: Do you have sufficient data to a learn a function of the complexity needed to map $x \\rightarrow y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">End-to-End DL trains a complex task from raw input to output on a single NN, as opposed to breaking the task into a pipeline of steps. E2E lets the data decide for itself what are the most useful intermediate representations, but it requires a lot of labeled data to train and potentially loses out on the knowledge injected by manually designing the intermediate steps. Often the multi-step approach can work better if the data sets for the individual tasks can be more easily augmented with external data than for the E2E task. The key question is \"Do you have sufficient data to a learn a function of the complexity needed to map $x \\rightarrow y$?\" </span>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:scipybase_Apr2019]",
   "language": "python",
   "name": "conda-env-scipybase_Apr2019-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "300px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
