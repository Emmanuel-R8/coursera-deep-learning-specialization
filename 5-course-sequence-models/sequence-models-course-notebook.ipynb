{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Week-1:-Recurrent-Neural-Networks\" data-toc-modified-id=\"Week-1:-Recurrent-Neural-Networks-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Week 1: Recurrent Neural Networks</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Why-Sequence-Models\" data-toc-modified-id=\"Vid:-Why-Sequence-Models-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Vid: Why Sequence Models</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Notation\" data-toc-modified-id=\"Vid:-Notation-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Vid: Notation</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Recurrent-Neural-Network-Model\" data-toc-modified-id=\"Vid:-Recurrent-Neural-Network-Model-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Vid: Recurrent Neural Network Model</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Backpropagation-through-Time\" data-toc-modified-id=\"Vid:-Backpropagation-through-Time-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Vid: Backpropagation through Time</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Different-Types-of-RNNs\" data-toc-modified-id=\"Vid:-Different-Types-of-RNNs-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Vid: Different Types of RNNs</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Language-Model-and-Sequence-Generation\" data-toc-modified-id=\"Vid:-Language-Model-and-Sequence-Generation-16\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Vid: Language Model and Sequence Generation</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Sampling-Novel-Sequences\" data-toc-modified-id=\"Vid:-Sampling-Novel-Sequences-17\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Vid: Sampling Novel Sequences</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Vanishing-Gradients-with-RNNs\" data-toc-modified-id=\"Vid:-Vanishing-Gradients-with-RNNs-18\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Vid: Vanishing Gradients with RNNs</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Gated-Recurrent-Unit-(GRU)\" data-toc-modified-id=\"Vid:-Gated-Recurrent-Unit-(GRU)-19\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Vid: Gated Recurrent Unit (GRU)</a></div><div class=\"lev3 toc-item\"><a href=\"#Simplified-GRU\" data-toc-modified-id=\"Simplified-GRU-191\"><span class=\"toc-item-num\">1.9.1&nbsp;&nbsp;</span>Simplified GRU</a></div><div class=\"lev3 toc-item\"><a href=\"#Full-GRU\" data-toc-modified-id=\"Full-GRU-192\"><span class=\"toc-item-num\">1.9.2&nbsp;&nbsp;</span>Full GRU</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Long-Short-Term-Memory-(LSTM)\" data-toc-modified-id=\"Vid:-Long-Short-Term-Memory-(LSTM)-110\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Vid: Long Short Term Memory (LSTM)</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Bidirectional-RNN\" data-toc-modified-id=\"Vid:-Bidirectional-RNN-111\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Vid: Bidirectional RNN</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Deep-RNNs\" data-toc-modified-id=\"Vid:-Deep-RNNs-112\"><span class=\"toc-item-num\">1.12&nbsp;&nbsp;</span>Vid: Deep RNNs</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-1:-Homework\" data-toc-modified-id=\"Week-1:-Homework-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Week 1: Homework</a></div><div class=\"lev3 toc-item\"><a href=\"#RNN-Step-by-Step\" data-toc-modified-id=\"RNN-Step-by-Step-201\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>RNN Step-by-Step</a></div><div class=\"lev4 toc-item\"><a href=\"#With-Basic-Units\" data-toc-modified-id=\"With-Basic-Units-2011\"><span class=\"toc-item-num\">2.0.1.1&nbsp;&nbsp;</span>With Basic Units</a></div><div class=\"lev4 toc-item\"><a href=\"#With-LSTM-Units\" data-toc-modified-id=\"With-LSTM-Units-2012\"><span class=\"toc-item-num\">2.0.1.2&nbsp;&nbsp;</span>With LSTM Units</a></div><div class=\"lev3 toc-item\"><a href=\"#Character-level-language-model-(Dinosaur-Land)\" data-toc-modified-id=\"Character-level-language-model-(Dinosaur-Land)-202\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Character level language model (Dinosaur Land)</a></div><div class=\"lev3 toc-item\"><a href=\"#LSTM-Network-(Improvise-Jazz-Music)\" data-toc-modified-id=\"LSTM-Network-(Improvise-Jazz-Music)-203\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>LSTM Network (Improvise Jazz Music)</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-2:-Introduction-to-Word-Embeddings\" data-toc-modified-id=\"Week-2:-Introduction-to-Word-Embeddings-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Week 2: Introduction to Word Embeddings</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Word-Representation\" data-toc-modified-id=\"Vid:-Word-Representation-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Vid: Word Representation</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Using-Word-Embeddings\" data-toc-modified-id=\"Vid:-Using-Word-Embeddings-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Vid: Using Word Embeddings</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Properties-of-Word-Embeddings\" data-toc-modified-id=\"Vid:-Properties-of-Word-Embeddings-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Vid: Properties of Word Embeddings</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Embedding-Matrix\" data-toc-modified-id=\"Vid:-Embedding-Matrix-34\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Vid: Embedding Matrix</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-2:-Learning-Word-Embeddings\" data-toc-modified-id=\"Week-2:-Learning-Word-Embeddings-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Week 2: Learning Word Embeddings</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Learning-Word-Embeddings\" data-toc-modified-id=\"Vid:-Learning-Word-Embeddings-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Vid: Learning Word Embeddings</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Word2Vec\" data-toc-modified-id=\"Vid:-Word2Vec-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Vid: Word2Vec</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Negative-Sampling\" data-toc-modified-id=\"Vid:-Negative-Sampling-43\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Vid: Negative Sampling</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-GloVe-Word-Vectors\" data-toc-modified-id=\"Vid:-GloVe-Word-Vectors-44\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Vid: GloVe Word Vectors</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-2:-Applications-of-Embeddings\" data-toc-modified-id=\"Week-2:-Applications-of-Embeddings-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Week 2: Applications of Embeddings</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Sentiment-Classification\" data-toc-modified-id=\"Vid:-Sentiment-Classification-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Vid: Sentiment Classification</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Debiasing-Word-Embeddings\" data-toc-modified-id=\"Vid:-Debiasing-Word-Embeddings-52\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Vid: Debiasing Word Embeddings</a></div><div class=\"lev1 toc-item\"><a href=\"#Week-3:-Various-Sequence-to-Sequence-Architectures\" data-toc-modified-id=\"Week-3:-Various-Sequence-to-Sequence-Architectures-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Week 3: Various Sequence-to-Sequence Architectures</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Basic-Models\" data-toc-modified-id=\"Vid:-Basic-Models-61\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Vid: Basic Models</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Picking-the-Most-Likely-Sentence\" data-toc-modified-id=\"Vid:-Picking-the-Most-Likely-Sentence-62\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Vid: Picking the Most Likely Sentence</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Beam-Search\" data-toc-modified-id=\"Vid:-Beam-Search-63\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Vid: Beam Search</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Refinements-to-Beam-Search\" data-toc-modified-id=\"Vid:-Refinements-to-Beam-Search-64\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Vid: Refinements to Beam Search</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Error-Analysis-in-Beam-Search\" data-toc-modified-id=\"Vid:-Error-Analysis-in-Beam-Search-65\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Vid: Error Analysis in Beam Search</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Bleu-Score\" data-toc-modified-id=\"Vid:-Bleu-Score-66\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>Vid: Bleu Score</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Attention-Model-Intuition\" data-toc-modified-id=\"Vid:-Attention-Model-Intuition-67\"><span class=\"toc-item-num\">6.7&nbsp;&nbsp;</span>Vid: Attention Model Intuition</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Attention-Model\" data-toc-modified-id=\"Vid:-Attention-Model-68\"><span class=\"toc-item-num\">6.8&nbsp;&nbsp;</span>Vid: Attention Model</a></div><div class=\"lev1 toc-item\"><a href=\"#Speech-Recognition---Audio-Data\" data-toc-modified-id=\"Speech-Recognition---Audio-Data-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Speech Recognition - Audio Data</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Speech-Recognition\" data-toc-modified-id=\"Vid:-Speech-Recognition-71\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Vid: Speech Recognition</a></div><div class=\"lev2 toc-item\"><a href=\"#Vid:-Trigger-Word-Detection\" data-toc-modified-id=\"Vid:-Trigger-Word-Detection-72\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Vid: Trigger Word Detection</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 1: Recurrent Neural Networks\n",
    "\n",
    "## Vid: Why Sequence Models\n",
    "Some examples of sequence model applications\n",
    "- Speech recognition takes a sequence input (audio over time) and generates a sequence output (text transcript)\n",
    "- Music generation often has no inputs, but generates a sequence (musical note sequence)\n",
    "- Sentiment classification e.g. text input and rating output 1 through 5 stars\n",
    "- DNA sequence analysis takes in a base-pair sequence and outputs subsequences that correspond to proteins\n",
    "- Machine translation takes in a text sequence in one language and outputs a text sequence in another language\n",
    "- Video activity recognition inputs a sequence of image frames and outputs a classification of the activity being performed\n",
    "- Name entity recognition takes in a text input and outputs those components of the text which are people\n",
    "\n",
    "## Vid: Notation\n",
    "As an example, we want a sequence model to take an input $x$ which is a sequence of text like \"Harry Potter and Hermione Granger invented a new spell.\" and our model should tell us where are the people's names in the text. This is used e.g. by news sources to index all the people mentioned in the last 24 hours of news articles. You can also apply this idea to recognizing country names, currency names, times, locations etc. One output representation is a vector $y$ which gives a binary 0 or 1 for each word indicating whether it is a name (not the best representation). \n",
    "\n",
    "The elements/features of the input are words; The $i^{th}$ training example has length $T^{(i)}_x$ with components indexed by $x^{(i)<t>}$, with corresponding label having length $T_y^{(i)}$ and components $y^{(i)<t>}$.\n",
    "\n",
    "How do you represent individual words in a sequence? The first step is to create a *vocabular* or *dictionary* which is an ordered list of all the words that might be present in a training example (a dictionary of size 100,000 is not uncommon). Each feature $x^{<t>}$ can then be represented by a one-hot vector which has a single \"1\" in the corresponding position of the dictionary vector, and the sample $x$ can be represented as a matrix of these column vectors. The goal is given this $x$, learn a mapping to the target $y$ vector.\n",
    "\n",
    "## Vid: Recurrent Neural Network Model\n",
    "Consider trying to use a standard NN for this task. The first main problem is that the inputs and outputs can have varying length $T$ between the examples. Another drawback is classic NNs don't share features that are learned at different positions in the input text i.e. we want 'rules' learned in one part of the text to generalize to other parts (recall this is one of the advantages of a CNN). Finally, the first hidden layer would need an enormous number of parameters to be fully connected to every component of every one-hot vector in the input layer, even for short text segments.\n",
    "\n",
    "We instead construct a *recurrent neural network (RNN)* as follows. Feed the first feature $x^{<1>}$ into a layer to get an activation state $a^{<1>}$ and try to output $y^{<1>}$. Next feed the second feature AND the previous activation into the layer and predict $y^{<2>}$. Proceed in this fashion until the last layer which inputs $x^{<T_x>}$  and $a^{<T_x - 1>}$ and outputs $y^{<T_y>}$. This network \"scans\" through the data in order. The parameters governing the connection between the $x$ inputs and the hidden units, $W_{ax}$, are shared i.e. identical at every timestep. Likewise the parameters governing the connection between the $a$ inputs and the hidden units, $W_{aa}$, are shared, and also the paramaters governing the connection between the activations $a$ and the output $y$, $W_{y}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic1.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this way the information from any *preceeding* $x$ has a path through the network to influence any later $y$ predictions, however one weakness is that this RNN does not use information from *later* words in input text. This can be addressed by Bidirectional RNNs (BRNNs).\n",
    "\n",
    "Formally we have\n",
    "\n",
    "\\begin{align*}\n",
    "a^{<0>} = \\vec{0}\\\\[1em]\n",
    "a^{<t>} = g_1(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a)\\\\[1em]\n",
    "y^{<t>} = g_2(W_{y}a^{<t>} + b_y)\n",
    "\\end{align*}\n",
    "\n",
    "The non-linear activation $g_1$ is typically $\\tanh$ or RelU, while the depending on the output representation $g_2$ is often sigmoid. The notation can be simplified if we define a new parameter matrix and new vector by concatening two quantities side-by-side horizontally \n",
    "\n",
    "\\begin{align*}\n",
    "W_a = \\big[ W_{aa} \\; \\lvert \\; W_ax \\big]\\\\[1em]\n",
    "a^{<t>} = g_1(W_a \\big[a^{<t-1>}, x^{<t>}\\big] + b_a)\\\\[1em]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Backpropagation through Time\n",
    "When drawing the computational graph for an RNN we should picture single nodes for each of the parameter $W_{a}$, $b_a$, $W_y$ and $b_y$ with arrows showing that they flow into computational nodes for every layer. Thus we will need to backprop gradients along all of these paths backward into each parameter node, and the total gradient flowing into a parameter node will be the sum of gradients along every path.\n",
    "\n",
    "We define the loss as the sum of individual cross-entropy losses over all the components of the output:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}^{<t>}(\\hat{y}^{<t>}, y^{<t>}) = -y^{<t>}\\log \\hat{y}^{<t>} - (1-y^{<t>})\\log (1 - \\hat{y}^{<t>}\\\\[1em]\n",
    "\\mathcal{L}(\\hat{y}, y) = \\sum_{t=1}^{T_y} \\mathcal{L}^{<t>}(\\hat{y}^{<t>}, y^{<t>})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic2.png\" width=550/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this backprop algorithm the most significant backward propagation path is from $a^{<t>}$ to $a^{<t-1>}$.\n",
    "\n",
    "## Vid: Different Types of RNNs\n",
    "For some applications we may have $T_x \\neq T_y$. The basic \"many-to-many\" RNN architecture can me modified to address all these use cases (see figure below):\n",
    "- \"many-to-one\" sentiment classification (input text and output a number between 1 and 5); rather than having an output $y^{<t>}$ at each timestep, we have only a single $\\hat{y}$ output from the final layer. \n",
    "- \"one-to-many\" music generation (input nothing or an integer genre, and output musical notes); in this case rather than having an input $x^{<t>}$ at every timestep we have only a single input $x$ at the first timestep, and often the outputs $y{<t>}$ are given explicit input connections to the next layer.\n",
    "- \"many-to-many\" machine translation (input and output text, but of different lengths); here the network begins with layers that take in the $x^{<t>}$ inputs but do not produce $y^{<t>}$ outputs (encoder) and finishes with layers that produce the outputs but do not take any $x^{<t>}$ inputs (decoder).\n",
    "- Attention based architectures are yet another variant (to be discussed later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic3.png\" width=750/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">In sequence models the input and/or output are ordered sequences of elements (like words in a sentence); the $i^{th}$ training example has length $T^{(i)}_x$ with elements $x^{(i)<t>}$, with label of length $T_y^{(j)}$ and elements $y^{(j)<t>}$. Each element is represented by a one-hot vector in a *dictionary* vector space - the space of all possible values that will appear. The basic RNN architecture is to stack *identical* layers, each of which takes as input a single $x^{<t>}$ (in the correct order) and the activations $a^{<t-1>}$ from the previous layer, and outputs a single $y^{<t>}$; thus the information from *preceeding* input elements has a path through the network to influence later $y$ predictions. The weights on the $x^{<t>}$ ($W_{ax}$), $a^{<t-1>}$ ($W_{aa}$) and connection to the outputs ($W_{y}$) are \"shared\" i.e. identical for each layer/timestep. In the computational graph these parameter nodes have inputs to the computation of every layer, so in backprop the total gradient flowing into a parameter node will be the sum of gradients along every path. A common loss is the sum of individual cross-entropy losses over all the elements of the output. This basic architecture can be modified for many-to-one (sentiment classification), one-to-many (music generation) and differnt length many-to-many (machine translation) applications.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Language Model and Sequence Generation\n",
    "In speech recognition the words \"pair\" and \"pear\" sound identical within a sentence, but a *language model* allows the system to determine which of the two sentence variants has higher probability. A language model inputs a sentence (sequence) $y$ with elements $y^{<t>}$ and gives values for $P(y)$ from the distribution of all the sentences (sequences) out in the world! To build such a model, you need a training set that is a large corpus of english text. Each sentence in the corpus is *tokenized* into elements; the end of sentence is indicated by an \"EOS\" token, punctuation tokenization can be set as desired, and words not in your \"dictionary\" are indicated by \"UNK\" token. \n",
    "\n",
    "The first layer of the RNN takes inputs $a^{<0>} = \\vec{0}$ and $x^{<1>} = \\vec{0}$ and uses a softmax output over the whole dictionary to generate an estimator of the first word $\\hat{y}^{<1>}$. The second layer takes as input the activations $a^{<1}$ and the ACTUAL first word in the sentence, $y^{<1>}$, and outputs a softmax prediction for the second word $\\hat{y}^{<2>}$. This is repeated for successive layers, so for any timestep $t$ we have $y^{<t>} = x^{<t+1>}$. Each layer outputs the probability $P(\\textrm{next word}\\; \\lvert \\; \\textrm{preceeding word sequence})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For this model the loss on a single example is defined by summing the standard softmax loss function over all the elements:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L} = \\sum_t \\bigg(- \\sum_i y_i^{<t>}\\log \\hat{y}_i^{<t>}\\bigg)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Given a new sentence, the overall probability of a new complete sentence is given by multiplying the output conditional probabilities from the forward pass: $P(y) = P(y^{<1>})\\cdot P(y^{<2>} \\lvert y^{<1>})\\cdot  P(y^{<3>} \\lvert y^{<2>}, y^{<3>})...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Sampling Novel Sequences\n",
    "An interesting application of a trained language model RNN is you can sample sequences from the model:\n",
    "1. Input the zero vectors to the first layer and randomly sample according to the softmax output probabilities to draw a $y^{<1>}$.\n",
    "2. Feed the drawn $y^{<1>}$ forward to the next layer and randomly sample $y^{<2>}$ based on it's softmax output probabilities.\n",
    "3. Repeat the above until you draw the EOS token or until you reach a preset number of time steps.\n",
    "\n",
    "Notice that we could apply this kind of model and sampling approach using instead a *character level* vocabulary, which results in samples with many more elements (larger $T_y$). This has some pros and cons, for instance you never have an unknown word but because of the large $T_y$ these models are worse at capturing \"long range\" dependencies of how earlier parts of the sentence affect later parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">A language model RNN inputs a sequence $y$ and outputs conditional probabilities of each element given the preceeding elements. This allows us to calculate the overall sequence probability $P(y) = P(y^{<1>})\\cdot P(y^{<2>} \\lvert y^{<1>})\\cdot  P(y^{<3>} \\lvert y^{<2>}, y^{<3>})...$, the probability of $y$ in the distribution of all the sequences (as inferred from a large training corpus). The first layer takes inputs $a^{<0>} = \\vec{0}$ and $x^{<1>} = \\vec{0}$ and outputs softmax over the whole dictionary to generate an estimator of the first element $\\hat{y}^{<1>}$. The second layer takes inputs $a^{<1}$ and the TRUE $y^{<1>}$ and outputs a softmax prediction $\\hat{y}^{<2>}$, this is repeated for successive layers. The loss on a single example is defined by summing the standard softmax loss function over all the elements. These models can be sampled from to generate novel sequences by forward propagating beginning with the zero input vectors, where each $y^{<t>}$ is drawn from the softmax output probabilities of the layer and then fed forward.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Vanishing Gradients with RNNs\n",
    "As we saw with classic NNs, for deep NNs the gradient has a difficult time propagating back to earlier layers without severe attentuation. Consider a language model which needs to represent long-range dependencies like \"The cat... was full.\" versus \"The cats... were full.\" This model will have a similar difficulty propagating the error in the plurality of the verb far enough back to affect the noun layer. RNNs have strong local influences, but need to be modified to capture long range dependencies more effectively. Exploding gradients do also occur, but are typically easier to spot as the parameters will quickly explode to NaN; in this case you can apply the \"gradient clipping\" approach.\n",
    "\n",
    "\n",
    "## Vid: Gated Recurrent Unit (GRU)\n",
    "This is a modification to the RNN which helps with the vanishing gradient problem enabling better capture of long-range interactions. Recall for a single unit of a standard RNN\n",
    "\n",
    "\\begin{align*}\n",
    "a^{<t>} = g_1(W_a \\big[a^{<t-1>}, x^{<t>}\\big] + b_a)\\\\[1em]\n",
    "y^{<t>} = g_2(W_{y}a^{<t>} + b_y)\n",
    "\\end{align*}\n",
    "\n",
    "In a GRUnit a more complicated quantity called the *memory cell* $c^{<t>}$ replaces the activation and is combined with $x$ in a less trivial way within the computation of a layer. Like the activations, the $c^{<t>}$ are many-dimensional vectors; and some elements can be copied forward unchanged so that information about earlier parts of the sequence is \"remembered\" until it is needed in later timesteps. \n",
    "\n",
    "### Simplified GRU\n",
    "At every timestep, to generate the layer's $c^{<t>}$ we partially overwrite the value from the previous step/layer, $c^{<t-1>}$, with a candidate value $\\tilde{c}^{<t>}$, where the degree of overwriting is controlled by the *update gate* $\\Gamma_u$ variable for the layer. The update gate will typically have values close to 0 or 1 for all it's elements so that the previous timestep/layer's value is either mostly copied forward or mostly overwritten by the new candidate value. Specifically\n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{c}^{<t>} = \\tanh (W_c\\big[c^{<t-1>}, x^{<t>} \\big] + b_c)\\\\[1em]\n",
    "\\Gamma_u = \\sigma(W_u\\big[c^{<t-1>}, x^{<t>} \\big] + b_u)\\\\[1em]\n",
    "c^{<t>} = \\Gamma_u \\times \\tilde{c}^{<t>} + (1-\\Gamma_u)\\times c^{<t-1>}\\\\[1em]\n",
    "\\hat{y}^{<t>} = g_2(W_{y}c^{<t>} + b_y)\n",
    "\\end{align*}\n",
    "\n",
    "This computation can be diagrammed as below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic5.png\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Keep in mind that the activations/memories in an RNN unit are vectors (with the same dimension as the dictionary), and the calculation of $c^{<t>}$ involves an elementwise multiplication (not dot product). The vector $c^{<t>}$ will have some components that are ~0 and some ~1. Consider again the example of \"The cat... was full.\" versus \"The cats... were full.\" The usefulness of the memory cell is that the RNN can learn to update the values of different components of $c^{<t>}$ when important pieces of information appear (like the plurality of the noun \"cat\") and then hold that value through subsequent layers until reaching the point later in the sentence where the information becomes relevant (deciding the plurality of the verb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Full GRU\n",
    "A more complicated version of the GRU involves gate, the *relevance gate* $\\Gamma_r$ which dictates how relevant each component of the previous memory $c^{<t-1>}$ is for computing the new candidate $\\tilde{c}^{<t>}$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{c}^{<t>} = \\tanh (W_c\\big[\\Gamma_r \\times c^{<t-1>}, x^{<t>} \\big] + b_c)\\\\[1em]\n",
    "\\Gamma_u = \\sigma(W_u\\big[c^{<t-1>}, x^{<t>} \\big] + b_u)\\\\[1em]\n",
    "\\Gamma_r = \\sigma(W_r\\big[c^{<t-1>}, x^{<t>} \\big] + b_r)\\\\[1em]\n",
    "c^{<t>} = \\Gamma_u \\times \\tilde{c}^{<t>} + (1-\\Gamma_u)\\times c^{<t-1>}\\\\[1em]\n",
    "\\hat{y}^{<t>} = g_2(W_{y}c^{<t>} + b_y)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Long Short Term Memory (LSTM)\n",
    "The LSTM is a more complicated variant that like a GRU also allows you to capture long-range connections. In this implementation we have both the activations and the memories as separate (but related) quantities, and we add two new gates the *forget gate* $\\Gamma_f$ and the *output gate* $\\Gamma_o$. As before, all three gates and the candidate value are computed by combining the previous layer activation with the new $x$ input.\n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{c}^{<t>} = \\tanh (W_c\\big[a^{<t-1>}, x^{<t>} \\big] + b_c)\\\\[1em]\n",
    "\\Gamma_u = \\sigma(W_u\\big[a^{<t-1>}, x^{<t>} \\big] + b_u)\\\\[1em]\n",
    "\\Gamma_f = \\sigma(W_f\\big[a^{<t-1>}, x^{<t>} \\big] + b_f)\\\\[1em]\n",
    "\\Gamma_o = \\sigma(W_o\\big[a^{<t-1>}, x^{<t>} \\big] + b_o)\\\\[1em]\n",
    "c^{<t>} = \\Gamma_u \\times \\tilde{c}^{<t>} + \\Gamma_f \\times c^{<t-1>}\\\\[1em]\n",
    "a^{<t>} = \\Gamma_o \\times c^{<t>}\\\\[1em]\n",
    "\\hat{y}^{<t>} = g_2(W_{y}a^{<t>} + b_y)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic6.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the \"Peephole Connection\" variant the gate computations are dependent on $c^{<t-1>}$ as well.\n",
    "\n",
    "There is no widespread consensus on when to use an LSTM versus a GRU. The GRU is a simpler model so computation scales better, but LSTM is historically the more popular choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">Because of the vanishing gradient effect, errors in predictions of later sequence elements have trouble propagating back to affect parameters in much earlier layers, thus standard RNNs have trouble capturing long-range dependencies like \"The cat... was full.\" versus \"The cats... were full.\" In a GRUnit a *memory cell* $c^{<t>}$ replaces the activation and is computed in such a way that some of its components are copied forward unchanged and thus information from earlier parts of the sequence can be \"remembered\" until needed in later timesteps. The copying versus updating of individual components is controlled by an *update gate* vector which is a function of new weights $W_u$. In a full GRU another *relevance gate* $\\Gamma_r$ (with weights $W_r$) dictates how relevant each component of the previous memory $c^{<t-1>}$ is for computing the updated values for $c^{<t>}$. An LSTM is a popular variant of a GRU with more computational complexity within each layer.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Bidirectional RNN\n",
    "A BRNN allows elements from the later in the input sequence to affect computations in earlier timesteps, which gives a more complete picture of the context of any single element within the sequence. The BRNN has a \"forward recurrent layer\" which has the typical architecture and a \"backward recurrent layer\" which has the same architecture but is evaluated beginning with the last layer (inputing the last element of the sequence) and feeding from back to front. In a single forward pass of the net both layers are run to calculate the forward activations ($\\stackrel{\\rightarrow}{a}^{<t>}$) and backward activations ($\\stackrel{\\rightarrow}{a}^{<t>}$) of each layer, and then the two activations are combined to generate $\\hat{y}^{<t>}$ for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic7.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\\begin{align*}\n",
    "\\hat{y}^{<t>} = g\\bigg(W_y \\big[\\stackrel{\\rightarrow}{a}^{<t>}, \\stackrel{\\leftarrow}{a}^{<t>} \\big] + b_y \\bigg)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that this is quite a general setup for a BRNN and each block could instead be a GRU or LSTM block. A BRNN with LSTM is a good choice for most NLP problems. One disadvantage of BRNNs is they require you to have the complete sequence before any computation can proceed e.g. you would need to wait for the person to stop talking before trying to perform speech recognition, so for real-time applications there are more complicated implementations.\n",
    "\n",
    "## Vid: Deep RNNs\n",
    "For learning complex mappings it is often useful to stack RNNs layers vertically (where the activations of underlying layers feed into overlying layers, and the final $\\hat{y}^{<t>}$ predictions come from the top layer) in order to build deeper models. Notationally the computations/quantities within a single horizontal layer are denoted with square brackets e.g. $a^{[l]<t>}$. This is diagrammed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic8.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For each horizontal layer $l$ the computation at a given timestep $t$ would be \n",
    "\\begin{align*}\n",
    "a^{[l]<t>} = g(W_a^{[l]} \\big[ a^{[l]<t-1>}, a^{[l-1]<t>}\\big] + b^{[l]}_a).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In practice 3 layers is already quite a complex architecture. You can stack standard (fully connected) deep NNs onto the topmost horizontal layer to achieve even more complexity. You can also build stacked versions of BRNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">*Bidirectional RNNs* represent context more completely by allowing later elements in the sequence to affect computations in earlier timesteps. The BRNN has a typical \"forward layer\" and a \"backward layer\" which has the same architecture but is evaluated beginning with the last timestep and feeding from back to front. Both layers are run to calculate the forward activations ($\\stackrel{\\rightarrow}{a}^{<t>}$) and backward activations ($\\stackrel{\\rightarrow}{a}^{<t>}$) and the two are combined to generate $\\hat{y}^{<t>}$ for each timestep. BRNNs can be implemented with GRU or LSTM blocks. To build more complex models we can stack RNNs vertically; the activations of underlying layers feed into overlying layers in place of the $x^{<t>}$, and the final $\\hat{y}^{<t>}$ predictions come from the top layer. In practice this is only done with 2 or 3 layers, but you can also add standard fully-connected NNs onto each of the outputs of the top layer to achieve more depth.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 1: Homework\n",
    "\n",
    "### RNN Step-by-Step\n",
    "We implement an RNN from scratch in `numpy` by functionalizing the computation of a single unit/cell/timestep/layer and then calling this in a loop over all the timesteps in order to process all the input elements one by one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### With Basic Units\n",
    "- `a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)` executes the computation for a single unit on a 2D slice of the input volume containing the $t^{th}$ word of every sample (as pictured below), and outputs a cache of `(a_next, a_prev, xt, parameters)`.\n",
    "- `a, y_pred, cache =  rnn_forward(x, a0, parameters)` loops through the 2D slices from $t=0$ to $t=T_x$ and calls `rnn_cell_forward` on each input slice in turn (also passing in the activation slice computed on the previous iteration of the loop). It returns a volume with predictions from every timestep, a volume with the activations from every time step, and a list of the caches from each timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic9.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic11.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic10.png\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### With LSTM Units\n",
    "This code is organized identically to the basic unit RNN, except we need to compute/pass memory values $c$ and activations $a$.\n",
    "- `a_next, c_next, yt_pred, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)` executes the computation for a single unit on a 2D slice of the input volume containing the $t^{th}$ word of every sample (as pictured below), and outputs a cache of `(a_next, c_next, a_prev, c_prev, ft, it, cct ot, xt, parameters)`. Here `ft` is the forget gate, `it` is the update gate, `cct` is the candidate $\\tilde{c}$ and `ot` is the output gate.\n",
    "- `a, y, c, caches = lstm_forward(x, a0, parameters)` loops through the 2D slices from $t=0$ to $t=T_x$ and calls `lstm_cell_forward` on each input slice in turn (also passing in the activation and memory slices computed on the previous iteration of the loop). It returns a volume with predictions from every timestep, a volume with the activations from every timestep, a volume with the memories from every timestep and a list of the caches from each timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic12.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Character level language model (Dinosaur Land)\n",
    "### LSTM Network (Improvise Jazz Music)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 2: Introduction to Word Embeddings\n",
    "\n",
    "## Vid: Word Representation\n",
    "\n",
    "Word embeddings is a way of representing words so that algorithms can understand analogies like \"man:woman as king:queen\". This allows us to build models for applications with modestly sized labeled training sets.\n",
    "\n",
    "Previously we have represented words with a one-hot vector like $O_{5291}$ with a single one in the position 5291 in the complete dictionary $V$. One weakness of this representation is it treats each word as an independent thing and doesn't allow an algorithm to generalize to representations of new words. For instance an algorithm that has learned that the phrase \"I want a glass of orange...\" typically terminates with the word \"juice\" will not be able to generalize this knowledge to the phrase \"I want a glass of apple...\", even though the two words have a strong relationship. This is because in our one-hot representation the inner product between any two vectors is zero i.e. we do not capture any degrees of similarity between words. Alternatively we could use a *featurized representation* AKA *embedding* where the different components of the vector reflect different characteristics of a word's meaning on a scale from [-1, 1]. Features might be \"Gender\", \"Food\", \"Age\", \"Alive\", \"Verb\", \"Color\" (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic13.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Each high-dimensional vector in this new space can be denoted with vector $e_{5291}$. We often visualize the result of these featurized representations by reducing them 2D space (e.g. t-SNE algorithm). It is possible to train a model to learn the best features to use for this kind of representation.\n",
    "\n",
    "## Vid: Using Word Embeddings\n",
    "Word embeddings can improve named entity recognition models. For instance it may allow the model to generalize from learning relationships like \"Sally is an orange farmer\" to \"John is a durian cultivator\" even though in the entity recognition training set the uncommon words \"durian\" and \"cultivator\" didn't appear, because the embedding may have been trained a very large corpus of unlabeled text off the internet that did see those words and was able to represent the relationship between them. This is essentially a form of transfer learning, so word embedding is generally most helpful to NLP applications where you have smaller training sets. It also allows you to use smaller feature vectors than the one-hot dictionary embedding e.g. 300 components vs. 10,000. You can optionally continue to adjust the word embedding model on your specific training set (if you have enough data).\n",
    "\n",
    "Word embedding is similar to the idea of training the Siamese network for face recognition to learn the most useful vector encoding of a face image.\n",
    "\n",
    "## Vid: Properties of Word Embeddings\n",
    "Using a word embedding representation permits reasoning about analogies. For instance $e_{\\textrm{man}} - e_{\\textrm{woman}}$ may be the vector [-2, 0, 0, 0] where the first component is the gender feature; this tells us the main difference between these two word meanings is the gender. We expect that $e_{\\textrm{king}} - e_{\\textrm{queen}}$ would be a relatively *similar* vector, which allows us to reason that \"Man is to Woman as King is to Queen\". This approach often gives 30-75% accuracy on such analogy reasoning using word embeddings. The most common way to measure vector similarity is *cosine similarity* given by $\\textrm{sim}(u,v) = \\frac{u\\cdot v}{||u||_2 ||v||_2}$.\n",
    "\n",
    "## Vid: Embedding Matrix\n",
    "Algorithms to learn word embeddings will learn an *embedding matrix* $E$, where the rows correspond to the feature in the embedding space and the columns give the embeddings of all the words in the dictionary, so e.g. dimension 300x10000. Multiplying $E$ by a one-hot word vector will end up picking out only the corresponding column of the matrix, so $E \\cdot O_{j} = e_{j}$ where $j$ denotes a specific word in the dictionary. Note that in practice it is not actually efficient to use matrix-vector multiplication when executing this conversion, instead we use a specialized function to just \"look up\" the corresponding column of $E$. For training we will initialize $E$ randomly and use gradient descent to learn its elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">In one-hot representations $O_{j}$ all words are orthogonal (completely dissimilar) which prevents algorithms from generalizing learned relationships to new words. Instead we use an *embedding* representation $e_{j}$ where vector components reflect different features of a word's meaning; the best features to use in terms of capturing word relationships can be learned by training the embedding on a huge unlabeled corpus of text. We learn an *embedding matrix* such that $E \\cdot O_j = e_j$. A good embedding captures word relationships like \"man is to woman as king is to queen\" such that $e_{\\textrm{man}} - e_{\\textrm{woman}} \\sim e_{\\textrm{king}} - e_{\\textrm{queen}}$, where the *similarity* here is commonly measured as vector cosine similarity. Word embeddings are a form of transfer learning, so they are most helpful in NLP tasks with limited training data.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 2: Learning Word Embeddings\n",
    "\n",
    "## Vid: Learning Word Embeddings\n",
    "Consider building a language model where your model should predict the next word in a sequence based on the preceding words, and we have a dictionary of 10,000 words. To predict a single word in a training sequence we start with a one-hot representation $O$ of every preceeding word, use $E$ to convert these to feature representations $e$ and concatenate them, then feed that into a hidden layer ($W^{[1]}$, $b^{[1]}$), and finally feed that output to a softmax layer ($W^{[2]}$, $b^{[2]}$) which outputs a vector of probabilities for the next word in the 10,000 dimensional one-hot space. It's common to use a fixed *historical window* so that every next word is predicted by inputting only the preceeding $N$ words (thus the input size to the firs hidden layer is fixed). We can use backprop to optimize the parameters of the hidden layers as well as the components of $E$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic14.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For many applications of language models it is natural to only look at the few preceeding words, but if our goal is to learn a word embedding we can modify the model to look at different kinds of *context* for a word in the sequence. For instance we could input the preceeding 4 words AND the following 4 words, or a simpler context of only 1 nearby word (this is called a \"skip gram\" model). The latter allows us to greatly simplify the algorithm.\n",
    "\n",
    "## Vid: Word2Vec\n",
    "In the skip-grams model we use our training corpus to stochastically generate \"context\" and \"target\" pairs of words, e.g. by picking a context word at random then picking a target word from within a 4-word window around the context word. We set up a model where we input a single context word $c$ and output the corresponding target $t$. Of course this is quite a difficult task in terms of accuracy, but the goal is not to perform well on the task but rather learn the best embeddings.\n",
    "\n",
    "We begin with the one-hot $O_c$ and use $E$ to get $e_c$ which we feed into a softmax layer. The softmax layer applies a linear transformation with a weight matrix $\\theta \\cdot e_c$  and then applies the softmax function in order to output a vector of probabilities in the one-hot space, thus $\\theta$ has shape (dictionary length) x (embedding length). If we denote $\\theta_t$ as the $t^{th}$ row of the weight matrix (corresponding to the $t^{th}$ position of the dictionary) then the components of the output are:\n",
    "\\begin{align*}\n",
    "P(t\\lvert c) = \\frac{e^{\\theta_t^T\\cdot e_c}}{\\sum_{j=1}^{10000} e^{\\theta_j \\cdot e_c}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The loss is the typical softmax loss function (remember the target $y$ is represented as a one-hot vector) and the cost is minimized by adjusting both the parameters of $\\theta$ and $E$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{\\hat{y}, y} = -\\sum_{i=1}^{10000} y_i\\log \\hat{y}_i\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The computational bottleneck of this is approach is the softmax layer summing the values in the denominator of $P(t\\lvert c)$. Instead we can use a *heirarchical softmax* where you have an initial classifier indicate whether the target is in the first or second half of the dictionary, and then apply another classifier on the chosen branch etc. Each node in this tree can be a binary classifier and the ordering of the one-hot space can be chosen so that most common words are loaded toward the front so we use asymmetric split points.\n",
    "\n",
    "For generating training pairs, if you randomly sample $c$ you will get some words (like \"the\", \"of, \"a\") that too frequently. Instead we can apply some heuristics to balance out the sampling of $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Negative Sampling\n",
    "Consider a binary classification task where given a pair of words we predict whether this is a \"context-target\" pair or not. We generate a positive pair randomly from our training corpus as described above (pick a $c$ then pick a $t$ from the surrounding window), and then we generate $k$ negative examples by using the same $c$ but picking a random target word from the dictionary (note, for larger data sets we should use smaller values of $k$). On this set of training points we apply a logistic regression model where the output layer dots $e_c$ with a vector of weights $\\theta_t$ (analogous to a single row of the weight matrix for in the softmax model above) before applying the sigmoid function. Note that each target has a it's own own separate $\\theta_t$. So if the dictionary has 10,000 words then in essence we are training 10,000 binary classifiers each with it's own logistic layer weight vector $\\theta_t$ but all sharing the parameters of $E$, and for each batch of training points we are only training/updating a few of the classifiers (corresponding to the randomly chosen target words). \n",
    "\n",
    "The negative examples for a batch can be samples according to the word frequencies, $f(w_i)$ in your training corpus but you will overrepresent \"the\", \"of\", \"and\" etc., on the other hand a uniform distribution is not very representative. In practice it works well to sample words using\n",
    "\n",
    "\\begin{align*}\n",
    "P(w_i) = \\frac{f(w_i)^{3/4}}{\\sum_{j=1}^10000 f(w_j)^{3/4}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: GloVe Word Vectors\n",
    "Let $x_{ij}$ be the number of times the word $i$ appears in the context of the word $j$ (where context can be defined with different window sizes/asymmetries). The GloVe model minimizes the difference between $\\theta_i^T \\cdot e_j$ and the log of $x_{ij}$ so that for words that appear together very often the two embedding vectors ($e_j$ and the row of the weight matrix $\\theta_i$) are \"similar\" i.e. have a larger positive dot product, and vice versa. Formally we minimize\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{10000} \\sum_{j=1}^{10000} f(x_{ij}) \\big(\\theta_i^T \\cdot e_j + b_i + b_j' - \\log x_{ij}\\big)^2\n",
    "\\end{align*}\n",
    "\n",
    "Here $f(x_{ij})$ is a weighting function that gives zero weight if $x_{ij} = 0$ (so the log doesn't explode), and can adjust weights so that very common words are not overrpresented and uncommon words are not underrepresented. There are various heuristics for choosing $f$.\n",
    "\n",
    "One interesting feature of this model is that in the objective function the matrices $\\theta$ and $E$ play a symmetric role (but transposed), so to obtain the final embeddings you can actually average the encodings given by the two i.e. $e_w^{\\textrm{final}} = \\frac{e_w + \\theta_w}{2}$.\n",
    "\n",
    "We originally thought of emeddings as a featurized vector representation, but when you inspect embedding models you often cannot find straightforward interpretations of the diffent features (components). This is because we have no way of forcing the algorithm to use a basis in the feature space that corresponds to the basis that we are naturally thinking in terms of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic15.png\" width=200/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">A language model approach to learning embedding matrix $E$ is trained by predicting the next word in a sequence based on the *context* of the few preceeding words. The preceeding words are embedded using $E$, concatenated, fed into a hidden layer and finally into a softmax layer whose output gives probabilities for the next word in the one-hot vector space. In general we can choose different contexts; in the *skip-grams* model we stochastically generate single \"context\" $c$ and nearby \"target\" $t$ word pairs in our corpus. In this model the softmax layer applies a weight matrix $\\theta$ with shape (dictionary length) x (embedding length) where the $t^{th}$ row ($\\theta_t$) corresponds to the $t^{th}$ word of the dictionary]. To address the computational bottleneck of the softmax function in these models, *negative sampling* employs a separate binary logistic classifier for each possible target word $t$ (with weight vector $\\theta_t$ dotted into $e_c$). In one iteration we generate a positive pair ($c$, $t_1$) from our corpus and then randomly sample $k$ words from the dictionary to make negative pairs ($c$, $t_2$)...($c$, $t_{k+1}$), and then we train only those corresponding $k+1$ binary classifiers. In the GloVe model we let $x_{ij}$ be the number of times word $i$ appears in the context of the word $j$ and we minimize the difference between $\\theta_i^T \\cdot e_j$ and the log of $x_{ij}$ so that for words that appear together very often the two embedding vectors ($e_j$ and the row of the weight matrix $\\theta_i$) are \"similar\". In the formal loss function $\\theta$ and $E$ play a symmetric (but transposed) role, so the final embedding matrix after training can be taken as the average of the two learned matrices.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 2: Applications of Embeddings\n",
    "\n",
    "## Vid: Sentiment Classification\n",
    "Sentiment classification models look at a piece of text and decide whether someone likes or dislikes the thing they are talking about. Often the training sets are relatively small, but word embeddings can help us learn powerful models regardless. Consider a model inputing a text sequence (like a review) and outputing a number of stars 1 through 5. We can embed all the words of the input using $E$, average the $e$ vectors elementwise, then feed the resulting vector into a 5-node softmax.\n",
    "\n",
    "However this approach ignores order in the sequence e.g. \"completely lacking in good taste, good service and good ambience\" is a negative review even though the phrase \"good taste\" appears. Instead we can use a many-to-one RNN architecture where the inputs $x^{<t>}$ are the embedded versions $e^{<t>}$ for each word in the sequence (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic16.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Debiasing Word Embeddings\n",
    "We would like to eliminate undesirable bias from algorithms e.g. bias by gender, sexual orientation etc. For instance, some word embeddings learn analogies like \"father:doctor as mother:nurse\". These biases are learned from the training data presented. First we identify the direction corresponding to a bias we want to eliminate. For instance, for gender bias we need to identify the \"gender direction\" which we could do by averaging $e_{\\textrm{he}} - e_{\\textrm{she}}$, $e_{\\textrm{boy}} - e_{\\textrm{girl}}$ etc. Then for every word that is not *definitional*, we project the embedding into the subspace orthogonal to the bias direction. Definitional words are those where gender is intrinsic to the meaning e.g. \"male\" vs. \"female\". Finally we \"equalize\" definitional pairs by moving the words in pairs like \"grandmother\" and \"grandfather\" so that they are equidistant from words that should be \"gender neutral\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">Sentiment classification takes input text and outputs a category (like rating number of stars); often training sets are small so employing word embeddings are helpful. Debiasing word embeddings can remove biases learned from the training data. The \"direction\" corresponding to a bias is the difference vector in word pairs like $e_{\\textrm{he}} - e_{\\textrm{she}}$, $e_{\\textrm{boy}} - e_{\\textrm{girl}}$ etc. (we can average many such differences to get a vector representing gender). For words that do not have gender as a part of their definition, we then project their embedding into the subspace orthogonal to the bias direction.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Week 3: Various Sequence-to-Sequence Architectures\n",
    "\n",
    "## Vid: Basic Models\n",
    "Many applications like machine translation or speech recognition both input and output a sequence. Consider a model that can translate an english sentence with length $T_x$ to the French sentence of length $T_y \\neq T_x$. We construct an RNN that begins with an encoder network that inputs the elements of $x$ one by one with no output predictions and terminates with a decoder network that outputs the elements of $y$ one by one where the input at step $t$ is $y^{<t-1>}$ during training (and the max-probability element or a sampled element of $\\hat{y}^{<t-1>}$ when we want to \"sample\" from the model). This approach also works for image captioning, your encoder in this case is a pre-trained CNN outputting an encoding i.e. a large, flat vector of features, which is passed to the decoder. The decoder operates just like a language model because $\\hat{y}^{<t-1>}$ is passed as input to the $t^{th}$ layer.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic17.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Picking the Most Likely Sentence\n",
    "Machine translation is like building a conditional language model that is modeling the probability of an output translation given an input text: $\\textrm{P}(y^{<1>},...y^{<T_y>}\\lvert x)$. Butunlike with some applications of a language model, we don't want to sample outputs at random but rather find the most likely translation. One approach is *greedy search* where you pick the most likely first word from the $\\hat{y}^{<1>}$, feed that forward and pick the most likely $\\hat{y}^{<2>}$ etc. This is not ideal because at each step the probability only reflects the word context preceeding it. The total number of possible combinations to search is huge so exhaustive search is not an option. Instead we use an *approximate search algorithm*.\n",
    "\n",
    "## Vid: Beam Search\n",
    "*Beam Search* is an approximate search algorithm to find the best / most likely translation given an input sentence.\n",
    "\n",
    "- (1) Run the network up to the first unit of the decoder to evaluate $P(y^{<1>} \\lvert x)$ and choose the $B$ most likely words.\n",
    "\n",
    "\n",
    "- (2) For each of the $B$ possible first words, run the network up to the second unit of the decoder and pass it the candidate first word as input to evaluate $P(y^{<2>} \\lvert x, y^{<1>}_{candidate})$. After performing this for each of the $B$ candidate first words we select the $B$ pairs that gives the highest total probability: \n",
    "\n",
    "    \\begin{align*}\n",
    "    P(y^{<1>}, y^{<2>} \\lvert x) = P(y^{<1>} \\lvert x)\\cdot P(y^{<2>}\\lvert y^{<1>}, x)\n",
    "    \\end{align*}\n",
    "    \n",
    "- (3) With our $B$ candidates for the first+second word sequence we evaluate the third unit of the decoder to identify the $B$ most likely first+second+third word sequence. \n",
    "\n",
    "- (4) Repeat in this fashion until reaching some maximum set length. Any time the EOS token appears in a top candidate sequence, store that sequence for consideration (it's branch terminates at that point).\n",
    "\n",
    "- (5) From all the terminated stored sequences, compare the total probability metric (possibly normalized, see below) to select the final choice.\n",
    "\n",
    "Note that if $B=1$ this is equivalent to greedy search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Refinements to Beam Search\n",
    "In beam search we calculate the total probability of a sequence; if the multiplied probabilities are small and/or the sequence is long then the total probability may be too small for accurate floating point representation. For this reason we instead work with log probs (so the total probability is a sum) which is more numerically stable. \n",
    "\n",
    "An unintended consequence of our metric is that the total probability will tend to be higher for shorter sequences (you are multiplying fewer small numbers), so we can modify our metric to use *length normalization*. In given step of beam search we now select the top $B$ candidate sequences according to a normalized total probability metric \n",
    "\\begin{align*}\n",
    "\\frac{1}{T_y^\\alpha} \\sum_{t=1}^{T_y}\\log P(y^{<t>} \\lvert x, y^{<1>},...y^{<t-1>})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here tunable hyperparameter $\\alpha$ controls the strength of normalization, and a common value is $\\alpha=0.7$.\n",
    "\n",
    "The beam width $B$ is tunable; smaller values run faster, but larger values will explore more possibilities so you will tend to get better results. In production systems $B=10$ is not uncommon, in academic papers $B=1000$ will squeeze out the best possible result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Error Analysis in Beam Search\n",
    "It's important to figure out if you should spend time improving your RNN (encoder+decoder), or increasing your beam width for beam search. Consider a french sentence with a good human translation in english ($y^*$) and the predicted translation from your algorithm ($\\hat{y}$) which is not very good. One analysis tactic is to use your RNN to compare the computed values $P(y^* \\lvert x)$ versus $P(\\hat{y} \\lvert x)$. If the algorithm output evaluates to a higher probability than the actual correct human translation, it is likely that you should improve your RNN, whereas the other case implies that beam search is at fault. Perform this analysis for a random sample of errors to get a sense of what fraction of errors are due to the RNN versus the Beam, and then choose how to allocate your time.\n",
    "\n",
    "## Vid: Bleu Score\n",
    "One issue with machine translation is that there are often multiple possible translations that are equally valid, so how can we measure accuracy? The Bleu (bilingual evaluation understudy) score gives a score to a machine translation output based on how close it is to a set of human translation *references*, and allows us to automatically evaluate algorithm output.\n",
    "\n",
    "We define the *modified precision* of the output as the fraction of the words in the output sequence that also appear in any of the references, but where each unique word in the output is only given \"credit\" for appearances up to a \"clipped\" value dictated by the maximum number of times the word appears in any of the references. In the example below the output would be given a modified precision of 2/7.\n",
    "- Input: \"Le chat est sur le tapis.\"\n",
    "- Human Ref 1: \"The cat is on the mat.\"\n",
    "- Human Ref 2: \"There is a cat on the mat.\"\n",
    "- Output: \"the the the the the the the.\"\n",
    "\n",
    "We also can consider word-pairs (*bigrams*) in computing the modified precision. If we change the output of the above example to:\n",
    "- Output: \"The cat the cat on the mat.\"\n",
    "- Output bigrams: \"the cat\", \"cat the\", \"the cat\", \"cat on\", \"on the\", \"the mat\"\n",
    "\n",
    "Again, each bigram is only given credit for appearances up to the maximum number of appearances in any of the references so of the 6 (non-unique) bigrams that appear, \"the cat\" only gets credit for one appearance and \"cat the\" gets no credit because it does not exist in any reference, thus precision is 4/6.\n",
    "\n",
    "In general we define\n",
    "\n",
    "\\begin{align*}\n",
    "p_n = \\frac{ \\sum_{\\textrm{all n-grams} \\: \\in \\: \\hat{y}} \\textrm{Count}_{clipped}(\\textrm{n-gram})}{\\sum_{\\textrm{all n-grams} \\: \\in \\: \\hat{y}} \\textrm{Count}(\\textrm{n-gram})}\n",
    "\\end{align*}\n",
    "\n",
    "The Bleu score reflects the modified precision on various *n-grams* like bigrams and unigrams (single words) and includes a *Brevity Penalty* ($BP$) which penalizes shorter translations to correct the bias due to shorter translations having an easier time achieving high precision scores.\n",
    "\\begin{align*}\n",
    "\\textrm{Bleu} = \\textrm{BP}\\cdot \\exp \\big (\\frac{1}{4} \\sum_{n=1}^4 p_n \\big)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The Bleu score can also be used for other applications where there may be multiple ground truths, such as image captioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">In *Machine Translation* (MT) we input text with length $T_x$ and output a translation of length $T_y \\neq T_x$. We use an RNN  beginning with encoder units inputing $x^{<t>}$ one-by-one with no output and ending with decoder units that output $y^{<t>}$ one-by-one where the input at step $t$ is $y^{<t-1>}$ during training. The decoder network in the RNN is wired exactly like a language model, but rather than using it to randomly sample after training, we need to use it to find the translation $y$ with the highest likelihood. This is done with *Beam Search* which evaluates the decoder unit-by-unit, propagating only the $B$ most likely sequence \"branches\" into the next timestep. At each step if a top candidate sequence terminates with EOS it is removed from propagation and stored for later consideration. After a set maximum number of steps, all the stored candidates are evaluated using a *length-normalized total probability* $\\frac{1}{T_y^\\alpha} \\sum_{t=1}^{T_y}\\log P(y^{<t>} \\lvert x, y^{<1>},...y^{<t-1>})$, where $\\alpha$ is a tunable hyperparameter. The *Beam Width* ($B$) can also be tuned - lower values search faster but tend to give worse results. The *Bleu score* is an accuracy metric that can evaluate the \"correctness\" of a MT output when there are many equally valid human translation references; it is also applicable to other multiple-ground-truth scenarios such as image captioning.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vid: Attention Model Intuition\n",
    "In the basic MT RNN a encoder network reads in a sentence and a completely separate decoder network outputs a sentence, so in a sense the encoder is being asked to store a memory of the entire sentence encoded in a single activation output of its final unit. The *attention model* modifies this to improve performance; analagous to how a human translator will read and translate smaller chunks one at a time from longer sentences. Consider a bidirectional RNN inputing a french sentence and computing (bidirectional) activations $a^{<t>}$ at each timestep. For a decoder RNN taking inputs from this bidirectional encoder, for predicting the first word of the translation $y^{<1>}$ it should probably only look at activations from the first few units (words) in the encoder RNN (input text). The *attention weights* $\\alpha^{<i, j>}$ denote the amount of weight placed on the $j^{th}$ encoder unit activation when the decoder is computing the $i^{th}$ word. The encoder activations together with their corresponding weights tell us the *context* $c$ that any decoder unit (word of the output text) should be paying attention to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic18.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In fact, the weight $\\alpha^{<i, t>}$ is determined by both the activation from the previous unit of the decoder, $s^{<i-1>}$ as well as the forward and backward activations of the encoder at the corresponding timestep, $\\stackrel{\\rightarrow}{a}^{<t>}$ and $\\stackrel{\\rightarrow}{a}^{<t>}$.\n",
    "\n",
    "## Vid: Attention Model\n",
    "To formalize the model described above, in the encoder portion we will use $t'$ to index to input words/units which each have a vector comprising the forward and backward activations concatenated together $a^{<t'>} = (\\stackrel{\\rightarrow}{a}^{<t>}, \\stackrel{\\rightarrow}{a}^{<t>})$. For the decoder portion we will use a unidirectional forward RNN and let the hidden states be denoted with $s^{<t>}$. Each decoder unit takes as input the previous hidden activation $s^{<t-1>}$, the context $c^{<t>}$ and the preceeding ground truth or predicted word in the sequence, $y^{<t-1>}$, and outputs $\\hat{y}^{<t>}$. The context for a timestep is computed from the weighted activations of the encoder net:\n",
    "\n",
    "\\begin{align*}\n",
    "c^{<t>} = \\sum_{t'} \\alpha^{<t, t'>} a^{<t'>}\\\\[1em]\n",
    "\\textrm{where the weights satisfy normalization} \\qquad \\sum_{t'} \\alpha^{<t, t'>} = 1 \\; \\forall \\; t\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It is slightly complicated to compute good weights. We can ensure the normalization constraint by constructing weights from a set of *factors* $e$ using the softmax equation:\n",
    "\\begin{align*}\n",
    "\\alpha^{<t, t'>} = \\frac{\\exp (e^{<t, t'>})}{ \\sum_{t'=1}^{T_x} \\exp (e^{<t, t'>})}\\\\[1em]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "These factors $e^{<t, t'>}$ can be computed from a small (single FC layer) sub-net which inputs $s^{<t-1>}$ and $a^{<t'>}$. Remember that the weights capture the attention the $t^{th}$ output word should pay to the $t'^{th}$ input word. Unfortunately this algorithm does take quadratic cost to run.\n",
    "\n",
    "A use case for attention models is to take in dates in various text formats and output a normalized form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/pic19.png\" width=900/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">An attention model in MT allows the output word from a decoder unit to reflect different levels of attention to various parts of the input sentence. The encoder is a bidirectional RNN indexed by $t'$ with unit activations $a^{<t'>} = (\\stackrel{\\rightarrow}{a}^{<t>}, \\stackrel{\\rightarrow}{a}^{<t>})$. The decoder is a unidirectional RNN with hidden states denoted by $s^{<t>}$; each decoder unit takes as input the previous hidden activation $s^{<t-1>}$, the *context* $c^{<t>}$ and the preceeding ground truth / prediction, $y^{<t-1>}$, and outputs $\\hat{y}^{<t>}$. The context for a timestep is computed as an average of the encoder activations using weights $\\alpha^{<t, t'>}$ that reflect the attention the $t^{th}$ output word should pay to the $t'^{th}$ input word. The weights must be normalized for each $t$, so they are calculated from a set of factors $e^{<t, t'>}$ using the softmax formula, and the $e^{<t, t'>}$ in turn are computed from a small sub-net (single FC layer) which inputs $s^{<t-1>}$ and $a^{<t'>}$. One disadvantage of attention models is they run in quadratic time.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Speech Recognition - Audio Data\n",
    "\n",
    "## Vid: Speech Recognition\n",
    "In speech recognition you input an audio clip $x$ and output a text transcript $y$. The audio clips $x$ can be interpreted as a plot of air pressure versus time. The signal has various harmonic components, and a common preprocessing step is to extract the strength of these components in the form of a \"spectrogram\" (a heatmap with time on the x-axis, frequency on the y-axis and colored by intensity). The human ear has structures which perform an analagous function. \n",
    "\n",
    "One successful method for speech recognition uses the *Connectionist Temporal Classification Cost* (CTC). We use a model (often bidirectional LSTM) with $T_x = T_y$ and we need to use a large number of timesteps because each input $x^{<t>}$ is a single sample point from the audio clip (typically recorded at ~100 Hz so that even short audio clips have many timepoints). This means that any letter or phoneme being uttered within the clip will span over many units, thus we may have a sequence of predictions like [t, t, t, _, h, _, e, e, e, _, _, _, space, _, _, _, q, q, q]. Here the \"_\" character stands for blank and we have a space character. In the CTC cost we collapse repeated characters that are not separated by a \"blank\" and then remove the \"blanks\". This allows the RNN to represent e.g. a 19-character output transcript with a 1000-element output sequence.\n",
    "\n",
    "## Vid: Trigger Word Detection\n",
    "Production systems for general speech recognition typically require vast amounts of data to train. However trigger word detection (for devices that you \"wake up\" with your voice) can be implemented more easily. One algorithm is to begin with a (possibly spectrogram-ifiied) audio clip and create a corresponding vector $y$ of labels where each element is zero except at the points where a person finished saying the trigger word. Unfortunately this creates a very imbalanced training set, so you can instead set a sequence of following timepoints to have value 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span><span style=\"background-color:#FFFFbb\">*Speech recognition* inputs an audio clip $x$ (air pressure versus time) and outputs a text transcript $y$. a common preprocessing step is to convert the audio to a heatmap with time on the x-axis, frequency on the y-axis and colored by intensity of the frequency harmonic component. A common algorithm is a bidirectional LSTM with $T_x = T_y$ where each input $x^{<t>}$ is a single sample point so that any phoneme being uttered will span over many units. We output a sequence of $y^{<t>}$ values like [t, t, t, _, h, _, e, e, e, _, _, _, space, _, _, _, q, q, q] where \"_\" stands for blank. In the CTC cost we collapse repeated characters that are not separated by a \"blank\" and then remove the \"blanks\". This allows the RNN to represent e.g. a 19-character output transcript with a 1000-element output sequence. Trigger word detection can be implemented similiarly except each $y^{<t>}$ is binary and the vector $y$ has a 1 (or a run of 1s) only at time points where a person just finished uttering the trigger word.</span>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:scipybase_Apr2019]",
   "language": "python",
   "name": "conda-env-scipybase_Apr2019-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "373px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
